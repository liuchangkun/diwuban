# æ°´æ³µæ•°æ®å¤„ç†æŠ€æœ¯è§„èŒƒ

## ğŸ”§ å¦‚ä½•ä½¿ç”¨æœ¬è§„èŒƒ

1. æ•°æ®å¤„ç†å¿«é€Ÿä¸Šæ‰‹ï¼šé€‰æ‹©æ›²çº¿ç±»å‹ â†’ æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ§åˆ¶ â†’ é€‰æ‹©æ‹Ÿåˆæ–¹æ³• â†’ éªŒè¯ç²¾åº¦ï¼ˆRÂ² â‰¥ 0.95ï¼‰ã€‚
1. ä»£ç å¤ç”¨ï¼šæ‰€æœ‰ä»£ç å—å¯ç‹¬ç«‹è¿è¡Œï¼Œç›´æ¥å¤åˆ¶åˆ°é¡¹ç›®ä¸­å³å¯ä½¿ç”¨ï¼›æ³¨æ„numpy/pandas/scipyä¾èµ–ã€‚
1. è´¨é‡æ ‡å‡†ï¼šéµå¾ªç‰©ç†çº¦æŸéªŒè¯ â†’ å•è°ƒæ€§æ£€æŸ¥ â†’ å¼‚å¸¸å€¼å¤„ç† â†’ å•ä½è½¬æ¢ä¸å½’ä¸€åŒ–çš„æ ‡å‡†æµç¨‹ã€‚
1. æ‰©å±•é€‚é…ï¼šæ–°è®¾å¤‡ç±»å‹å‚è€ƒ"æ•°æ®æ‰©å±•é€‚åº”æ€§è®¾è®¡"ï¼›æ–°ä¼ æ„Ÿå™¨æ•°æ®æ ¼å¼å‚è€ƒ"å¤šæ ¼å¼æ•°æ®è§£æå™¨"ã€‚

ç‰ˆæœ¬ï¼šv1.0 | é€‚ç”¨èŒƒå›´ï¼šCSVæ•°æ®å¯¼å…¥ã€ç‰¹æ€§æ›²çº¿æ‹Ÿåˆã€æ•°æ®è´¨é‡æ§åˆ¶ã€é¢å®šå‚æ•°è®¡ç®—

## ç›®å½•

1. [æ•°æ®ç»“æ„åˆ†æ](#1-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90)
1. [ç‰¹æ€§æ›²çº¿æ‹ŸåˆæŠ€æœ¯è§„èŒƒ](#2-%E7%89%B9%E6%80%A7%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E6%8A%80%E6%9C%AF%E8%A7%84%E8%8C%83)
1. [æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ§åˆ¶](#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E8%B4%A8%E9%87%8F%E6%8E%A7%E5%88%B6)
1. [æ•°æ®è¡¥é½æŠ€æœ¯è§„èŒƒ](#4-%E6%95%B0%E6%8D%AE%E8%A1%A5%E9%BD%90%E6%8A%80%E6%9C%AF%E8%A7%84%E8%8C%83)
1. [æ³µé¢å®šå‚æ•°è®¡ç®—è§„èŒƒ](#5-%E6%B3%B5%E9%A2%9D%E5%AE%9A%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97%E8%A7%84%E8%8C%83)
1. [æ•°æ®æ‰©å±•é€‚åº”æ€§è®¾è®¡](#6-%E6%95%B0%E6%8D%AE%E6%89%A9%E5%B1%95%E9%80%82%E5%BA%94%E6%80%A7%E8%AE%BE%E8%AE%A1)

______________________________________________________________________

## 1. æ•°æ®ç»“æ„åˆ†æ

### 1.1 è®¾å¤‡åˆ†ç±»

æ ¹æ®éœ€æ±‚åˆ†æï¼Œç³»ç»Ÿéœ€å¤„ç†ä»¥ä¸‹æ°´æ³µè®¾å¤‡ç±»å‹ï¼š

#### 1.1.1 æŒ‰æ³µç±»å‹åˆ†ç±»

- **ç¦»å¿ƒæ³µ**: å•çº§ã€å¤šçº§ã€åŒå¸ã€å¯¼å¶å¼
- **è½´æµæ³µ**: ç«‹å¼ã€å§å¼ã€æ··æµå¼
- **å¾€å¤æ³µ**: æŸ±å¡æ³µã€éš”è†œæ³µ
- **è½¬å­æ³µ**: é½¿è½®æ³µã€èºæ†æ³µã€ç½—èŒ¨æ³µ
- **ç‰¹æ®Šæ³µ**: ç£åŠ›æ³µã€å±è”½æ³µã€æ¶²ç¯æ³µ

#### 1.1.2 æŒ‰é©±åŠ¨æ–¹å¼åˆ†ç±»

- **ç”µæœºé©±åŠ¨**: ç›´è”ã€è”è½´å™¨è¿æ¥ã€çš®å¸¦ä¼ åŠ¨
- **å˜é¢‘é©±åŠ¨**: å˜é¢‘å™¨æ§åˆ¶çš„ç”µæœºæ³µç»„
- **è½¯å¯åŠ¨**: è½¯å¯åŠ¨å™¨æ§åˆ¶çš„æ³µç»„

### 1.2 æ•°æ®æ ¼å¼è§„èŒƒ

#### 1.2.1 CSVæ–‡ä»¶æ ¼å¼

```python
# å®é™…CSVæ–‡ä»¶ç»“æ„ï¼ˆåŸºäºdataç›®å½•ä¸‹çš„æ–‡ä»¶æ ¼å¼ï¼‰
REQUIRED_COLUMNS = {
    'TagName': 'æ ‡ç­¾åç§°/ä¼ æ„Ÿå™¨æ ‡è¯†',
    'DataTime': 'æ•°æ®æ—¶é—´æˆ³',
    'DataVersion': 'æ•°æ®ç‰ˆæœ¬',
    'DataQuality': 'æ•°æ®è´¨é‡æ ‡è¯†',
    'DataValue': 'æ•°æ®å€¼'
}

# CSVæ–‡ä»¶å‘½åè§„èŒƒï¼ˆåŸºäºå®é™…æ–‡ä»¶ï¼‰
FILE_NAMING_PATTERN = {
    'format': '_{æœŸæ•°}_{æ³µæˆ¿ç±»å‹}_{è®¾å¤‡ç¼–å·}_{ç‰©ç†é‡}_{åé¦ˆç±»å‹}.csv',
    'examples': [
        '_ä¸€æœŸ_ä¾›æ°´æ³µæˆ¿_1#åŠ å‹æ³µ_æœ‰åŠŸåŠŸç‡_åé¦ˆ.csv',
        '_äºŒæœŸ_ä¾›æ°´æ³µæˆ¿_1#åŠ å‹æ³µ_æŒ¯åŠ¨A_åé¦ˆ.csv',
        'äºŒæœŸ_ä¾›æ°´æ³µæˆ¿_å¤šåŠŸèƒ½ç”µè¡¨_1#åŠ å‹æ³µ_Aç›¸ç”µå‹_åé¦ˆ.csv'
    ]
}

# æ”¯æŒçš„ç‰©ç†é‡ç±»å‹
PHYSICAL_QUANTITIES = {
    'electrical': ['æœ‰åŠŸåŠŸç‡', 'Aç›¸æœ‰åŠŸåŠŸç‡', 'Bç›¸æœ‰åŠŸåŠŸç‡', 'Cç›¸æœ‰åŠŸåŠŸç‡',
                   'Ia', 'Ib', 'Ic', 'Ua', 'Ub', 'Uc', 'åŠŸç‡å› æ•°',
                   'æ­£å‘æœ‰åŠŸç”µèƒ½', 'æ— åŠŸåŠŸç‡', 'é¢‘ç‡'],
    'hydraulic': ['å‡ºå£å‹åŠ›', 'ç¬æ—¶æµé‡', 'ç´¯è®¡æµé‡', 'æ­£å‘ç´¯è®¡æµé‡'],
    'mechanical': ['æŒ¯åŠ¨A', 'æŒ¯åŠ¨B', 'æ¸©åº¦_é€šé“1', 'æ¸©åº¦_é€šé“2',
                   'æ¸©åº¦_é€šé“3', 'æ¸©åº¦_é€šé“4', 'æ¸©åº¦_é€šé“5',
                   'æ¸©åº¦_é€šé“6', 'æ¸©åº¦_é€šé“7', 'æ¸©åº¦_é€šé“8']
}
```

#### 1.2.2 æ•°æ®çº¦æŸæ¡ä»¶

```python
# åŸºäºå®é™…CSVæ–‡ä»¶æ ¼å¼çš„æ•°æ®çº¦æŸ
DATA_CONSTRAINTS = {
    'TagName': {'type': 'string', 'required': True, 'description': 'ä¼ æ„Ÿå™¨æ ‡è¯†ç¬¦'},
    'DataTime': {'type': 'datetime', 'required': True, 'format': 'YYYY-MM-DD HH:MM:SS'},
    'DataVersion': {'type': 'string', 'required': True, 'description': 'æ•°æ®ç‰ˆæœ¬æ ‡è¯†'},
    'DataQuality': {'type': 'string', 'required': True, 'description': 'æ•°æ®è´¨é‡æ ‡è¯†'},
    'DataValue': {'type': 'float', 'required': True, 'description': 'å®é™…æµ‹é‡å€¼'}
}

# ç‰©ç†é‡æ•°å€¼èŒƒå›´çº¦æŸ
PHYSICAL_VALUE_CONSTRAINTS = {
    'æœ‰åŠŸåŠŸç‡': {'min': 0, 'max': 5000, 'unit': 'kW'},
    'ç”µæµ': {'min': 0, 'max': 1000, 'unit': 'A'},
    'ç”µå‹': {'min': 0, 'max': 500, 'unit': 'V'},
    'å‹åŠ›': {'min': 0, 'max': 50, 'unit': 'bar'},
    'æµé‡': {'min': 0, 'max': 10000, 'unit': 'mÂ³/h'},
    'é¢‘ç‡': {'min': 0, 'max': 100, 'unit': 'Hz'},
    'æŒ¯åŠ¨': {'min': 0, 'max': 100, 'unit': 'mm/s'},
    'æ¸©åº¦': {'min': -50, 'max': 200, 'unit': 'Â°C'},
    'åŠŸç‡å› æ•°': {'min': 0, 'max': 1, 'unit': 'æ— é‡çº²'},
    'ç”µèƒ½': {'min': 0, 'max': 999999999, 'unit': 'kWh'}
}
```

______________________________________________________________________

## 2. ç‰¹æ€§æ›²çº¿æ‹ŸåˆæŠ€æœ¯è§„èŒƒ

### 2.1 æ‹Ÿåˆç›®æ ‡

æ ¹æ®éœ€æ±‚åˆ†æï¼Œç³»ç»Ÿéœ€è¦æ‹Ÿåˆä»¥ä¸‹ç‰¹æ€§æ›²çº¿ï¼š

#### 2.1.1 åŸºç¡€æ°´åŠ›ç‰¹æ€§æ›²çº¿

- **æ‰¬ç¨‹-æµé‡æ›²çº¿** (H-Q): H = f(Q)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.95
- **æ•ˆç‡-æµé‡æ›²çº¿** (Î·-Q): Î· = f(Q)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.92
- **åŠŸç‡-æµé‡æ›²çº¿** (P-Q): P = f(Q)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.90
- **NPSH-æµé‡æ›²çº¿**: NPSH = f(Q)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.85
- **æ‰­çŸ©-æµé‡æ›²çº¿** (T-Q): T = f(Q)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.88
- **è½´åŠŸç‡-è½¬é€Ÿæ›²çº¿** (P-n): P = f(n)ï¼Œç²¾åº¦è¦æ±‚ RÂ² â‰¥ 0.93

#### 2.1.2 ç”µæ°”ç‰¹æ€§æ›²çº¿

- **ç”µæµ-æµé‡æ›²çº¿** (I-Q): I = f(Q)ï¼Œè€ƒè™‘è´Ÿè½½å˜åŒ–
- **ç”µå‹-è´Ÿè½½æ›²çº¿** (U-Load): U = f(Load)ï¼Œç”µå‹é™åˆ†æ
- **åŠŸç‡å› æ•°-è´Ÿè½½æ›²çº¿** (cosÏ†-Load): cosÏ† = f(Load)
- **é¢‘ç‡-è½¬é€Ÿæ›²çº¿** (f-n): n = f(f)ï¼Œå˜é¢‘æ§åˆ¶å…³ç³»
- **ç”µèƒ½æ¶ˆè€—-æµé‡æ›²çº¿** (E-Q): E = f(Q)ï¼Œèƒ½è€—æ•ˆç‡åˆ†æ

#### 2.1.3 æŒ¯åŠ¨ä¸æ§åˆ¶ç‰¹æ€§æ›²çº¿

- **æŒ¯åŠ¨-è½¬é€Ÿæ›²çº¿** (V-n): V = f(n)ï¼ŒæŒ¯åŠ¨å¹…åº¦åˆ†æ
- **æŒ¯åŠ¨-æµé‡æ›²çº¿** (V-Q): V = f(Q)ï¼Œå·¥å†µç‚¹æŒ¯åŠ¨

#### 2.1.4 å˜é¢‘æ³µç‰¹æ€§æ›²çº¿

- **å˜é¢‘æ‰¬ç¨‹æ›²çº¿**: H = f(Q, n)ï¼Œä¸‰ç»´æ›²é¢æ‹Ÿåˆ
- **å˜é¢‘åŠŸç‡æ›²çº¿**: P = f(Q, n)ï¼ŒåŒ…å«å˜é¢‘å™¨æ•ˆç‡
- **å˜é¢‘æ•ˆç‡æ›²çº¿**: Î· = f(Q, n)ï¼Œä¿®æ­£æ¨¡å‹
- **å˜é¢‘è°æ³¢ç‰¹æ€§**: THD = f(frequency)

#### 2.1.5 è½¯å¯æ³µç‰¹æ€§æ›²çº¿

- **å¯åŠ¨ç‰¹æ€§æ›²çº¿**: è½¬çŸ©-æ—¶é—´ã€ç”µæµ-æ—¶é—´åŠ¨æ€ç‰¹æ€§
- **è¿è¡Œç‰¹æ€§æ›²çº¿**: æ ‡å‡†å•æ³µç‰¹æ€§
- **å¯åŠ¨ç”µæµæ›²çº¿**: I_start = f(t)ï¼Œè½¯å¯åŠ¨ç”µæµå˜åŒ–
- **å¯åŠ¨æ‰­çŸ©æ›²çº¿**: T_start = f(t)ï¼Œå¯åŠ¨è¿‡ç¨‹æ‰­çŸ©

#### 2.1.6 æ³µç»„ç‰¹æ€§æ›²çº¿

- **å¹¶è”è¿è¡Œæ›²çº¿**: H_group = f(Q_total, n_pumps)
- **ä¸²è”è¿è¡Œæ›²çº¿**: H_total = f(Q, n_pumps)
- **ç»„åˆæ•ˆç‡æ›²çº¿**: Î·_group = f(Q_total, combination)
- **è´Ÿè½½åˆ†é…ç‰¹æ€§**: Load_distribution = f(pump_combination)

### 2.2 æ‹Ÿåˆæ–¹æ³•ä½“ç³»

#### 2.2.1 ç‰©ç†æ‹Ÿåˆæ–¹æ³•

##### 2.2.1.1 æ¬§æ‹‰æ–¹ç¨‹æ‹Ÿåˆæ³•

```python
import numpy as np

class EulerEquationFitter:
    """
    åŸºäºæ¬§æ‹‰æ–¹ç¨‹çš„ç‰©ç†æ‹Ÿåˆå™¨
    """

    def __init__(self, impeller_diameter, rated_speed):
        self.D = impeller_diameter  # å¶è½®ç›´å¾„ (m)
        self.n_rated = rated_speed  # é¢å®šè½¬é€Ÿ (r/min)
        self.coefficients = None

    def fit(self, Q, H, n=None):
        """
        åŸºäºæ¬§æ‹‰æ–¹ç¨‹æ‹Ÿåˆ
        H_th = A * u2^2 / g - B * Q^2
        """
        if n is None:
            n = self.n_rated

        # è®¡ç®—å¶è½®å‡ºå£åœ†å‘¨é€Ÿåº¦
        u2 = np.pi * self.D * n / 60  # m/s

        # æ„å»ºæ‹ŸåˆçŸ©é˜µ
        # H = A * u2^2 / 9.81 - B * Q^2
        X = np.column_stack([np.ones(len(Q)) * u2**2 / 9.81, -Q**2])

        # æœ€å°äºŒä¹˜æ‹Ÿåˆ
        self.coefficients = np.linalg.lstsq(X, H, rcond=None)[0]

        return {
            'A': self.coefficients[0],  # å¶è½®æ•ˆç‡ç³»æ•°
            'B': self.coefficients[1],  # æµåŠ¨æŸå¤±ç³»æ•°
            'theoretical_head': self.coefficients[0] * u2**2 / 9.81
        }

    def predict(self, Q, n=None):
        """
        é¢„æµ‹æ‰¬ç¨‹
        """
        if n is None:
            n = self.n_rated

        u2 = np.pi * self.D * n / 60
        A, B = self.coefficients

        return A * u2**2 / 9.81 - B * Q**2
```

##### 2.2.1.2 ç›¸ä¼¼å®šå¾‹æ‹Ÿåˆæ³•

```python
import numpy as np

class SimilarityLawFitter:
    """
    åŸºäºç›¸ä¼¼å®šå¾‹çš„æ‹Ÿåˆå™¨
    """

    def __init__(self, rated_params):
        self.Q_rated = rated_params['flow']      # é¢å®šæµé‡
        self.H_rated = rated_params['head']      # é¢å®šæ‰¬ç¨‹
        self.n_rated = rated_params['speed']     # é¢å®šè½¬é€Ÿ
        self.base_curve = None

    def fit(self, Q, H, n):
        """
        åŸºäºç›¸ä¼¼å®šå¾‹æ‹Ÿåˆå¤šè½¬é€Ÿç‰¹æ€§
        """
        # æ— é‡çº²åŒ–å¤„ç†
        Q_norm = Q / (n / self.n_rated)
        H_norm = H / (n / self.n_rated)**2

        # æ‹Ÿåˆæ— é‡çº²ç‰¹æ€§æ›²çº¿
        self.base_curve = np.polyfit(Q_norm, H_norm, 2)

        return {
            'base_coefficients': self.base_curve,
            'similarity_law_valid': self._validate_similarity_law(Q, H, n)
        }

    def predict(self, Q, n):
        """
        åŸºäºç›¸ä¼¼å®šå¾‹é¢„æµ‹ä¸åŒè½¬é€Ÿä¸‹çš„æ‰¬ç¨‹
        """
        # è½¬æ¢åˆ°åŸºå‡†è½¬é€Ÿ
        Q_base = Q / (n / self.n_rated)
        H_base = np.polyval(self.base_curve, Q_base)

        # è½¬æ¢å›å®é™…è½¬é€Ÿ
        return H_base * (n / self.n_rated)**2

    def _validate_similarity_law(self, Q, H, n):
        """
        éªŒè¯ç›¸ä¼¼å®šå¾‹çš„é€‚ç”¨æ€§
        """
        predicted_H = self.predict(Q, n)
        r2 = 1 - np.sum((H - predicted_H)**2) / np.sum((H - np.mean(H))**2)
        return r2 > 0.95
```

##### 2.2.1.3 æ¯”è½¬é€Ÿæ‹Ÿåˆæ³•

```python
import numpy as np

class SpecificSpeedFitter:
    """
    åŸºäºæ¯”è½¬é€Ÿçš„æ‹Ÿåˆå™¨
    """

    def __init__(self):
        self.ns = None  # æ¯”è½¬é€Ÿ
        self.efficiency_model = None

    def fit(self, Q, H, P, n):
        """
        åŸºäºæ¯”è½¬é€Ÿæ‹Ÿåˆæ•ˆç‡ç‰¹æ€§
        """
        # è®¡ç®—æ¯”è½¬é€Ÿ
        Q_m3s = Q / 3600  # è½¬æ¢ä¸º mÂ³/s
        self.ns = n * np.sqrt(Q_m3s) / (H**(3/4))

        # è®¡ç®—æ•ˆç‡
        efficiency = (1000 * 9.81 * Q_m3s * H) / (P * 1000)  # æ°´åŠ›æ•ˆç‡

        # åŸºäºæ¯”è½¬é€Ÿçš„æ•ˆç‡æ¨¡å‹
        # Î· = Î·_max * f(Q/Q_opt)
        Q_opt = Q[np.argmax(efficiency)]
        Q_ratio = Q / Q_opt

        # æ‹Ÿåˆæ•ˆç‡æ›²çº¿
        self.efficiency_model = np.polyfit(Q_ratio, efficiency, 3)

        return {
            'specific_speed': np.mean(self.ns),
            'optimal_flow': Q_opt,
            'max_efficiency': np.max(efficiency),
            'pump_type': self._classify_pump_type(np.mean(self.ns))
        }

    def predict_efficiency(self, Q, Q_opt):
        """
        åŸºäºæ¯”è½¬é€Ÿé¢„æµ‹æ•ˆç‡
        """
        Q_ratio = Q / Q_opt
        return np.polyval(self.efficiency_model, Q_ratio)

    def _classify_pump_type(self, ns):
        """
        æ ¹æ®æ¯”è½¬é€Ÿåˆ†ç±»æ³µå‹
        """
        if ns < 150:
            return 'ç¦»å¿ƒæ³µ'
        elif ns < 500:
            return 'æ··æµæ³µ'
        else:
            return 'è½´æµæ³µ'
```

#### 2.2.2 ä¼ ç»Ÿæ•°å­¦æ–¹æ³•

##### 2.2.2.1 å¤šé¡¹å¼æ‹Ÿåˆæ³•

```python
import numpy as np

class PolynomialFitter:
    """
    å¤šé¡¹å¼æ‹Ÿåˆå™¨
    """

    def __init__(self, degree=2):
        self.degree = degree
        self.coefficients = None

    def fit(self, Q, H):
        """
        å¤šé¡¹å¼æ‹Ÿåˆ
        """
        # æ ‡å‡†äºŒæ¬¡å¤šé¡¹å¼ï¼šH = a*QÂ² + b*Q + c
        self.coefficients = np.polyfit(Q, H, self.degree)
        return self.coefficients

    def predict(self, Q):
        """
        é¢„æµ‹æ‰¬ç¨‹
        """
        return np.polyval(self.coefficients, Q)

    def get_optimal_flow(self):
        """
        è·å–æœ€ä¼˜æµé‡ç‚¹ï¼ˆæ•ˆç‡æœ€é«˜ç‚¹ï¼‰
        """
        if self.degree == 2:
            a, b, c = self.coefficients
            Q_opt = -b / (2 * a)
            return Q_opt
        return None

# åˆ†æ®µå¤šé¡¹å¼æ‹Ÿåˆ
import numpy as np

class PiecewisePolynomialFitter:
    """
    åˆ†æ®µå¤šé¡¹å¼æ‹Ÿåˆå™¨
    """

    def __init__(self, breakpoints=None):
        self.breakpoints = breakpoints
        self.segment_fitters = []

    def fit(self, Q, H):
        """
        åˆ†æ®µæ‹Ÿåˆ
        """
        if self.breakpoints is None:
            # è‡ªåŠ¨ç¡®å®šåˆ†æ®µç‚¹
            self.breakpoints = self._find_optimal_breakpoints(Q, H)

        # ä¸ºæ¯ä¸ªæ®µæ‹Ÿåˆå¤šé¡¹å¼
        for i in range(len(self.breakpoints) + 1):
            if i == 0:
                mask = Q <= self.breakpoints[0]
            elif i == len(self.breakpoints):
                mask = Q >= self.breakpoints[-1]
            else:
                mask = (Q >= self.breakpoints[i-1]) & (Q <= self.breakpoints[i])

            if np.sum(mask) > 2:
                fitter = PolynomialFitter(degree=2)
                fitter.fit(Q[mask], H[mask])
                self.segment_fitters.append(fitter)
```

##### 2.2.2.2 æ ·æ¡æ’å€¼æ³•

```python
from scipy.interpolate import CubicSpline, UnivariateSpline, BSpline

class SplineFitter:
    """
    æ ·æ¡æ’å€¼æ‹Ÿåˆå™¨
    """

    def __init__(self, spline_type='cubic'):
        self.spline_type = spline_type
        self.spline_func = None

    def fit(self, Q, H, smoothing=None):
        """
        æ ·æ¡æ‹Ÿåˆ
        """
        if self.spline_type == 'cubic':
            # ä¸‰æ¬¡æ ·æ¡æ’å€¼
            self.spline_func = CubicSpline(Q, H, bc_type='natural')
        elif self.spline_type == 'univariate':
            # å•å˜é‡æ ·æ¡ï¼ˆå¯è°ƒèŠ‚å¹³æ»‘åº¦ï¼‰
            s = smoothing if smoothing else len(Q)
            self.spline_func = UnivariateSpline(Q, H, s=s)
        elif self.spline_type == 'bspline':
            # Bæ ·æ¡
            from scipy.interpolate import splrep, BSpline
            tck = splrep(Q, H, s=smoothing)
            self.spline_func = BSpline(*tck)

    def predict(self, Q):
        """
        é¢„æµ‹æ‰¬ç¨‹
        """
        return self.spline_func(Q)

    def get_derivative(self, Q, order=1):
        """
        è·å–å¯¼æ•°ï¼ˆç”¨äºåˆ†ææ›²çº¿ç‰¹æ€§ï¼‰
        """
        return self.spline_func.derivative(order)(Q)
```

______________________________________________________________________

## 3. æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ§åˆ¶

### 3.1 æ•°æ®æ¸…æ´—è§„èŒƒ

#### 3.1.1 å¼‚å¸¸å€¼æ£€æµ‹

```python
import numpy as np

class OutlierDetector:
    """
    å¼‚å¸¸å€¼æ£€æµ‹å™¨
    """

    def __init__(self, method='iqr'):
        self.method = method
        self.fitted_params = {}

    def detect_outliers(self, data, column):
        """
        æ£€æµ‹å¼‚å¸¸å€¼
        """
        if self.method == 'iqr':
            return self._iqr_outliers(data[column])
        elif self.method == 'zscore':
            return self._zscore_outliers(data[column])
        elif self.method == 'isolation_forest':
            return self._isolation_forest_outliers(data[column])

    def _iqr_outliers(self, series):
        """
        åŸºäºå››åˆ†ä½è·çš„å¼‚å¸¸å€¼æ£€æµ‹
        """
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        return (series < lower_bound) | (series > upper_bound)

    def _zscore_outliers(self, series, threshold=3):
        """
        åŸºäºZ-scoreçš„å¼‚å¸¸å€¼æ£€æµ‹
        """
        import numpy as np
        z_scores = np.abs((series - series.mean()) / series.std())
        return z_scores > threshold

    def _isolation_forest_outliers(self, series):
        """
        åŸºäºå­¤ç«‹æ£®æ—çš„å¼‚å¸¸å€¼æ£€æµ‹
        """
        from sklearn.ensemble import IsolationForest

        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        outliers = iso_forest.fit_predict(series.values.reshape(-1, 1))

        return outliers == -1
```

#### 3.1.2 ç¼ºå¤±å€¼å¤„ç†

```python
class MissingValueHandler:
    """
    ç¼ºå¤±å€¼å¤„ç†å™¨
    """

    def __init__(self):
        self.imputation_strategy = {}

    def handle_missing_values(self, data, strategy='interpolation'):
        """
        å¤„ç†ç¼ºå¤±å€¼
        """
        if strategy == 'interpolation':
            return self._interpolation_imputation(data)
        elif strategy == 'forward_fill':
            return data.fillna(method='ffill')
        elif strategy == 'backward_fill':
            return data.fillna(method='bfill')
        elif strategy == 'mean':
            return data.fillna(data.mean())
        elif strategy == 'median':
            return data.fillna(data.median())

    def _interpolation_imputation(self, data):
        """
        åŸºäºæ’å€¼çš„ç¼ºå¤±å€¼å¡«å……
        """
        # å¯¹äºç‰¹æ€§æ›²çº¿æ•°æ®ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
        for column in ['Q', 'H', 'P', 'eta']:
            if column in data.columns:
                data[column] = data[column].interpolate(method='linear')

        return data
```

### 3.2 æ•°æ®éªŒè¯è§„èŒƒ

#### 3.2.1 ç‰©ç†çº¦æŸéªŒè¯

```python
import numpy as np

class PhysicalConstraintValidator:
    """
    ç‰©ç†çº¦æŸéªŒè¯å™¨
    """

    def __init__(self):
        self.constraints = {
            'Q': {'min': 0, 'max': 10000, 'type': 'positive'},
            'H': {'min': 0, 'max': 1000, 'type': 'positive'},
            'P': {'min': 0, 'max': 5000, 'type': 'positive'},
            'eta': {'min': 0, 'max': 100, 'type': 'percentage'},
            'n': {'min': 0, 'max': 10000, 'type': 'positive'}
        }

    def validate_constraints(self, data):
        """
        éªŒè¯ç‰©ç†çº¦æŸ
        """
        violations = {}

        for column, constraint in self.constraints.items():
            if column in data.columns:
                # èŒƒå›´æ£€æŸ¥
                out_of_range = (data[column] < constraint['min']) | \
                              (data[column] > constraint['max'])

                if out_of_range.any():
                    violations[column] = {
                        'type': 'range_violation',
                        'count': out_of_range.sum(),
                        'indices': data[out_of_range].index.tolist()
                    }

                # ç‰©ç†åˆç†æ€§æ£€æŸ¥
                if constraint['type'] == 'positive':
                    negative_values = data[column] < 0
                    if negative_values.any():
                        violations[f'{column}_negative'] = {
                            'type': 'physical_violation',
                            'count': negative_values.sum(),
                            'indices': data[negative_values].index.tolist()
                        }

        return violations

    def validate_curve_monotonicity(self, Q, H):
        """
        éªŒè¯æ‰¬ç¨‹æ›²çº¿çš„å•è°ƒæ€§
        """
        import numpy as np
        # æ‰¬ç¨‹æ›²çº¿åº”è¯¥éšæµé‡å¢åŠ è€Œä¸‹é™
        dH_dQ = np.diff(H) / np.diff(Q)

        # å…è®¸å°å¹…ä¸Šå‡ï¼ˆè€ƒè™‘æµ‹é‡è¯¯å·®ï¼‰
        tolerance = 0.05  # 5%å®¹å·®
        non_monotonic = dH_dQ > tolerance * np.mean(H)

        return {
            'is_monotonic': not non_monotonic.any(),
            'violation_count': non_monotonic.sum(),
            'violation_points': np.where(non_monotonic)[0]
        }
```

### 3.3 æ•°æ®æ ‡å‡†åŒ–

#### 3.3.1 å•ä½è½¬æ¢

```python
class UnitConverter:
    """
    å•ä½è½¬æ¢å™¨
    """

    def __init__(self):
        self.conversion_factors = {
            'flow': {
                'mÂ³/h': 1.0,
                'mÂ³/s': 3600.0,
                'L/min': 1/60.0,
                'L/s': 1.0,
                'gpm': 0.227125  # ç¾åˆ¶åŠ ä»‘/åˆ†é’Ÿ
            },
            'head': {
                'm': 1.0,
                'ft': 0.3048,
                'mbar': 0.0102,
                'psi': 0.7031
            },
            'power': {
                'kW': 1.0,
                'hp': 0.746,
                'W': 0.001
            }
        }

    def convert_units(self, data, unit_mapping):
        """
        è½¬æ¢å•ä½
        unit_mapping: {'column': {'from': 'unit1', 'to': 'unit2'}}
        """
        converted_data = data.copy()

        for column, units in unit_mapping.items():
            if column in data.columns:
                from_unit = units['from']
                to_unit = units['to']

                # è·å–è½¬æ¢ç³»æ•°
                if column in ['Q', 'flow']:
                    factor = self.conversion_factors['flow'][from_unit] / \
                            self.conversion_factors['flow'][to_unit]
                elif column in ['H', 'head']:
                    factor = self.conversion_factors['head'][from_unit] / \
                            self.conversion_factors['head'][to_unit]
                elif column in ['P', 'power']:
                    factor = self.conversion_factors['power'][from_unit] / \
                            self.conversion_factors['power'][to_unit]

                converted_data[column] = data[column] * factor

        return converted_data
```

#### 3.3.2 æ•°æ®å½’ä¸€åŒ–

```python
class DataNormalizer:
    """
    æ•°æ®å½’ä¸€åŒ–å™¨
    """

    def __init__(self, method='minmax'):
        self.method = method
        self.scaler_params = {}

    def fit_transform(self, data, columns=None):
        """
        æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®
        """
        if columns is None:
            columns = data.select_dtypes(include=[np.number]).columns

        normalized_data = data.copy()

        for column in columns:
            if self.method == 'minmax':
                min_val = data[column].min()
                max_val = data[column].max()

                self.scaler_params[column] = {'min': min_val, 'max': max_val}
                normalized_data[column] = (data[column] - min_val) / (max_val - min_val)

            elif self.method == 'zscore':
                mean_val = data[column].mean()
                std_val = data[column].std()

                self.scaler_params[column] = {'mean': mean_val, 'std': std_val}
                normalized_data[column] = (data[column] - mean_val) / std_val

        return normalized_data

    def inverse_transform(self, normalized_data, columns=None):
        """
        åå‘è½¬æ¢æ•°æ®
        """
        if columns is None:
            columns = self.scaler_params.keys()

        original_data = normalized_data.copy()

        for column in columns:
            if column in self.scaler_params:
                params = self.scaler_params[column]

                if self.method == 'minmax':
                    original_data[column] = normalized_data[column] * \
                                          (params['max'] - params['min']) + params['min']
                elif self.method == 'zscore':
                    original_data[column] = normalized_data[column] * \
                                          params['std'] + params['mean']

        return original_data
```

______________________________________________________________________

## 4. æ•°æ®è¡¥é½æŠ€æœ¯è§„èŒƒ

### 4.1 æ’å€¼è¡¥é½æ–¹æ³•

#### 4.1.1 çº¿æ€§æ’å€¼

```python
class LinearInterpolator:
    """
    çº¿æ€§æ’å€¼å™¨
    """

    def interpolate(self, Q_known, H_known, Q_target):
        """
        çº¿æ€§æ’å€¼
        """
        from scipy.interpolate import interp1d

        # åˆ›å»ºæ’å€¼å‡½æ•°
        f = interp1d(Q_known, H_known, kind='linear',
                    bounds_error=False, fill_value='extrapolate')

        return f(Q_target)
```

#### 4.1.2 æ ·æ¡æ’å€¼

```python
class SplineInterpolator:
    """
    æ ·æ¡æ’å€¼å™¨
    """

    def interpolate(self, Q_known, H_known, Q_target, order=3):
        """
        æ ·æ¡æ’å€¼
        """
        from scipy.interpolate import UnivariateSpline

        # åˆ›å»ºæ ·æ¡å‡½æ•°
        spline = UnivariateSpline(Q_known, H_known, k=order, s=0)

        return spline(Q_target)
```

### 4.2 åŸºäºç‰©ç†æ¨¡å‹çš„è¡¥é½

### 4.3 æœºå™¨å­¦ä¹ è¡¥é½æ–¹æ³•

#### 4.3.1 å›å½’æ¨¡å‹è¡¥é½

```python
class MLBasedFiller:
    """
    åŸºäºæœºå™¨å­¦ä¹ çš„æ•°æ®è¡¥é½å™¨
    """

    def __init__(self, model_type='random_forest'):
        self.model_type = model_type
        self.model = None

    def fit_and_fill(self, data, target_column, feature_columns):
        """
        è®­ç»ƒæ¨¡å‹å¹¶è¡¥é½æ•°æ®
        """
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LinearRegression

        # åˆ†ç¦»å·²çŸ¥å’ŒæœªçŸ¥æ•°æ®
        known_mask = data[target_column].notna()
        X_known = data.loc[known_mask, feature_columns]
        y_known = data.loc[known_mask, target_column]
        X_unknown = data.loc[~known_mask, feature_columns]

        # é€‰æ‹©æ¨¡å‹
        if self.model_type == 'random_forest':
            self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        elif self.model_type == 'linear':
            self.model = LinearRegression()

        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X_known, y_known)

        # é¢„æµ‹ç¼ºå¤±å€¼
        y_predicted = self.model.predict(X_unknown)

        # å¡«å……æ•°æ®
        filled_data = data.copy()
        filled_data.loc[~known_mask, target_column] = y_predicted

        return filled_data
```

______________________________________________________________________

## 5. æ³µé¢å®šå‚æ•°è®¡ç®—è§„èŒƒ

### 5.1 é¢å®šå·¥å†µç‚¹ç¡®å®š

#### 5.1.1 æœ€ä¼˜æ•ˆç‡ç‚¹è®¡ç®—

```python
class RatedPointCalculator:
    """
    é¢å®šå·¥å†µç‚¹è®¡ç®—å™¨
    """

    def find_optimal_efficiency_point(self, Q_data, eta_data):
        """
        æ‰¾åˆ°æœ€ä¼˜æ•ˆç‡ç‚¹
        """
        max_eta_idx = np.argmax(eta_data)
        Q_opt = Q_data[max_eta_idx]
        eta_opt = eta_data[max_eta_idx]

        return {
            'Q_opt': Q_opt,
            'eta_opt': eta_opt,
            'index': max_eta_idx
        }

    def calculate_rated_parameters(self, pump_data):
        """
        è®¡ç®—é¢å®šå‚æ•°
        """
        # æ‰¾åˆ°æœ€ä¼˜æ•ˆç‡ç‚¹
        opt_point = self.find_optimal_efficiency_point(
            pump_data['Q'], pump_data['eta']
        )

        Q_rated = opt_point['Q_opt']
        idx = opt_point['index']

        rated_params = {
            'Q_rated': Q_rated,
            'H_rated': pump_data['H'].iloc[idx],
            'P_rated': pump_data['P'].iloc[idx],
            'eta_rated': opt_point['eta_opt'],
            'n_rated': pump_data['n'].iloc[idx]
        }

        return rated_params
```

### 5.2 æ€§èƒ½æ›²çº¿ç‰¹å¾ç‚¹è®¡ç®—

#### 5.2.1 å…³é”®å·¥å†µç‚¹è¯†åˆ«

```python
class PerformancePointCalculator:
    """
    æ€§èƒ½ç‰¹å¾ç‚¹è®¡ç®—å™¨
    """

    def calculate_characteristic_points(self, Q_data, H_data, P_data, eta_data):
        """
        è®¡ç®—ç‰¹æ€§æ›²çº¿å…³é”®ç‚¹
        """
        characteristic_points = {}

        # 1. æœ€å¤§æ‰¬ç¨‹ç‚¹ï¼ˆQ=0æ—¶çš„æ‰¬ç¨‹ï¼‰
        H_max_idx = np.argmin(Q_data)  # æœ€å°æµé‡ç‚¹
        characteristic_points['shutoff'] = {
            'Q': Q_data[H_max_idx],
            'H': H_data[H_max_idx],
            'description': 'å…³é—­ç‚¹æ‰¬ç¨‹'
        }

        # 2. æœ€å¤§æ•ˆç‡ç‚¹
        eta_max_idx = np.argmax(eta_data)
        characteristic_points['BEP'] = {  # Best Efficiency Point
            'Q': Q_data[eta_max_idx],
            'H': H_data[eta_max_idx],
            'P': P_data[eta_max_idx],
            'eta': eta_data[eta_max_idx],
            'description': 'æœ€ä¼˜æ•ˆç‡ç‚¹'
        }

        # 3. è®¾è®¡ç‚¹ï¼ˆé€šå¸¸ä¸ºæœ€ä¼˜æ•ˆç‡ç‚¹çš„80-120%æµé‡èŒƒå›´ï¼‰
        Q_bep = Q_data[eta_max_idx]
        design_range_mask = (Q_data >= 0.8 * Q_bep) & (Q_data <= 1.2 * Q_bep)

        if design_range_mask.any():
            design_eta_avg = eta_data[design_range_mask].mean()
            characteristic_points['design_range'] = {
                'Q_min': 0.8 * Q_bep,
                'Q_max': 1.2 * Q_bep,
                'eta_avg': design_eta_avg,
                'description': 'è®¾è®¡èŒƒå›´'
            }

        # 4. æœ€å¤§æµé‡ç‚¹
        Q_max_idx = np.argmax(Q_data)
        characteristic_points['max_flow'] = {
            'Q': Q_data[Q_max_idx],
            'H': H_data[Q_max_idx],
            'eta': eta_data[Q_max_idx],
            'description': 'æœ€å¤§æµé‡ç‚¹'
        }

        return characteristic_points
```

______________________________________________________________________

## 6. æ•°æ®æ‰©å±•é€‚åº”æ€§è®¾è®¡

### 6.1 æ–°è®¾å¤‡ç±»å‹é€‚é…

### 6.2 åŠ¨æ€æ‹Ÿåˆå‚æ•°è°ƒæ•´

#### 6.2.1 è‡ªé€‚åº”æ‹Ÿåˆç­–ç•¥

```python
class AdaptiveFittingStrategy:
    """
    è‡ªé€‚åº”æ‹Ÿåˆç­–ç•¥
    """

    def __init__(self):
        self.fitting_methods = [
            'polynomial',
            'spline',
            'physical_model',
            'hybrid'
        ]
        self.performance_threshold = 0.95  # RÂ²é˜ˆå€¼

    def select_best_method(self, Q_data, H_data, pump_type=None):
        """
        é€‰æ‹©æœ€ä½³æ‹Ÿåˆæ–¹æ³•
        """
        best_method = None
        best_score = 0
        best_params = None

        for method in self.fitting_methods:
            try:
                score, params = self._evaluate_method(method, Q_data, H_data, pump_type)

                if score > best_score:
                    best_score = score
                    best_method = method
                    best_params = params

            except Exception as e:
                logger.warning(f"æ‹Ÿåˆæ–¹æ³• {method} å¤±è´¥: {e}")
                continue

        return {
            'method': best_method,
            'score': best_score,
            'parameters': best_params,
            'meets_threshold': best_score >= self.performance_threshold
        }

    def _evaluate_method(self, method, Q_data, H_data, pump_type):
        """
        è¯„ä¼°æ‹Ÿåˆæ–¹æ³•æ€§èƒ½
        """
        if method == 'polynomial':
            return self._evaluate_polynomial(Q_data, H_data)
        elif method == 'spline':
            return self._evaluate_spline(Q_data, H_data)
        elif method == 'physical_model':
            return self._evaluate_physical_model(Q_data, H_data, pump_type)
        elif method == 'hybrid':
            return self._evaluate_hybrid(Q_data, H_data, pump_type)

    def _evaluate_polynomial(self, Q_data, H_data):
        """
        è¯„ä¼°å¤šé¡¹å¼æ‹Ÿåˆ
        """
        import numpy as np
        from sklearn.metrics import r2_score

        # å°è¯•ä¸åŒé˜¶æ•°
        best_score = 0
        best_degree = 2

        for degree in [2, 3, 4]:
            coeffs = np.polyfit(Q_data, H_data, degree)
            H_pred = np.polyval(coeffs, Q_data)
            score = r2_score(H_data, H_pred)

            if score > best_score:
                best_score = score
                best_degree = degree

        return best_score, {'degree': best_degree}
```

### 6.3 æ•°æ®æ ¼å¼æ‰©å±•æ”¯æŒ

#### 6.3.1 å¤šæ ¼å¼æ•°æ®è§£æå™¨

```python
class DataFormatParser:
    """
    å¤šæ ¼å¼æ•°æ®è§£æå™¨
    """

    def __init__(self):
        self.supported_formats = ['csv', 'excel', 'json', 'xml', 'database']
        self.column_mappings = {}

    def parse_data(self, file_path, format_type=None):
        """
        è§£æå¤šç§æ ¼å¼çš„æ•°æ®æ–‡ä»¶
        """
        if format_type is None:
            format_type = self._detect_format(file_path)

        if format_type == 'csv':
            return self._parse_csv(file_path)
        elif format_type == 'excel':
            return self._parse_excel(file_path)
        elif format_type == 'json':
            return self._parse_json(file_path)
        elif format_type == 'xml':
            return self._parse_xml(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {format_type}")

    def _detect_format(self, file_path):
        """
        è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼
        """
        file_extension = file_path.split('.')[-1].lower()

        format_map = {
            'csv': 'csv',
            'xlsx': 'excel',
            'xls': 'excel',
            'json': 'json',
            'xml': 'xml'
        }

        return format_map.get(file_extension, 'unknown')

    def _parse_csv(self, file_path):
        """
        è§£æCSVæ–‡ä»¶
        """
        import pandas as pd  # å±€éƒ¨å¯¼å…¥ï¼Œé¿å…æœªä½¿ç”¨æ—¶çš„ç¯å¢ƒä¾èµ–
        # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦å’Œç¼–ç 
        separators = [',', ';', '\t']
        encodings = ['utf-8', 'gbk', 'latin-1']

        for sep in separators:
            for encoding in encodings:
                try:
                    data = pd.read_csv(file_path, sep=sep, encoding=encoding)
                    if len(data.columns) > 1:  # æˆåŠŸè§£æ
                        return self._standardize_columns(data)
                except:
                    continue

        raise ValueError("æ— æ³•è§£æCSVæ–‡ä»¶")

    def _standardize_columns(self, data):
        """
        æ ‡å‡†åŒ–åˆ—å
        """
        column_aliases = {
            'flow': ['Q', 'Flow', 'æµé‡', 'flow_rate'],
            'head': ['H', 'Head', 'æ‰¬ç¨‹', 'head'],
            'power': ['P', 'Power', 'åŠŸç‡', 'power'],
            'efficiency': ['eta', 'Efficiency', 'æ•ˆç‡', 'eff'],
            'speed': ['n', 'Speed', 'è½¬é€Ÿ', 'rpm']
        }

        standardized_data = data.copy()

        for standard_name, aliases in column_aliases.items():
            for alias in aliases:
                if alias in data.columns:
                    standardized_data.rename(columns={alias: standard_name}, inplace=True)
                    break

        return standardized_data
```

______________________________________________________________________

## æ€»ç»“

æœ¬æ–‡æ¡£å®šä¹‰äº†æ°´æ³µæ•°æ®å¤„ç†çš„å®Œæ•´æŠ€æœ¯è§„èŒƒï¼ŒåŒ…æ‹¬ï¼š

1. **æ•°æ®ç»“æ„åˆ†æ**: æ˜ç¡®äº†è®¾å¤‡åˆ†ç±»å’Œæ•°æ®æ ¼å¼è¦æ±‚
1. **ç‰¹æ€§æ›²çº¿æ‹Ÿåˆ**: æä¾›äº†ç‰©ç†æ¨¡å‹å’Œæ•°å­¦æ–¹æ³•çš„å®Œæ•´æ‹Ÿåˆä½“ç³»
1. **æ•°æ®é¢„å¤„ç†**: å»ºç«‹äº†æ•°æ®æ¸…æ´—ã€éªŒè¯å’Œæ ‡å‡†åŒ–æµç¨‹
1. **æ•°æ®è¡¥é½**: å®šä¹‰äº†å¤šç§æ’å€¼å’Œæ¨¡å‹è¡¥é½æ–¹æ³•
1. **é¢å®šå‚æ•°è®¡ç®—**: è§„èŒƒäº†å…³é”®æ€§èƒ½ç‚¹çš„è¯†åˆ«å’Œè®¡ç®—
1. **æ‰©å±•é€‚åº”æ€§**: æä¾›äº†æ–°è®¾å¤‡ç±»å‹å’Œæ•°æ®æ ¼å¼çš„é€‚é…æœºåˆ¶

è¿™äº›è§„èŒƒç¡®ä¿äº†ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å„ç§ç±»å‹çš„æ°´æ³µæ•°æ®ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„ç‰¹æ€§æ›²çº¿æ‹Ÿåˆç»“æœã€‚
