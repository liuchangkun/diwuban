# 泵站数据优化系统 - 开发规范与标准

## 🚀 如何使用本规范

### 新入职开发者快速启动

1. **环境配置** → [📋 版本与环境规范](#-%E7%89%88%E6%9C%AC%E4%B8%8E%E7%8E%AF%E5%A2%83%E8%A7%84%E8%8C%83)：配置Python 3.13.6环境，安装依赖
1. **代码风格** → [🎨 代码风格规范](#-%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A7%84%E8%8C%83)：配置ruff、mypy，学习命名约定
1. **项目结构** → [🏗️ 项目结构规范](#%EF%B8%8F-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E8%A7%84%E8%8C%83)：了解目录组织，找到开发入口
1. **测试实践** → [🧪 测试规范](#-%E6%B5%8B%E8%AF%95%E8%A7%84%E8%8C%83)：编写单元测试，运行质量检查

### 日常开发检查清单

- [ ] 功能开发前：同步依赖 → 运行质量检查 → 创建功能分支
- [ ] 编码过程中：遵循命名规范 → 添加类型注解 → 编写文档字符串
- [ ] 提交前检查：运行测试 → 代码格式化 → 类型检查通过
- [ ] 代码审查：单一职责 → 异常处理 → 性能考虑

### 常见问题快速导航

- **配置问题** → [⚙️ 配置管理](#%EF%B8%8F-%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86)
- **日志规范** → [📊 日志规范](#-%E6%97%A5%E5%BF%97%E8%A7%84%E8%8C%83)
- **错误处理** → [⚠️ 错误处理规范](#%EF%B8%8F-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86%E8%A7%84%E8%8C%83)
- **性能优化** → [⚡ 性能规范](#-%E6%80%A7%E8%83%BD%E8%A7%84%E8%8C%83)

______________________________________________________________________

## 🎯 概述

本文档定义了泵站数据优化系统开发过程中必须遵循的编码规范、工程实践和质量标准，确保代码的可维护性、可扩展性和高质量。

## 📋 版本与环境规范

### Python版本

- **主版本**：Python 3.13.6
- **虚拟环境**：使用项目级 `.venv` 目录
- **依赖管理**：使用 `pyproject.toml` 统一管理

### 依赖管理规范

```toml
# pyproject.toml 示例
[project]
name = "pump-station-optimizer"
version = "0.1.0"
description = "泵站运行数据优化系统"
requires-python = ">=3.13"
dependencies = [
    "fastapi>=0.104.0,<0.105.0",
    "sqlalchemy>=2.0.0,<2.1.0",
    "pandas>=2.1.0,<2.2.0",
    "pydantic>=2.5.0,<2.6.0",
    "asyncpg>=0.29.0,<0.30.0",
]

[project.optional-dependencies]
dev = [
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
]
```

### 开发前检查清单

- [ ] 同步最新依赖：`pip install -e .[dev]`
- [ ] 运行代码检查：`ruff check .`
- [ ] 运行类型检查：`mypy .`
- [ ] 运行测试套件：`pytest`

## 🎨 代码风格规范

### 基础规范

- **遵循 PEP 8**：使用 ruff 自动格式化
- **行长度**：最大 88 字符
- **缩进**：4个空格，禁用制表符
- **编码**：UTF-8，文件头声明 `# -*- coding: utf-8 -*-`

### 代码组织与架构原则

- **单一职责原则**：每个函数不超过 50 行，每个类不超过 500 行
- **模块聚焦**：每个模块专注一个业务领域，公共工具集中到 `utils` 子包
- **依赖倒置**：高层模块依赖抽象接口，不依赖具体实现
- **接口隔离**：定义最小化的接口，避免臃肿的接口
- **分层架构**：明确区分数据层、业务层、表示层

### 命名规范

```python
# 模块名：小写+下划线
data_processor.py
config_manager.py

# 类名：大驼峰
class DataProcessor:
class ConfigManager:

# 函数名：小写+下划线
def process_csv_data():
def validate_config():

# 变量名：小写+下划线
user_data = {}
max_retry_count = 3

# 常量：全大写+下划线
MAX_CHUNK_SIZE = 10000
DEFAULT_TIMEOUT = 30

# 私有属性：单下划线前缀
class DataProcessor:
    def __init__(self):
        self._connection = None
        self._config = {}
```

### 文档字符串规范

```python
def process_pump_data(
    data_file: Path,
    equipment_id: int,
    chunk_size: int = 10000
) -> ProcessResult:
    """
    处理泵站CSV数据文件并导入数据库。

    该函数负责读取CSV文件，进行数据验证和清洗，然后批量导入到数据库中。
    支持大文件分块处理，确保内存使用效率。

    参数:
        data_file: CSV数据文件路径
        equipment_id: 设备ID，用于关联数据库记录
        chunk_size: 分块大小，默认10000行

    返回:
        ProcessResult: 包含处理结果的对象，包括成功行数、失败行数等

    异常:
        FileNotFoundError: 当数据文件不存在时抛出
        ValidationError: 当数据验证失败时抛出
        DatabaseError: 当数据库操作失败时抛出

    示例:
        >>> import logging
        >>> logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        >>> logger = logging.getLogger(__name__)
        >>> result = process_pump_data(
        ...     Path("data/pump_data.csv"),
        ...     equipment_id=1,
        ...     chunk_size=5000
        ... )
        >>> logger.info(f"成功处理 {result.success_count} 行数据")
    """
    pass
```

## 🔧 类型注解规范

### 基本原则

- **所有公共函数**必须有完整类型注解
- **重要内部函数**建议添加类型注解
- **避免使用 Any**，尽量使用精确类型
- **使用泛型**提高类型安全性

### 类型注解示例

```python
from typing import Dict, List, Optional, Union, Protocol
from pathlib import Path
from decimal import Decimal
from datetime import datetime

# 基础类型注解
def get_equipment_count(station_id: int) -> int:
    pass

# 容器类型注解
def get_sensor_data(equipment_ids: List[int]) -> Dict[int, List[float]]:
    pass

# 可选类型
def find_equipment(name: str) -> Optional[Equipment]:
    pass

# 联合类型
def parse_value(raw_value: str) -> Union[int, float, str]:
    pass

# 协议类型
class DataProcessor(Protocol):
    def process(self, data: pd.DataFrame) -> ProcessResult:
        ...

# 泛型类型
from typing import TypeVar, Generic

T = TypeVar('T')

class Repository(Generic[T]):
    def save(self, entity: T) -> T:
        pass

    def find_by_id(self, id: int) -> Optional[T]:
        pass
```

### mypy配置

```ini
# mypy.ini
[mypy]
python_version = 3.13
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
warn_unreachable = True
strict_equality = True
```

## 🚨 错误处理规范

### 异常处理原则

- **禁止裸 except**：必须捕获具体异常类型
- **异常链**：使用 `raise ... from ...` 保留原始异常
- **上下文信息**：异常中包含关键参数和状态
- **日志记录**：所有异常都要记录到日志

### 自定义异常体系

```python
# exceptions.py
class PumpStationError(Exception):
    """泵站系统基础异常类"""
    pass

class ConfigurationError(PumpStationError):
    """配置相关异常"""
    pass

class DataValidationError(PumpStationError):
    """数据验证异常"""
    def __init__(self, message: str, invalid_rows: List[int]):
        super().__init__(message)
        self.invalid_rows = invalid_rows

class DatabaseError(PumpStationError):
    """数据库操作异常"""
    pass

class OptimizationError(PumpStationError):
    """优化算法异常"""
    pass
```

### 异常处理示例

```python
import logging
from typing import Optional

logger = logging.getLogger(__name__)

def import_csv_data(file_path: Path, equipment_id: int) -> ImportResult:
    """
    导入CSV数据，包含完整的异常处理。
    """
    try:
        # 验证文件存在
        if not file_path.exists():
            raise FileNotFoundError(f"数据文件不存在: {file_path}")

        # 读取和处理数据
        data = pd.read_csv(file_path, encoding='utf-8')
        validated_data = validate_sensor_data(data)

        # 导入数据库
        result = save_to_database(validated_data, equipment_id)

        logger.info(
            "CSV数据导入成功",
            extra={
                "file_path": str(file_path),
                "equipment_id": equipment_id,
                "rows_imported": result.success_count
            }
        )
        return result

    except FileNotFoundError as e:
        logger.error(
            "文件不存在错误",
            extra={"file_path": str(file_path), "error": str(e)}
        )
        raise ConfigurationError(f"无法找到数据文件: {file_path}") from e

    except pd.errors.EmptyDataError as e:
        logger.error(
            "CSV文件为空",
            extra={"file_path": str(file_path)}
        )
        raise DataValidationError("CSV文件不包含有效数据") from e

    except ValidationError as e:
        logger.error(
            "数据验证失败",
            extra={
                "file_path": str(file_path),
                "validation_errors": e.errors()
            }
        )
        raise DataValidationError(f"数据验证失败: {e}") from e

    except Exception as e:
        logger.error(
            "CSV导入未知错误",
            extra={
                "file_path": str(file_path),
                "equipment_id": equipment_id,
                "error_type": type(e).__name__,
                "error_message": str(e)
            },
            exc_info=True
        )
        raise DatabaseError(f"数据导入失败: {e}") from e
```

## 📊 日志规范

### 日志配置

```python
# logging_config.py
import logging.config
from pathlib import Path

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(pathname)s:%(lineno)d',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '%(levelname)s - %(message)s'
        },
        'json': {
            'format': '%(asctime)s %(name)s %(levelname)s %(message)s',
            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'simple',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'DEBUG',
            'formatter': 'detailed',
            'filename': 'logs/app.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        },
        'error_file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'ERROR',
            'formatter': 'json',
            'filename': 'logs/error.log',
            'maxBytes': 10485760,
            'backupCount': 10
        }
    },
    'loggers': {
        'pump_station': {
            'level': 'DEBUG',
            'handlers': ['console', 'file', 'error_file'],
            'propagate': False
        }
    },
    'root': {
        'level': 'INFO',
        'handlers': ['console']
    }
}

def setup_logging():
    """初始化日志配置"""
    Path('logs').mkdir(exist_ok=True)
    logging.config.dictConfig(LOGGING_CONFIG)
```

### 日志使用规范

```python
import logging
from typing import Any, Dict

logger = logging.getLogger('pump_station.data_processor')

class DataProcessor:
    def process_data(self, data: pd.DataFrame, equipment_id: int) -> ProcessResult:
        start_time = time.time()

        logger.info(
            "开始处理数据",
            extra={
                "equipment_id": equipment_id,
                "data_rows": len(data),
                "data_columns": list(data.columns)
            }
        )

        try:
            # 数据处理逻辑
            result = self._process_internal(data, equipment_id)

            processing_time = time.time() - start_time
            logger.info(
                "数据处理完成",
                extra={
                    "equipment_id": equipment_id,
                    "processing_time_seconds": round(processing_time, 3),
                    "success_count": result.success_count,
                    "error_count": result.error_count,
                    "throughput_rows_per_second": round(len(data) / processing_time, 2)
                }
            )
            return result

        except Exception as e:
            logger.error(
                "数据处理失败",
                extra={
                    "equipment_id": equipment_id,
                    "error_type": type(e).__name__,
                    "processing_time_seconds": round(time.time() - start_time, 3)
                },
                exc_info=True
            )
            raise
```

## 🔧 配置管理

### 配置文件结构

```python
# config/settings.py
from pydantic import BaseSettings, Field
from typing import Optional
from pathlib import Path

class DatabaseSettings(BaseSettings):
    host: str = Field(default="localhost")
    port: int = Field(default=3306)
    username: str = Field(..., env="DB_USERNAME")
    database: str = Field(..., env="DB_DATABASE")

    @property
    def url(self) -> str:
        return f"mysql+asyncpg://{self.username}@{self.host}:{self.port}/{self.database}"

class RedisSettings(BaseSettings):
    host: str = Field(default="localhost")
    port: int = Field(default=6379)

    db: int = Field(default=0)

class AppSettings(BaseSettings):
    # 应用配置
    app_name: str = Field(default="泵站数据优化系统")
    debug: bool = Field(default=False, env="DEBUG")
    log_level: str = Field(default="INFO", env="LOG_LEVEL")

    # 数据处理配置
    max_chunk_size: int = Field(default=10000)
    max_workers: int = Field(default=4)

    # 数据库配置
    database: DatabaseSettings = Field(default_factory=DatabaseSettings)
    redis: RedisSettings = Field(default_factory=RedisSettings)

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        env_nested_delimiter = "__"

# 全局配置实例
settings = AppSettings()
```

### 环境变量文件

```bash
# .env.example
# 数据库配置
DB_USERNAME=pump_user
DB_DATABASE=pump_station_db

# 应用配置
DEBUG=false
LOG_LEVEL=INFO

# 数据处理配置
MAX_CHUNK_SIZE=10000
MAX_WORKERS=4
```

### 配置验证

```python
# config/validator.py
from pydantic import validator
from pathlib import Path

class DataMappingConfig(BaseModel):
    data_directory: Path
    mapping_file: Path

    @validator('data_directory')
    def validate_data_directory(cls, v):
        if not v.exists():
            raise ValueError(f"数据目录不存在: {v}")
        if not v.is_dir():
            raise ValueError(f"数据路径不是目录: {v}")
        return v

    @validator('mapping_file')
    def validate_mapping_file(cls, v):
        if not v.exists():
            raise ValueError(f"映射文件不存在: {v}")
        if v.suffix != '.json':
            raise ValueError(f"映射文件必须是JSON格式: {v}")
        return v
```

## 📁 文件与路径处理

### 路径处理规范

```python
from pathlib import Path
from typing import List
import os

# 使用 pathlib 而不是 os.path
data_dir = Path("data")
config_file = data_dir / "config" / "mapping.json"

# 文件读写指定编码
def read_csv_file(file_path: Path) -> pd.DataFrame:
    """安全读取CSV文件"""
    try:
        return pd.read_csv(
            file_path,
            encoding='utf-8',
            newline='',  # 处理不同平台的换行符
        )
    except UnicodeDecodeError:
        # 尝试其他编码
        return pd.read_csv(file_path, encoding='gbk')

def write_json_file(file_path: Path, data: dict) -> None:
    """安全写入JSON文件"""
    file_path.parent.mkdir(parents=True, exist_ok=True)

    with open(file_path, 'w', encoding='utf-8', newline='') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


```

## 🧪 测试规范

### 测试结构

```
tests/
├── unit/                 # 单元测试
│   ├── test_config.py
│   ├── test_data_processor.py
│   └── test_models.py
├── integration/          # 集成测试
│   ├── test_database.py
│   └── test_api.py
├── fixtures/            # 测试数据
│   ├── sample_data.csv
│   └── test_config.json
└── conftest.py          # pytest配置
```

### pytest配置

```python
# conftest.py
import pytest
import asyncio
from pathlib import Path
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="session")
def event_loop():
    """创建事件循环用于异步测试"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def db_session():
    """创建测试数据库会话"""
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async_session = sessionmaker(engine, class_=AsyncSession)

    async with async_session() as session:
        yield session

@pytest.fixture
def sample_csv_data():
    """提供示例CSV数据"""
    return Path("tests/fixtures/sample_data.csv")

@pytest.fixture
def test_config():
    """提供测试配置"""
    return {
        "database": {
            "url": "sqlite+aiosqlite:///:memory:"
        },
        "chunk_size": 1000
    }
```

### 测试示例

```python
# tests/unit/test_data_processor.py
import pytest
import pandas as pd
from unittest.mock import Mock, patch
from pump_station.data_processor import DataProcessor
from pump_station.exceptions import DataValidationError

class TestDataProcessor:

    @pytest.fixture
    def processor(self, test_config):
        return DataProcessor(test_config)

    @pytest.fixture
    def valid_data(self):
        return pd.DataFrame({
            'TagName': ['pump_1_power', 'pump_1_power'],
            'DataTime': ['2024-01-01 10:00:00', '2024-01-01 10:01:00'],
            'DataValue': [100.5, 102.3],
            'DataQuality': [1, 1]
        })

    def test_validate_data_success(self, processor, valid_data):
        """测试数据验证成功情况"""
        result = processor.validate_data(valid_data)
        assert len(result) == 2
        assert result['DataValue'].dtype == 'float64'

    def test_validate_data_missing_columns(self, processor):
        """测试缺少必要列的情况"""
        invalid_data = pd.DataFrame({
            'TagName': ['pump_1_power'],
            'DataValue': [100.5]
            # 缺少 DataTime 列
        })

        with pytest.raises(DataValidationError) as exc_info:
            processor.validate_data(invalid_data)

        assert "缺少必要列" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_process_data_success(self, processor, valid_data, db_session):
        """测试数据处理成功情况"""
        with patch.object(processor, '_save_to_database') as mock_save:
            mock_save.return_value = Mock(success_count=2, error_count=0)

            result = await processor.process_data(valid_data, equipment_id=1)

            assert result.success_count == 2
            assert result.error_count == 0
            mock_save.assert_called_once()

    @pytest.mark.parametrize("chunk_size,expected_chunks", [
        (1, 2),
        (2, 1),
        (10, 1)
    ])
    def test_chunk_data(self, processor, valid_data, chunk_size, expected_chunks):
        """测试数据分块功能"""
        chunks = list(processor._chunk_data(valid_data, chunk_size))
        assert len(chunks) == expected_chunks
```

### 性能测试

```python
# tests/performance/test_performance.py
import pytest
import time
import pandas as pd
from pump_station.data_processor import DataProcessor

class TestPerformance:

    @pytest.mark.slow
    def test_large_data_processing_performance(self):
        """测试大数据处理性能"""
        # 生成大量测试数据
        large_data = pd.DataFrame({
            'TagName': ['pump_1_power'] * 100000,
            'DataTime': pd.date_range('2024-01-01', periods=100000, freq='1min'),
            'DataValue': range(100000),
            'DataQuality': [1] * 100000
        })

        processor = DataProcessor()

        start_time = time.time()
        result = processor.validate_data(large_data)
        processing_time = time.time() - start_time

        # 性能要求：10万行数据处理时间不超过5秒
        assert processing_time < 5.0
        assert len(result) == 100000

    @pytest.mark.benchmark
    def test_csv_import_benchmark(self, benchmark, sample_csv_data):
        """基准测试CSV导入性能"""
        processor = DataProcessor()

        def import_csv():
            return processor.import_csv_file(sample_csv_data)

        result = benchmark(import_csv)
        assert result.success_count > 0
```

## 📈 性能与监控规范

### 性能监控装饰器

```python
# utils/monitoring.py
import time
import functools
import logging
from typing import Callable, Any

logger = logging.getLogger(__name__)

def monitor_performance(func: Callable) -> Callable:
    """性能监控装饰器"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        start_time = time.time()
        start_memory = get_memory_usage()

        try:
            result = func(*args, **kwargs)

            end_time = time.time()
            end_memory = get_memory_usage()

            logger.info(
                f"函数 {func.__name__} 执行完成",
                extra={
                    "function_name": func.__name__,
                    "execution_time_seconds": round(end_time - start_time, 3),
                    "memory_usage_mb": round(end_memory - start_memory, 2),
                    "args_count": len(args),
                    "kwargs_count": len(kwargs)
                }
            )

            return result

        except Exception as e:
            end_time = time.time()
            logger.error(
                f"函数 {func.__name__} 执行失败",
                extra={
                    "function_name": func.__name__,
                    "execution_time_seconds": round(end_time - start_time, 3),
                    "error_type": type(e).__name__,
                    "error_message": str(e)
                }
            )
            raise

    return wrapper

def get_memory_usage() -> float:
    """获取当前内存使用量（MB）"""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024
```

### 数据库性能监控

```python
# database/monitoring.py
from sqlalchemy import event
from sqlalchemy.engine import Engine
import logging
import time

logger = logging.getLogger('pump_station.database')

@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    context._query_start_time = time.time()

@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - context._query_start_time

    # 记录慢查询（超过1秒）
    if total > 1.0:
        logger.warning(
            "慢查询检测",
            extra={
                "query_time_seconds": round(total, 3),
                "statement": statement[:200],  # 截取前200字符
                "parameters": str(parameters)[:100] if parameters else None
            }
        )

    # 记录所有查询统计
    logger.debug(
        "数据库查询执行",
        extra={
            "query_time_seconds": round(total, 3),
            "statement_type": statement.strip().split()[0].upper(),
            "executemany": executemany
        }
    )
```

## 🔄 代码质量检查

### ruff配置

```toml
# pyproject.toml
[tool.ruff]
target-version = "py313"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
    "N",  # pep8-naming
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "D103"]

[tool.ruff.isort]
known-first-party = ["pump_station"]

[tool.ruff.pydocstyle]
convention = "google"
```

### pre-commit配置

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
```

### 质量门禁脚本

```bash
#!/bin/bash
# scripts/quality_check.sh

set -e

echo "🔍 运行代码质量检查..."

# 代码格式检查
echo "📝 检查代码格式..."
ruff check .
ruff format --check .

# 类型检查
echo "🔧 运行类型检查..."
mypy .

# 运行测试
echo "🧪 运行测试套件..."
pytest --cov=pump_station --cov-report=term-missing --cov-fail-under=80



echo "✅ 所有质量检查通过！"
```

## 🎯 编码最佳实践

### 质量阈值与检查

- **测试覆盖率**：≥ 80%（核心模块 ≥ 90%）
- **类型注解覆盖率**：≥ 95%
- **复杂度控制**：函数复杂度 ≤ 10，类复杂度 ≤ 50
- **代码重复率**：≤ 5%
- **强制代码审查**：所有代码变更必须经过审查

### 性能优化原则

- **延迟加载**：按需加载数据和资源
- **缓存策略**：合理使用内存缓存和持久化缓存
- **批处理优化**：大数据量操作使用批处理
- **异步处理**：I/O 密集型操作使用异步编程

### 调试友好设计

- **详细日志**：关键路径记录详细日志
- **状态追踪**：重要状态变化可追踪
- **错误上下文**：异常信息包含足够的上下文
- **调试接口**：提供调试和诊断接口

### 可维护性设计

- **配置外部化**：所有配置参数外部化
- **版本兼容性**：API 变更保持向后兼容
- **文档同步**：代码变更同步更新文档
- **重构友好**：代码结构支持重构

## 🚀 并发编程规范

### 线程安全

- **共享状态保护**：使用锁、信号量等同步原语保护共享状态
- **无锁设计**：优先使用不可变对象和函数式编程
- **线程池管理**：使用 `concurrent.futures` 管理线程池

### 异步编程

```python
# 异步数据处理示例
import asyncio
import aiofiles
from typing import List

async def process_csv_file(file_path: Path) -> pd.DataFrame:
    """异步处理 CSV 文件。"""
    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
        content = await f.read()
        # 使用 asyncio.to_thread 处理 CPU 密集型任务
        return await asyncio.to_thread(pd.read_csv, StringIO(content))

async def batch_process_files(file_paths: List[Path]) -> List[pd.DataFrame]:
    """批量异步处理文件。"""
    tasks = [process_csv_file(path) for path in file_paths]
    return await asyncio.gather(*tasks, return_exceptions=True)
```

## 🧪 测试驱动开发规范

### 测试分层

- **单元测试**：测试单个函数或方法的功能
- **集成测试**：测试模块间的交互
- **端到端测试**：测试完整的业务流程
- **性能测试**：测试系统性能和负载能力

### 测试编写规范

```python
import pytest
from unittest.mock import Mock, patch

class TestPumpEfficiencyCalculator:
    """泵效率计算器测试类。"""

    def setup_method(self):
        """每个测试方法执行前的设置。"""
        self.calculator = PumpEfficiencyCalculator()

    def test_calculate_efficiency_normal_case(self):
        """测试正常情况下的效率计算。"""
        # Arrange
        flow_rate = 100.0
        head = 50.0
        power = 15.0
        expected_efficiency = 0.85

        # Act
        result = self.calculator.calculate_efficiency(flow_rate, head, power)

        # Assert
        assert abs(result - expected_efficiency) < 0.01

    def test_calculate_efficiency_zero_power_raises_error(self):
        """测试功率为零时抛出异常。"""
        with pytest.raises(ValueError, match="功率不能为零"):
            self.calculator.calculate_efficiency(100.0, 50.0, 0.0)

    @patch('src.data.database.get_pump_data')
    def test_calculate_efficiency_with_mock(self, mock_get_data):
        """使用 Mock 测试数据库交互。"""
        # Arrange
        mock_get_data.return_value = {'flow': 100, 'head': 50, 'power': 15}

        # Act
        result = self.calculator.calculate_from_db(pump_id=1)

        # Assert
        mock_get_data.assert_called_once_with(pump_id=1)
        assert result > 0
```

### 测试数据管理

- **测试夹具**：使用 pytest fixtures 管理测试数据
- **数据隔离**：每个测试使用独立的测试数据
- **测试数据库**：使用内存数据库或隔离测试实例（避免依赖容器化环境）

## 🔧 调试与排错规范

### 日志设计

```python
import logging
import structlog
from typing import Any, Dict

# 结构化日志配置
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

def process_pump_data(pump_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
    """处理泵数据的示例函数。"""
    logger.info(
        "开始处理泵数据",
        pump_id=pump_id,
        data_size=len(data),
        operation="process_pump_data"
    )

    try:
        # 处理逻辑
        result = perform_calculation(data)

        logger.info(
            "泵数据处理完成",
            pump_id=pump_id,
            result_size=len(result),
            processing_time_ms=123.45
        )

        return result

    except Exception as e:
        logger.error(
            "泵数据处理失败",
            pump_id=pump_id,
            error=str(e),
            error_type=type(e).__name__,
            exc_info=True
        )
        raise
```

### 错误处理策略

- **分层异常处理**：在不同层次处理不同类型的异常
- **异常链**：保持原始异常信息
- **重试机制**：对临时性错误实现智能重试
- **熔断器模式**：防止级联故障

```python
import time
from functools import wraps
from typing import Callable, Type, Tuple

def retry_on_exception(
    exceptions: Tuple[Type[Exception], ...],
    max_retries: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0
) -> Callable:
    """异常重试装饰器。"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(
                            "函数执行失败，准备重试",
                            function=func.__name__,
                            attempt=attempt + 1,
                            max_retries=max_retries,
                            wait_time=wait_time,
                            error=str(e)
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(
                            "函数执行失败，已达最大重试次数",
                            function=func.__name__,
                            max_retries=max_retries,
                            final_error=str(e)
                        )

            raise last_exception
        return wrapper
    return decorator

# 使用示例
@retry_on_exception((ConnectionError, TimeoutError), max_retries=3)
def fetch_pump_data(pump_id: int) -> Dict[str, Any]:
    """获取泵数据，支持自动重试。"""
    # 网络请求逻辑
    pass
```

## 📊 性能监控与优化

### 性能监控

```python
import time
import psutil
from functools import wraps
from typing import Callable

def monitor_performance(func: Callable) -> Callable:
    """性能监控装饰器。"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # 记录开始状态
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss

        try:
            result = func(*args, **kwargs)

            # 记录结束状态
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss

            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory

            logger.info(
                "函数性能监控",
                function=func.__name__,
                execution_time_ms=execution_time * 1000,
                memory_delta_mb=memory_delta / 1024 / 1024,
                args_count=len(args),
                kwargs_count=len(kwargs)
            )

            return result

        except Exception as e:
            end_time = time.perf_counter()
            execution_time = end_time - start_time

            logger.error(
                "函数执行异常",
                function=func.__name__,
                execution_time_ms=execution_time * 1000,
                error=str(e)
            )
            raise

    return wrapper
```

### 内存管理

- **大数据处理**：使用生成器和迭代器减少内存占用
- **缓存管理**：实现 LRU 缓存和缓存过期机制
- **内存泄漏检测**：定期检查内存使用情况

## 📋 开发流程检查清单

### 🔄 版本控制规范

#### Git 提交规范

```bash
# 提交信息格式：<type>(<scope>): <description>
# 类型说明：
# feat: 新功能
# fix: 修复 bug
# docs: 文档更新
# style: 代码格式调整（不影响功能）
# refactor: 重构代码
# test: 添加或修改测试
# chore: 构建过程或辅助工具的变动

# 示例
git commit -m "feat(data): 添加 CSV 数据导入功能"
git commit -m "fix(pump): 修复效率计算精度问题"
git commit -m "docs(api): 更新 API 文档"
```

#### 分支管理策略

- **main/master**：生产环境分支，只接受经过测试的代码
- **develop**：开发主分支，集成所有功能分支
- **feature/xxx**：功能开发分支，从 develop 分出
- **hotfix/xxx**：紧急修复分支，从 main 分出
- **release/xxx**：发布准备分支，从 develop 分出

#### 代码审查流程

1. **创建 Pull Request**：详细描述变更内容和测试情况
1. **自动化检查**：CI/CD 流水线自动运行测试和质量检查
1. **人工审查**：至少一名同事进行代码审查
1. **修改完善**：根据审查意见修改代码
1. **合并代码**：审查通过后合并到目标分支

### 🚀 持续集成/持续部署 (CI/CD)

#### GitHub Actions 配置示例

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12, 3.13]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Lint with ruff
      run: |
        ruff check .
        ruff format --check .

    - name: Type check with mypy
      run: mypy src/



    - name: Run tests with pytest
      run: |
        pytest --cov=src --cov-report=xml --cov-report=html

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### 📦 依赖管理

#### pyproject.toml 配置

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "pump-optimization-system"
version = "0.1.0"
description = "泵站运行数据优化系统"
authors = [{name = "Your Name", email = "your.email@example.com"}]
readme = "README.md"
requires-python = ">=3.11"
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = [
    "fastapi>=0.104.0",
    "sqlalchemy>=2.0.0",
    "pandas>=2.1.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "pydantic>=2.4.0",
    "structlog>=23.1.0",
    "python-dotenv>=1.0.0",
    "asyncio-mqtt>=0.13.0",
    "aiofiles>=23.2.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.21.0",
    "mypy>=1.5.0",
    "ruff>=0.0.290",
    "bandit>=1.7.5",
    "pre-commit>=3.4.0",
    "black>=23.7.0",
    "isort>=5.12.0",
]
test = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.11.0",
    "factory-boy>=3.3.0",
]
docs = [
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.2.0",
    "mkdocstrings[python]>=0.22.0",
]

[tool.ruff]
target-version = "py311"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*" = ["S101", "S106"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = ["tests"]
markers = [
    "slow: marks tests as slow (deselect with '-m "not slow"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\bProtocol\):",
    "@(abc\.)?abstractmethod",
]
```

### 🔍 代码质量检查清单

#### 提交前检查 (Pre-commit)

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.0.290
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.5.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]


```

#### 开发环境设置检查清单

- [ ] Python 3.11+ 已安装
- [ ] 虚拟环境已创建并激活
- [ ] 依赖包已安装（`pip install -e ".[dev]"`）
- [ ] Pre-commit hooks 已安装（`pre-commit install`）
- [ ] IDE 配置了类型检查和代码格式化
- [ ] 环境变量配置文件已创建（`.env`）

#### 功能开发检查清单

- [ ] 功能需求明确，有详细的设计文档
- [ ] 创建功能分支（`git checkout -b feature/xxx`）
- [ ] 编写测试用例（TDD 方式）
- [ ] 实现功能代码
- [ ] 代码通过所有质量检查
- [ ] 测试覆盖率达到要求（≥80%）
- [ ] 更新相关文档
- [ ] 创建 Pull Request

#### 发布前检查清单

- [ ] 所有测试通过

- [ ] 代码覆盖率达标

- [ ] 性能测试通过

- [ ] 文档更新完整

- [ ] 版本号已更新

- [ ] 变更日志已更新

- [ ] 生产环境配置已准备

### 🐛 问题排查指南

#### 常见问题诊断

1. **性能问题**

   - 检查日志中的性能监控数据
   - 使用 profiler 分析瓶颈
   - 检查数据库查询性能
   - 分析内存使用情况

1. **数据问题**

   - 检查数据验证日志
   - 验证数据源完整性
   - 检查数据转换逻辑
   - 分析数据质量报告

1. **系统错误**

   - 查看结构化日志
   - 检查错误堆栈信息
   - 验证配置参数
   - 检查依赖服务状态

#### 调试工具推荐

- **日志分析**：ELK Stack, Grafana
- **性能监控**：Prometheus + Grafana
- **代码调试**：pdb, ipdb, VS Code debugger
- **内存分析**：memory_profiler, pympler
- **网络调试**：Wireshark, tcpdump

### 📚 文档维护规范

#### 文档类型

- **API 文档**：自动生成，保持与代码同步
- **架构文档**：系统设计和技术决策
- **用户手册**：功能使用说明
- **开发指南**：开发环境搭建和贡献指南
- **故障排除**：常见问题和解决方案

#### 文档更新流程

- 代码变更时同步更新相关文档
- 定期审查文档的准确性和完整性
- 使用版本控制管理文档变更
- 建立文档反馈和改进机制

### 开发前检查

- [ ] 创建功能分支：`git checkout -b feature/功能名称`
- [ ] 更新依赖：`pip install -e .[dev]`
- [ ] 运行质量检查：`./scripts/quality_check.sh`

### 开发中检查

- [ ] 遵循命名规范
- [ ] 添加类型注解
- [ ] 编写文档字符串
- [ ] 添加适当的日志
- [ ] 处理异常情况

### 提交前检查

- [ ] 运行完整测试：`pytest`
- [ ] 检查代码覆盖率：`pytest --cov`
- [ ] 运行性能测试：`pytest -m benchmark`
- [ ] 更新文档
- [ ] 提交信息规范

### 代码审查检查

- [ ] 代码逻辑正确性

- [ ] 性能影响评估

- [ ] 可维护性评估

- [ ] 测试覆盖充分性

这套开发规范确保了代码的高质量、可维护性和团队协作效率，为泵站数据优化系统的成功开发提供了坚实的基础。
