# æ³µç«™æ•°æ®ä¼˜åŒ–ç³»ç»Ÿ - å¼€å‘è§„èŒƒä¸æ ‡å‡†

## ğŸš€ å¦‚ä½•ä½¿ç”¨æœ¬è§„èŒƒ

### æ–°å…¥èŒå¼€å‘è€…å¿«é€Ÿå¯åŠ¨

1. **ç¯å¢ƒé…ç½®** â†’ [ğŸ“‹ ç‰ˆæœ¬ä¸ç¯å¢ƒè§„èŒƒ](#-%E7%89%88%E6%9C%AC%E4%B8%8E%E7%8E%AF%E5%A2%83%E8%A7%84%E8%8C%83)ï¼šé…ç½®Python 3.13.6ç¯å¢ƒï¼Œå®‰è£…ä¾èµ–
1. **ä»£ç é£æ ¼** â†’ [ğŸ¨ ä»£ç é£æ ¼è§„èŒƒ](#-%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A7%84%E8%8C%83)ï¼šé…ç½®ruffã€mypyï¼Œå­¦ä¹ å‘½åçº¦å®š
1. **é¡¹ç›®ç»“æ„** â†’ [ğŸ—ï¸ é¡¹ç›®ç»“æ„è§„èŒƒ](#%EF%B8%8F-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E8%A7%84%E8%8C%83)ï¼šäº†è§£ç›®å½•ç»„ç»‡ï¼Œæ‰¾åˆ°å¼€å‘å…¥å£
1. **æµ‹è¯•å®è·µ** â†’ [ğŸ§ª æµ‹è¯•è§„èŒƒ](#-%E6%B5%8B%E8%AF%95%E8%A7%84%E8%8C%83)ï¼šç¼–å†™å•å…ƒæµ‹è¯•ï¼Œè¿è¡Œè´¨é‡æ£€æŸ¥

### æ—¥å¸¸å¼€å‘æ£€æŸ¥æ¸…å•

- [ ] åŠŸèƒ½å¼€å‘å‰ï¼šåŒæ­¥ä¾èµ– â†’ è¿è¡Œè´¨é‡æ£€æŸ¥ â†’ åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
- [ ] ç¼–ç è¿‡ç¨‹ä¸­ï¼šéµå¾ªå‘½åè§„èŒƒ â†’ æ·»åŠ ç±»å‹æ³¨è§£ â†’ ç¼–å†™æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] æäº¤å‰æ£€æŸ¥ï¼šè¿è¡Œæµ‹è¯• â†’ ä»£ç æ ¼å¼åŒ– â†’ ç±»å‹æ£€æŸ¥é€šè¿‡
- [ ] ä»£ç å®¡æŸ¥ï¼šå•ä¸€èŒè´£ â†’ å¼‚å¸¸å¤„ç† â†’ æ€§èƒ½è€ƒè™‘

### å¸¸è§é—®é¢˜å¿«é€Ÿå¯¼èˆª

- **é…ç½®é—®é¢˜** â†’ [âš™ï¸ é…ç½®ç®¡ç†](#%EF%B8%8F-%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86)
- **æ—¥å¿—è§„èŒƒ** â†’ [ğŸ“Š æ—¥å¿—è§„èŒƒ](#-%E6%97%A5%E5%BF%97%E8%A7%84%E8%8C%83)
- **é”™è¯¯å¤„ç†** â†’ [âš ï¸ é”™è¯¯å¤„ç†è§„èŒƒ](#%EF%B8%8F-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86%E8%A7%84%E8%8C%83)
- **æ€§èƒ½ä¼˜åŒ–** â†’ [âš¡ æ€§èƒ½è§„èŒƒ](#-%E6%80%A7%E8%83%BD%E8%A7%84%E8%8C%83)

______________________________________________________________________

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£å®šä¹‰äº†æ³µç«™æ•°æ®ä¼˜åŒ–ç³»ç»Ÿå¼€å‘è¿‡ç¨‹ä¸­å¿…é¡»éµå¾ªçš„ç¼–ç è§„èŒƒã€å·¥ç¨‹å®è·µå’Œè´¨é‡æ ‡å‡†ï¼Œç¡®ä¿ä»£ç çš„å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’Œé«˜è´¨é‡ã€‚

## ğŸ“‹ ç‰ˆæœ¬ä¸ç¯å¢ƒè§„èŒƒ

### Pythonç‰ˆæœ¬

- **ä¸»ç‰ˆæœ¬**ï¼šPython 3.13.6
- **è™šæ‹Ÿç¯å¢ƒ**ï¼šä½¿ç”¨é¡¹ç›®çº§ `.venv` ç›®å½•
- **ä¾èµ–ç®¡ç†**ï¼šä½¿ç”¨ `pyproject.toml` ç»Ÿä¸€ç®¡ç†

### ä¾èµ–ç®¡ç†è§„èŒƒ

```toml
# pyproject.toml ç¤ºä¾‹
[project]
name = "pump-station-optimizer"
version = "0.1.0"
description = "æ³µç«™è¿è¡Œæ•°æ®ä¼˜åŒ–ç³»ç»Ÿ"
requires-python = ">=3.13"
dependencies = [
    "fastapi>=0.104.0,<0.105.0",
    "sqlalchemy>=2.0.0,<2.1.0",
    "pandas>=2.1.0,<2.2.0",
    "pydantic>=2.5.0,<2.6.0",
    "asyncpg>=0.29.0,<0.30.0",
]

[project.optional-dependencies]
dev = [
    "ruff>=0.1.0",
    "mypy>=1.7.0",
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
]
```

### å¼€å‘å‰æ£€æŸ¥æ¸…å•

- [ ] åŒæ­¥æœ€æ–°ä¾èµ–ï¼š`pip install -e .[dev]`
- [ ] è¿è¡Œä»£ç æ£€æŸ¥ï¼š`ruff check .`
- [ ] è¿è¡Œç±»å‹æ£€æŸ¥ï¼š`mypy .`
- [ ] è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼š`pytest`

## ğŸ¨ ä»£ç é£æ ¼è§„èŒƒ

### åŸºç¡€è§„èŒƒ

- **éµå¾ª PEP 8**ï¼šä½¿ç”¨ ruff è‡ªåŠ¨æ ¼å¼åŒ–
- **è¡Œé•¿åº¦**ï¼šæœ€å¤§ 88 å­—ç¬¦
- **ç¼©è¿›**ï¼š4ä¸ªç©ºæ ¼ï¼Œç¦ç”¨åˆ¶è¡¨ç¬¦
- **ç¼–ç **ï¼šUTF-8ï¼Œæ–‡ä»¶å¤´å£°æ˜ `# -*- coding: utf-8 -*-`

### ä»£ç ç»„ç»‡ä¸æ¶æ„åŸåˆ™

- **å•ä¸€èŒè´£åŸåˆ™**ï¼šæ¯ä¸ªå‡½æ•°ä¸è¶…è¿‡ 50 è¡Œï¼Œæ¯ä¸ªç±»ä¸è¶…è¿‡ 500 è¡Œ
- **æ¨¡å—èšç„¦**ï¼šæ¯ä¸ªæ¨¡å—ä¸“æ³¨ä¸€ä¸ªä¸šåŠ¡é¢†åŸŸï¼Œå…¬å…±å·¥å…·é›†ä¸­åˆ° `utils` å­åŒ…
- **ä¾èµ–å€’ç½®**ï¼šé«˜å±‚æ¨¡å—ä¾èµ–æŠ½è±¡æ¥å£ï¼Œä¸ä¾èµ–å…·ä½“å®ç°
- **æ¥å£éš”ç¦»**ï¼šå®šä¹‰æœ€å°åŒ–çš„æ¥å£ï¼Œé¿å…è‡ƒè‚¿çš„æ¥å£
- **åˆ†å±‚æ¶æ„**ï¼šæ˜ç¡®åŒºåˆ†æ•°æ®å±‚ã€ä¸šåŠ¡å±‚ã€è¡¨ç¤ºå±‚

### å‘½åè§„èŒƒ

```python
# æ¨¡å—åï¼šå°å†™+ä¸‹åˆ’çº¿
data_processor.py
config_manager.py

# ç±»åï¼šå¤§é©¼å³°
class DataProcessor:
class ConfigManager:

# å‡½æ•°åï¼šå°å†™+ä¸‹åˆ’çº¿
def process_csv_data():
def validate_config():

# å˜é‡åï¼šå°å†™+ä¸‹åˆ’çº¿
user_data = {}
max_retry_count = 3

# å¸¸é‡ï¼šå…¨å¤§å†™+ä¸‹åˆ’çº¿
MAX_CHUNK_SIZE = 10000
DEFAULT_TIMEOUT = 30

# ç§æœ‰å±æ€§ï¼šå•ä¸‹åˆ’çº¿å‰ç¼€
class DataProcessor:
    def __init__(self):
        self._connection = None
        self._config = {}
```

### æ–‡æ¡£å­—ç¬¦ä¸²è§„èŒƒ

```python
def process_pump_data(
    data_file: Path,
    equipment_id: int,
    chunk_size: int = 10000
) -> ProcessResult:
    """
    å¤„ç†æ³µç«™CSVæ•°æ®æ–‡ä»¶å¹¶å¯¼å…¥æ•°æ®åº“ã€‚

    è¯¥å‡½æ•°è´Ÿè´£è¯»å–CSVæ–‡ä»¶ï¼Œè¿›è¡Œæ•°æ®éªŒè¯å’Œæ¸…æ´—ï¼Œç„¶åæ‰¹é‡å¯¼å…¥åˆ°æ•°æ®åº“ä¸­ã€‚
    æ”¯æŒå¤§æ–‡ä»¶åˆ†å—å¤„ç†ï¼Œç¡®ä¿å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚

    å‚æ•°:
        data_file: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
        equipment_id: è®¾å¤‡IDï¼Œç”¨äºå…³è”æ•°æ®åº“è®°å½•
        chunk_size: åˆ†å—å¤§å°ï¼Œé»˜è®¤10000è¡Œ

    è¿”å›:
        ProcessResult: åŒ…å«å¤„ç†ç»“æœçš„å¯¹è±¡ï¼ŒåŒ…æ‹¬æˆåŠŸè¡Œæ•°ã€å¤±è´¥è¡Œæ•°ç­‰

    å¼‚å¸¸:
        FileNotFoundError: å½“æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º
        ValidationError: å½“æ•°æ®éªŒè¯å¤±è´¥æ—¶æŠ›å‡º
        DatabaseError: å½“æ•°æ®åº“æ“ä½œå¤±è´¥æ—¶æŠ›å‡º

    ç¤ºä¾‹:
        >>> import logging
        >>> logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        >>> logger = logging.getLogger(__name__)
        >>> result = process_pump_data(
        ...     Path("data/pump_data.csv"),
        ...     equipment_id=1,
        ...     chunk_size=5000
        ... )
        >>> logger.info(f"æˆåŠŸå¤„ç† {result.success_count} è¡Œæ•°æ®")
    """
    pass
```

## ğŸ”§ ç±»å‹æ³¨è§£è§„èŒƒ

### åŸºæœ¬åŸåˆ™

- **æ‰€æœ‰å…¬å…±å‡½æ•°**å¿…é¡»æœ‰å®Œæ•´ç±»å‹æ³¨è§£
- **é‡è¦å†…éƒ¨å‡½æ•°**å»ºè®®æ·»åŠ ç±»å‹æ³¨è§£
- **é¿å…ä½¿ç”¨ Any**ï¼Œå°½é‡ä½¿ç”¨ç²¾ç¡®ç±»å‹
- **ä½¿ç”¨æ³›å‹**æé«˜ç±»å‹å®‰å…¨æ€§

### ç±»å‹æ³¨è§£ç¤ºä¾‹

```python
from typing import Dict, List, Optional, Union, Protocol
from pathlib import Path
from decimal import Decimal
from datetime import datetime

# åŸºç¡€ç±»å‹æ³¨è§£
def get_equipment_count(station_id: int) -> int:
    pass

# å®¹å™¨ç±»å‹æ³¨è§£
def get_sensor_data(equipment_ids: List[int]) -> Dict[int, List[float]]:
    pass

# å¯é€‰ç±»å‹
def find_equipment(name: str) -> Optional[Equipment]:
    pass

# è”åˆç±»å‹
def parse_value(raw_value: str) -> Union[int, float, str]:
    pass

# åè®®ç±»å‹
class DataProcessor(Protocol):
    def process(self, data: pd.DataFrame) -> ProcessResult:
        ...

# æ³›å‹ç±»å‹
from typing import TypeVar, Generic

T = TypeVar('T')

class Repository(Generic[T]):
    def save(self, entity: T) -> T:
        pass

    def find_by_id(self, id: int) -> Optional[T]:
        pass
```

### mypyé…ç½®

```ini
# mypy.ini
[mypy]
python_version = 3.13
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
warn_unreachable = True
strict_equality = True
```

## ğŸš¨ é”™è¯¯å¤„ç†è§„èŒƒ

### å¼‚å¸¸å¤„ç†åŸåˆ™

- **ç¦æ­¢è£¸ except**ï¼šå¿…é¡»æ•è·å…·ä½“å¼‚å¸¸ç±»å‹
- **å¼‚å¸¸é“¾**ï¼šä½¿ç”¨ `raise ... from ...` ä¿ç•™åŸå§‹å¼‚å¸¸
- **ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šå¼‚å¸¸ä¸­åŒ…å«å…³é”®å‚æ•°å’ŒçŠ¶æ€
- **æ—¥å¿—è®°å½•**ï¼šæ‰€æœ‰å¼‚å¸¸éƒ½è¦è®°å½•åˆ°æ—¥å¿—

### è‡ªå®šä¹‰å¼‚å¸¸ä½“ç³»

```python
# exceptions.py
class PumpStationError(Exception):
    """æ³µç«™ç³»ç»ŸåŸºç¡€å¼‚å¸¸ç±»"""
    pass

class ConfigurationError(PumpStationError):
    """é…ç½®ç›¸å…³å¼‚å¸¸"""
    pass

class DataValidationError(PumpStationError):
    """æ•°æ®éªŒè¯å¼‚å¸¸"""
    def __init__(self, message: str, invalid_rows: List[int]):
        super().__init__(message)
        self.invalid_rows = invalid_rows

class DatabaseError(PumpStationError):
    """æ•°æ®åº“æ“ä½œå¼‚å¸¸"""
    pass

class OptimizationError(PumpStationError):
    """ä¼˜åŒ–ç®—æ³•å¼‚å¸¸"""
    pass
```

### å¼‚å¸¸å¤„ç†ç¤ºä¾‹

```python
import logging
from typing import Optional

logger = logging.getLogger(__name__)

def import_csv_data(file_path: Path, equipment_id: int) -> ImportResult:
    """
    å¯¼å…¥CSVæ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„å¼‚å¸¸å¤„ç†ã€‚
    """
    try:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # è¯»å–å’Œå¤„ç†æ•°æ®
        data = pd.read_csv(file_path, encoding='utf-8')
        validated_data = validate_sensor_data(data)

        # å¯¼å…¥æ•°æ®åº“
        result = save_to_database(validated_data, equipment_id)

        logger.info(
            "CSVæ•°æ®å¯¼å…¥æˆåŠŸ",
            extra={
                "file_path": str(file_path),
                "equipment_id": equipment_id,
                "rows_imported": result.success_count
            }
        )
        return result

    except FileNotFoundError as e:
        logger.error(
            "æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯",
            extra={"file_path": str(file_path), "error": str(e)}
        )
        raise ConfigurationError(f"æ— æ³•æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {file_path}") from e

    except pd.errors.EmptyDataError as e:
        logger.error(
            "CSVæ–‡ä»¶ä¸ºç©º",
            extra={"file_path": str(file_path)}
        )
        raise DataValidationError("CSVæ–‡ä»¶ä¸åŒ…å«æœ‰æ•ˆæ•°æ®") from e

    except ValidationError as e:
        logger.error(
            "æ•°æ®éªŒè¯å¤±è´¥",
            extra={
                "file_path": str(file_path),
                "validation_errors": e.errors()
            }
        )
        raise DataValidationError(f"æ•°æ®éªŒè¯å¤±è´¥: {e}") from e

    except Exception as e:
        logger.error(
            "CSVå¯¼å…¥æœªçŸ¥é”™è¯¯",
            extra={
                "file_path": str(file_path),
                "equipment_id": equipment_id,
                "error_type": type(e).__name__,
                "error_message": str(e)
            },
            exc_info=True
        )
        raise DatabaseError(f"æ•°æ®å¯¼å…¥å¤±è´¥: {e}") from e
```

## ğŸ“Š æ—¥å¿—è§„èŒƒ

### æ—¥å¿—é…ç½®

```python
# logging_config.py
import logging.config
from pathlib import Path

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(pathname)s:%(lineno)d',
            'datefmt': '%Y-%m-%d %H:%M:%S'
        },
        'simple': {
            'format': '%(levelname)s - %(message)s'
        },
        'json': {
            'format': '%(asctime)s %(name)s %(levelname)s %(message)s',
            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'simple',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'DEBUG',
            'formatter': 'detailed',
            'filename': 'logs/app.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5
        },
        'error_file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'ERROR',
            'formatter': 'json',
            'filename': 'logs/error.log',
            'maxBytes': 10485760,
            'backupCount': 10
        }
    },
    'loggers': {
        'pump_station': {
            'level': 'DEBUG',
            'handlers': ['console', 'file', 'error_file'],
            'propagate': False
        }
    },
    'root': {
        'level': 'INFO',
        'handlers': ['console']
    }
}

def setup_logging():
    """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
    Path('logs').mkdir(exist_ok=True)
    logging.config.dictConfig(LOGGING_CONFIG)
```

### æ—¥å¿—ä½¿ç”¨è§„èŒƒ

```python
import logging
from typing import Any, Dict

logger = logging.getLogger('pump_station.data_processor')

class DataProcessor:
    def process_data(self, data: pd.DataFrame, equipment_id: int) -> ProcessResult:
        start_time = time.time()

        logger.info(
            "å¼€å§‹å¤„ç†æ•°æ®",
            extra={
                "equipment_id": equipment_id,
                "data_rows": len(data),
                "data_columns": list(data.columns)
            }
        )

        try:
            # æ•°æ®å¤„ç†é€»è¾‘
            result = self._process_internal(data, equipment_id)

            processing_time = time.time() - start_time
            logger.info(
                "æ•°æ®å¤„ç†å®Œæˆ",
                extra={
                    "equipment_id": equipment_id,
                    "processing_time_seconds": round(processing_time, 3),
                    "success_count": result.success_count,
                    "error_count": result.error_count,
                    "throughput_rows_per_second": round(len(data) / processing_time, 2)
                }
            )
            return result

        except Exception as e:
            logger.error(
                "æ•°æ®å¤„ç†å¤±è´¥",
                extra={
                    "equipment_id": equipment_id,
                    "error_type": type(e).__name__,
                    "processing_time_seconds": round(time.time() - start_time, 3)
                },
                exc_info=True
            )
            raise
```

## ğŸ”§ é…ç½®ç®¡ç†

### é…ç½®æ–‡ä»¶ç»“æ„

```python
# config/settings.py
from pydantic import BaseSettings, Field
from typing import Optional
from pathlib import Path

class DatabaseSettings(BaseSettings):
    host: str = Field(default="localhost")
    port: int = Field(default=3306)
    username: str = Field(..., env="DB_USERNAME")
    database: str = Field(..., env="DB_DATABASE")

    @property
    def url(self) -> str:
        return f"mysql+asyncpg://{self.username}@{self.host}:{self.port}/{self.database}"

class RedisSettings(BaseSettings):
    host: str = Field(default="localhost")
    port: int = Field(default=6379)

    db: int = Field(default=0)

class AppSettings(BaseSettings):
    # åº”ç”¨é…ç½®
    app_name: str = Field(default="æ³µç«™æ•°æ®ä¼˜åŒ–ç³»ç»Ÿ")
    debug: bool = Field(default=False, env="DEBUG")
    log_level: str = Field(default="INFO", env="LOG_LEVEL")

    # æ•°æ®å¤„ç†é…ç½®
    max_chunk_size: int = Field(default=10000)
    max_workers: int = Field(default=4)

    # æ•°æ®åº“é…ç½®
    database: DatabaseSettings = Field(default_factory=DatabaseSettings)
    redis: RedisSettings = Field(default_factory=RedisSettings)

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        env_nested_delimiter = "__"

# å…¨å±€é…ç½®å®ä¾‹
settings = AppSettings()
```

### ç¯å¢ƒå˜é‡æ–‡ä»¶

```bash
# .env.example
# æ•°æ®åº“é…ç½®
DB_USERNAME=pump_user
DB_DATABASE=pump_station_db

# åº”ç”¨é…ç½®
DEBUG=false
LOG_LEVEL=INFO

# æ•°æ®å¤„ç†é…ç½®
MAX_CHUNK_SIZE=10000
MAX_WORKERS=4
```

### é…ç½®éªŒè¯

```python
# config/validator.py
from pydantic import validator
from pathlib import Path

class DataMappingConfig(BaseModel):
    data_directory: Path
    mapping_file: Path

    @validator('data_directory')
    def validate_data_directory(cls, v):
        if not v.exists():
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {v}")
        if not v.is_dir():
            raise ValueError(f"æ•°æ®è·¯å¾„ä¸æ˜¯ç›®å½•: {v}")
        return v

    @validator('mapping_file')
    def validate_mapping_file(cls, v):
        if not v.exists():
            raise ValueError(f"æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {v}")
        if v.suffix != '.json':
            raise ValueError(f"æ˜ å°„æ–‡ä»¶å¿…é¡»æ˜¯JSONæ ¼å¼: {v}")
        return v
```

## ğŸ“ æ–‡ä»¶ä¸è·¯å¾„å¤„ç†

### è·¯å¾„å¤„ç†è§„èŒƒ

```python
from pathlib import Path
from typing import List
import os

# ä½¿ç”¨ pathlib è€Œä¸æ˜¯ os.path
data_dir = Path("data")
config_file = data_dir / "config" / "mapping.json"

# æ–‡ä»¶è¯»å†™æŒ‡å®šç¼–ç 
def read_csv_file(file_path: Path) -> pd.DataFrame:
    """å®‰å…¨è¯»å–CSVæ–‡ä»¶"""
    try:
        return pd.read_csv(
            file_path,
            encoding='utf-8',
            newline='',  # å¤„ç†ä¸åŒå¹³å°çš„æ¢è¡Œç¬¦
        )
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        return pd.read_csv(file_path, encoding='gbk')

def write_json_file(file_path: Path, data: dict) -> None:
    """å®‰å…¨å†™å…¥JSONæ–‡ä»¶"""
    file_path.parent.mkdir(parents=True, exist_ok=True)

    with open(file_path, 'w', encoding='utf-8', newline='') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


```

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### æµ‹è¯•ç»“æ„

```
tests/
â”œâ”€â”€ unit/                 # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_data_processor.py
â”‚   â””â”€â”€ test_models.py
â”œâ”€â”€ integration/          # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_database.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ fixtures/            # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ sample_data.csv
â”‚   â””â”€â”€ test_config.json
â””â”€â”€ conftest.py          # pytesté…ç½®
```

### pytesté…ç½®

```python
# conftest.py
import pytest
import asyncio
from pathlib import Path
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯ç”¨äºå¼‚æ­¥æµ‹è¯•"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
async def db_session():
    """åˆ›å»ºæµ‹è¯•æ•°æ®åº“ä¼šè¯"""
    engine = create_async_engine("sqlite+aiosqlite:///:memory:")
    async_session = sessionmaker(engine, class_=AsyncSession)

    async with async_session() as session:
        yield session

@pytest.fixture
def sample_csv_data():
    """æä¾›ç¤ºä¾‹CSVæ•°æ®"""
    return Path("tests/fixtures/sample_data.csv")

@pytest.fixture
def test_config():
    """æä¾›æµ‹è¯•é…ç½®"""
    return {
        "database": {
            "url": "sqlite+aiosqlite:///:memory:"
        },
        "chunk_size": 1000
    }
```

### æµ‹è¯•ç¤ºä¾‹

```python
# tests/unit/test_data_processor.py
import pytest
import pandas as pd
from unittest.mock import Mock, patch
from pump_station.data_processor import DataProcessor
from pump_station.exceptions import DataValidationError

class TestDataProcessor:

    @pytest.fixture
    def processor(self, test_config):
        return DataProcessor(test_config)

    @pytest.fixture
    def valid_data(self):
        return pd.DataFrame({
            'TagName': ['pump_1_power', 'pump_1_power'],
            'DataTime': ['2024-01-01 10:00:00', '2024-01-01 10:01:00'],
            'DataValue': [100.5, 102.3],
            'DataQuality': [1, 1]
        })

    def test_validate_data_success(self, processor, valid_data):
        """æµ‹è¯•æ•°æ®éªŒè¯æˆåŠŸæƒ…å†µ"""
        result = processor.validate_data(valid_data)
        assert len(result) == 2
        assert result['DataValue'].dtype == 'float64'

    def test_validate_data_missing_columns(self, processor):
        """æµ‹è¯•ç¼ºå°‘å¿…è¦åˆ—çš„æƒ…å†µ"""
        invalid_data = pd.DataFrame({
            'TagName': ['pump_1_power'],
            'DataValue': [100.5]
            # ç¼ºå°‘ DataTime åˆ—
        })

        with pytest.raises(DataValidationError) as exc_info:
            processor.validate_data(invalid_data)

        assert "ç¼ºå°‘å¿…è¦åˆ—" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_process_data_success(self, processor, valid_data, db_session):
        """æµ‹è¯•æ•°æ®å¤„ç†æˆåŠŸæƒ…å†µ"""
        with patch.object(processor, '_save_to_database') as mock_save:
            mock_save.return_value = Mock(success_count=2, error_count=0)

            result = await processor.process_data(valid_data, equipment_id=1)

            assert result.success_count == 2
            assert result.error_count == 0
            mock_save.assert_called_once()

    @pytest.mark.parametrize("chunk_size,expected_chunks", [
        (1, 2),
        (2, 1),
        (10, 1)
    ])
    def test_chunk_data(self, processor, valid_data, chunk_size, expected_chunks):
        """æµ‹è¯•æ•°æ®åˆ†å—åŠŸèƒ½"""
        chunks = list(processor._chunk_data(valid_data, chunk_size))
        assert len(chunks) == expected_chunks
```

### æ€§èƒ½æµ‹è¯•

```python
# tests/performance/test_performance.py
import pytest
import time
import pandas as pd
from pump_station.data_processor import DataProcessor

class TestPerformance:

    @pytest.mark.slow
    def test_large_data_processing_performance(self):
        """æµ‹è¯•å¤§æ•°æ®å¤„ç†æ€§èƒ½"""
        # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
        large_data = pd.DataFrame({
            'TagName': ['pump_1_power'] * 100000,
            'DataTime': pd.date_range('2024-01-01', periods=100000, freq='1min'),
            'DataValue': range(100000),
            'DataQuality': [1] * 100000
        })

        processor = DataProcessor()

        start_time = time.time()
        result = processor.validate_data(large_data)
        processing_time = time.time() - start_time

        # æ€§èƒ½è¦æ±‚ï¼š10ä¸‡è¡Œæ•°æ®å¤„ç†æ—¶é—´ä¸è¶…è¿‡5ç§’
        assert processing_time < 5.0
        assert len(result) == 100000

    @pytest.mark.benchmark
    def test_csv_import_benchmark(self, benchmark, sample_csv_data):
        """åŸºå‡†æµ‹è¯•CSVå¯¼å…¥æ€§èƒ½"""
        processor = DataProcessor()

        def import_csv():
            return processor.import_csv_file(sample_csv_data)

        result = benchmark(import_csv)
        assert result.success_count > 0
```

## ğŸ“ˆ æ€§èƒ½ä¸ç›‘æ§è§„èŒƒ

### æ€§èƒ½ç›‘æ§è£…é¥°å™¨

```python
# utils/monitoring.py
import time
import functools
import logging
from typing import Callable, Any

logger = logging.getLogger(__name__)

def monitor_performance(func: Callable) -> Callable:
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs) -> Any:
        start_time = time.time()
        start_memory = get_memory_usage()

        try:
            result = func(*args, **kwargs)

            end_time = time.time()
            end_memory = get_memory_usage()

            logger.info(
                f"å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆ",
                extra={
                    "function_name": func.__name__,
                    "execution_time_seconds": round(end_time - start_time, 3),
                    "memory_usage_mb": round(end_memory - start_memory, 2),
                    "args_count": len(args),
                    "kwargs_count": len(kwargs)
                }
            )

            return result

        except Exception as e:
            end_time = time.time()
            logger.error(
                f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥",
                extra={
                    "function_name": func.__name__,
                    "execution_time_seconds": round(end_time - start_time, 3),
                    "error_type": type(e).__name__,
                    "error_message": str(e)
                }
            )
            raise

    return wrapper

def get_memory_usage() -> float:
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024
```

### æ•°æ®åº“æ€§èƒ½ç›‘æ§

```python
# database/monitoring.py
from sqlalchemy import event
from sqlalchemy.engine import Engine
import logging
import time

logger = logging.getLogger('pump_station.database')

@event.listens_for(Engine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    context._query_start_time = time.time()

@event.listens_for(Engine, "after_cursor_execute")
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - context._query_start_time

    # è®°å½•æ…¢æŸ¥è¯¢ï¼ˆè¶…è¿‡1ç§’ï¼‰
    if total > 1.0:
        logger.warning(
            "æ…¢æŸ¥è¯¢æ£€æµ‹",
            extra={
                "query_time_seconds": round(total, 3),
                "statement": statement[:200],  # æˆªå–å‰200å­—ç¬¦
                "parameters": str(parameters)[:100] if parameters else None
            }
        )

    # è®°å½•æ‰€æœ‰æŸ¥è¯¢ç»Ÿè®¡
    logger.debug(
        "æ•°æ®åº“æŸ¥è¯¢æ‰§è¡Œ",
        extra={
            "query_time_seconds": round(total, 3),
            "statement_type": statement.strip().split()[0].upper(),
            "executemany": executemany
        }
    )
```

## ğŸ”„ ä»£ç è´¨é‡æ£€æŸ¥

### ruffé…ç½®

```toml
# pyproject.toml
[tool.ruff]
target-version = "py313"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
    "N",  # pep8-naming
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "D103"]

[tool.ruff.isort]
known-first-party = ["pump_station"]

[tool.ruff.pydocstyle]
convention = "google"
```

### pre-commité…ç½®

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
```

### è´¨é‡é—¨ç¦è„šæœ¬

```bash
#!/bin/bash
# scripts/quality_check.sh

set -e

echo "ğŸ” è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥..."

# ä»£ç æ ¼å¼æ£€æŸ¥
echo "ğŸ“ æ£€æŸ¥ä»£ç æ ¼å¼..."
ruff check .
ruff format --check .

# ç±»å‹æ£€æŸ¥
echo "ğŸ”§ è¿è¡Œç±»å‹æ£€æŸ¥..."
mypy .

# è¿è¡Œæµ‹è¯•
echo "ğŸ§ª è¿è¡Œæµ‹è¯•å¥—ä»¶..."
pytest --cov=pump_station --cov-report=term-missing --cov-fail-under=80



echo "âœ… æ‰€æœ‰è´¨é‡æ£€æŸ¥é€šè¿‡ï¼"
```

## ğŸ¯ ç¼–ç æœ€ä½³å®è·µ

### è´¨é‡é˜ˆå€¼ä¸æ£€æŸ¥

- **æµ‹è¯•è¦†ç›–ç‡**ï¼šâ‰¥ 80%ï¼ˆæ ¸å¿ƒæ¨¡å— â‰¥ 90%ï¼‰
- **ç±»å‹æ³¨è§£è¦†ç›–ç‡**ï¼šâ‰¥ 95%
- **å¤æ‚åº¦æ§åˆ¶**ï¼šå‡½æ•°å¤æ‚åº¦ â‰¤ 10ï¼Œç±»å¤æ‚åº¦ â‰¤ 50
- **ä»£ç é‡å¤ç‡**ï¼šâ‰¤ 5%
- **å¼ºåˆ¶ä»£ç å®¡æŸ¥**ï¼šæ‰€æœ‰ä»£ç å˜æ›´å¿…é¡»ç»è¿‡å®¡æŸ¥

### æ€§èƒ½ä¼˜åŒ–åŸåˆ™

- **å»¶è¿ŸåŠ è½½**ï¼šæŒ‰éœ€åŠ è½½æ•°æ®å’Œèµ„æº
- **ç¼“å­˜ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨å†…å­˜ç¼“å­˜å’ŒæŒä¹…åŒ–ç¼“å­˜
- **æ‰¹å¤„ç†ä¼˜åŒ–**ï¼šå¤§æ•°æ®é‡æ“ä½œä½¿ç”¨æ‰¹å¤„ç†
- **å¼‚æ­¥å¤„ç†**ï¼šI/O å¯†é›†å‹æ“ä½œä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹

### è°ƒè¯•å‹å¥½è®¾è®¡

- **è¯¦ç»†æ—¥å¿—**ï¼šå…³é”®è·¯å¾„è®°å½•è¯¦ç»†æ—¥å¿—
- **çŠ¶æ€è¿½è¸ª**ï¼šé‡è¦çŠ¶æ€å˜åŒ–å¯è¿½è¸ª
- **é”™è¯¯ä¸Šä¸‹æ–‡**ï¼šå¼‚å¸¸ä¿¡æ¯åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
- **è°ƒè¯•æ¥å£**ï¼šæä¾›è°ƒè¯•å’Œè¯Šæ–­æ¥å£

### å¯ç»´æŠ¤æ€§è®¾è®¡

- **é…ç½®å¤–éƒ¨åŒ–**ï¼šæ‰€æœ‰é…ç½®å‚æ•°å¤–éƒ¨åŒ–
- **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼šAPI å˜æ›´ä¿æŒå‘åå…¼å®¹
- **æ–‡æ¡£åŒæ­¥**ï¼šä»£ç å˜æ›´åŒæ­¥æ›´æ–°æ–‡æ¡£
- **é‡æ„å‹å¥½**ï¼šä»£ç ç»“æ„æ”¯æŒé‡æ„

## ğŸš€ å¹¶å‘ç¼–ç¨‹è§„èŒƒ

### çº¿ç¨‹å®‰å…¨

- **å…±äº«çŠ¶æ€ä¿æŠ¤**ï¼šä½¿ç”¨é”ã€ä¿¡å·é‡ç­‰åŒæ­¥åŸè¯­ä¿æŠ¤å…±äº«çŠ¶æ€
- **æ— é”è®¾è®¡**ï¼šä¼˜å…ˆä½¿ç”¨ä¸å¯å˜å¯¹è±¡å’Œå‡½æ•°å¼ç¼–ç¨‹
- **çº¿ç¨‹æ± ç®¡ç†**ï¼šä½¿ç”¨ `concurrent.futures` ç®¡ç†çº¿ç¨‹æ± 

### å¼‚æ­¥ç¼–ç¨‹

```python
# å¼‚æ­¥æ•°æ®å¤„ç†ç¤ºä¾‹
import asyncio
import aiofiles
from typing import List

async def process_csv_file(file_path: Path) -> pd.DataFrame:
    """å¼‚æ­¥å¤„ç† CSV æ–‡ä»¶ã€‚"""
    async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
        content = await f.read()
        # ä½¿ç”¨ asyncio.to_thread å¤„ç† CPU å¯†é›†å‹ä»»åŠ¡
        return await asyncio.to_thread(pd.read_csv, StringIO(content))

async def batch_process_files(file_paths: List[Path]) -> List[pd.DataFrame]:
    """æ‰¹é‡å¼‚æ­¥å¤„ç†æ–‡ä»¶ã€‚"""
    tasks = [process_csv_file(path) for path in file_paths]
    return await asyncio.gather(*tasks, return_exceptions=True)
```

## ğŸ§ª æµ‹è¯•é©±åŠ¨å¼€å‘è§„èŒƒ

### æµ‹è¯•åˆ†å±‚

- **å•å…ƒæµ‹è¯•**ï¼šæµ‹è¯•å•ä¸ªå‡½æ•°æˆ–æ–¹æ³•çš„åŠŸèƒ½
- **é›†æˆæµ‹è¯•**ï¼šæµ‹è¯•æ¨¡å—é—´çš„äº¤äº’
- **ç«¯åˆ°ç«¯æµ‹è¯•**ï¼šæµ‹è¯•å®Œæ•´çš„ä¸šåŠ¡æµç¨‹
- **æ€§èƒ½æµ‹è¯•**ï¼šæµ‹è¯•ç³»ç»Ÿæ€§èƒ½å’Œè´Ÿè½½èƒ½åŠ›

### æµ‹è¯•ç¼–å†™è§„èŒƒ

```python
import pytest
from unittest.mock import Mock, patch

class TestPumpEfficiencyCalculator:
    """æ³µæ•ˆç‡è®¡ç®—å™¨æµ‹è¯•ç±»ã€‚"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®ã€‚"""
        self.calculator = PumpEfficiencyCalculator()

    def test_calculate_efficiency_normal_case(self):
        """æµ‹è¯•æ­£å¸¸æƒ…å†µä¸‹çš„æ•ˆç‡è®¡ç®—ã€‚"""
        # Arrange
        flow_rate = 100.0
        head = 50.0
        power = 15.0
        expected_efficiency = 0.85

        # Act
        result = self.calculator.calculate_efficiency(flow_rate, head, power)

        # Assert
        assert abs(result - expected_efficiency) < 0.01

    def test_calculate_efficiency_zero_power_raises_error(self):
        """æµ‹è¯•åŠŸç‡ä¸ºé›¶æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚"""
        with pytest.raises(ValueError, match="åŠŸç‡ä¸èƒ½ä¸ºé›¶"):
            self.calculator.calculate_efficiency(100.0, 50.0, 0.0)

    @patch('src.data.database.get_pump_data')
    def test_calculate_efficiency_with_mock(self, mock_get_data):
        """ä½¿ç”¨ Mock æµ‹è¯•æ•°æ®åº“äº¤äº’ã€‚"""
        # Arrange
        mock_get_data.return_value = {'flow': 100, 'head': 50, 'power': 15}

        # Act
        result = self.calculator.calculate_from_db(pump_id=1)

        # Assert
        mock_get_data.assert_called_once_with(pump_id=1)
        assert result > 0
```

### æµ‹è¯•æ•°æ®ç®¡ç†

- **æµ‹è¯•å¤¹å…·**ï¼šä½¿ç”¨ pytest fixtures ç®¡ç†æµ‹è¯•æ•°æ®
- **æ•°æ®éš”ç¦»**ï¼šæ¯ä¸ªæµ‹è¯•ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®
- **æµ‹è¯•æ•°æ®åº“**ï¼šä½¿ç”¨å†…å­˜æ•°æ®åº“æˆ–éš”ç¦»æµ‹è¯•å®ä¾‹ï¼ˆé¿å…ä¾èµ–å®¹å™¨åŒ–ç¯å¢ƒï¼‰

## ğŸ”§ è°ƒè¯•ä¸æ’é”™è§„èŒƒ

### æ—¥å¿—è®¾è®¡

```python
import logging
import structlog
from typing import Any, Dict

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

def process_pump_data(pump_id: int, data: Dict[str, Any]) -> Dict[str, Any]:
    """å¤„ç†æ³µæ•°æ®çš„ç¤ºä¾‹å‡½æ•°ã€‚"""
    logger.info(
        "å¼€å§‹å¤„ç†æ³µæ•°æ®",
        pump_id=pump_id,
        data_size=len(data),
        operation="process_pump_data"
    )

    try:
        # å¤„ç†é€»è¾‘
        result = perform_calculation(data)

        logger.info(
            "æ³µæ•°æ®å¤„ç†å®Œæˆ",
            pump_id=pump_id,
            result_size=len(result),
            processing_time_ms=123.45
        )

        return result

    except Exception as e:
        logger.error(
            "æ³µæ•°æ®å¤„ç†å¤±è´¥",
            pump_id=pump_id,
            error=str(e),
            error_type=type(e).__name__,
            exc_info=True
        )
        raise
```

### é”™è¯¯å¤„ç†ç­–ç•¥

- **åˆ†å±‚å¼‚å¸¸å¤„ç†**ï¼šåœ¨ä¸åŒå±‚æ¬¡å¤„ç†ä¸åŒç±»å‹çš„å¼‚å¸¸
- **å¼‚å¸¸é“¾**ï¼šä¿æŒåŸå§‹å¼‚å¸¸ä¿¡æ¯
- **é‡è¯•æœºåˆ¶**ï¼šå¯¹ä¸´æ—¶æ€§é”™è¯¯å®ç°æ™ºèƒ½é‡è¯•
- **ç†”æ–­å™¨æ¨¡å¼**ï¼šé˜²æ­¢çº§è”æ•…éšœ

```python
import time
from functools import wraps
from typing import Callable, Type, Tuple

def retry_on_exception(
    exceptions: Tuple[Type[Exception], ...],
    max_retries: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0
) -> Callable:
    """å¼‚å¸¸é‡è¯•è£…é¥°å™¨ã€‚"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logger.warning(
                            "å‡½æ•°æ‰§è¡Œå¤±è´¥ï¼Œå‡†å¤‡é‡è¯•",
                            function=func.__name__,
                            attempt=attempt + 1,
                            max_retries=max_retries,
                            wait_time=wait_time,
                            error=str(e)
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(
                            "å‡½æ•°æ‰§è¡Œå¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°",
                            function=func.__name__,
                            max_retries=max_retries,
                            final_error=str(e)
                        )

            raise last_exception
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@retry_on_exception((ConnectionError, TimeoutError), max_retries=3)
def fetch_pump_data(pump_id: int) -> Dict[str, Any]:
    """è·å–æ³µæ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨é‡è¯•ã€‚"""
    # ç½‘ç»œè¯·æ±‚é€»è¾‘
    pass
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

### æ€§èƒ½ç›‘æ§

```python
import time
import psutil
from functools import wraps
from typing import Callable

def monitor_performance(func: Callable) -> Callable:
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨ã€‚"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss

        try:
            result = func(*args, **kwargs)

            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss

            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory

            logger.info(
                "å‡½æ•°æ€§èƒ½ç›‘æ§",
                function=func.__name__,
                execution_time_ms=execution_time * 1000,
                memory_delta_mb=memory_delta / 1024 / 1024,
                args_count=len(args),
                kwargs_count=len(kwargs)
            )

            return result

        except Exception as e:
            end_time = time.perf_counter()
            execution_time = end_time - start_time

            logger.error(
                "å‡½æ•°æ‰§è¡Œå¼‚å¸¸",
                function=func.__name__,
                execution_time_ms=execution_time * 1000,
                error=str(e)
            )
            raise

    return wrapper
```

### å†…å­˜ç®¡ç†

- **å¤§æ•°æ®å¤„ç†**ï¼šä½¿ç”¨ç”Ÿæˆå™¨å’Œè¿­ä»£å™¨å‡å°‘å†…å­˜å ç”¨
- **ç¼“å­˜ç®¡ç†**ï¼šå®ç° LRU ç¼“å­˜å’Œç¼“å­˜è¿‡æœŸæœºåˆ¶
- **å†…å­˜æ³„æ¼æ£€æµ‹**ï¼šå®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ

## ğŸ“‹ å¼€å‘æµç¨‹æ£€æŸ¥æ¸…å•

### ğŸ”„ ç‰ˆæœ¬æ§åˆ¶è§„èŒƒ

#### Git æäº¤è§„èŒƒ

```bash
# æäº¤ä¿¡æ¯æ ¼å¼ï¼š<type>(<scope>): <description>
# ç±»å‹è¯´æ˜ï¼š
# feat: æ–°åŠŸèƒ½
# fix: ä¿®å¤ bug
# docs: æ–‡æ¡£æ›´æ–°
# style: ä»£ç æ ¼å¼è°ƒæ•´ï¼ˆä¸å½±å“åŠŸèƒ½ï¼‰
# refactor: é‡æ„ä»£ç 
# test: æ·»åŠ æˆ–ä¿®æ”¹æµ‹è¯•
# chore: æ„å»ºè¿‡ç¨‹æˆ–è¾…åŠ©å·¥å…·çš„å˜åŠ¨

# ç¤ºä¾‹
git commit -m "feat(data): æ·»åŠ  CSV æ•°æ®å¯¼å…¥åŠŸèƒ½"
git commit -m "fix(pump): ä¿®å¤æ•ˆç‡è®¡ç®—ç²¾åº¦é—®é¢˜"
git commit -m "docs(api): æ›´æ–° API æ–‡æ¡£"
```

#### åˆ†æ”¯ç®¡ç†ç­–ç•¥

- **main/master**ï¼šç”Ÿäº§ç¯å¢ƒåˆ†æ”¯ï¼Œåªæ¥å—ç»è¿‡æµ‹è¯•çš„ä»£ç 
- **develop**ï¼šå¼€å‘ä¸»åˆ†æ”¯ï¼Œé›†æˆæ‰€æœ‰åŠŸèƒ½åˆ†æ”¯
- **feature/xxx**ï¼šåŠŸèƒ½å¼€å‘åˆ†æ”¯ï¼Œä» develop åˆ†å‡º
- **hotfix/xxx**ï¼šç´§æ€¥ä¿®å¤åˆ†æ”¯ï¼Œä» main åˆ†å‡º
- **release/xxx**ï¼šå‘å¸ƒå‡†å¤‡åˆ†æ”¯ï¼Œä» develop åˆ†å‡º

#### ä»£ç å®¡æŸ¥æµç¨‹

1. **åˆ›å»º Pull Request**ï¼šè¯¦ç»†æè¿°å˜æ›´å†…å®¹å’Œæµ‹è¯•æƒ…å†µ
1. **è‡ªåŠ¨åŒ–æ£€æŸ¥**ï¼šCI/CD æµæ°´çº¿è‡ªåŠ¨è¿è¡Œæµ‹è¯•å’Œè´¨é‡æ£€æŸ¥
1. **äººå·¥å®¡æŸ¥**ï¼šè‡³å°‘ä¸€ååŒäº‹è¿›è¡Œä»£ç å®¡æŸ¥
1. **ä¿®æ”¹å®Œå–„**ï¼šæ ¹æ®å®¡æŸ¥æ„è§ä¿®æ”¹ä»£ç 
1. **åˆå¹¶ä»£ç **ï¼šå®¡æŸ¥é€šè¿‡ååˆå¹¶åˆ°ç›®æ ‡åˆ†æ”¯

### ğŸš€ æŒç»­é›†æˆ/æŒç»­éƒ¨ç½² (CI/CD)

#### GitHub Actions é…ç½®ç¤ºä¾‹

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12, 3.13]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Lint with ruff
      run: |
        ruff check .
        ruff format --check .

    - name: Type check with mypy
      run: mypy src/



    - name: Run tests with pytest
      run: |
        pytest --cov=src --cov-report=xml --cov-report=html

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### ğŸ“¦ ä¾èµ–ç®¡ç†

#### pyproject.toml é…ç½®

```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "pump-optimization-system"
version = "0.1.0"
description = "æ³µç«™è¿è¡Œæ•°æ®ä¼˜åŒ–ç³»ç»Ÿ"
authors = [{name = "Your Name", email = "your.email@example.com"}]
readme = "README.md"
requires-python = ">=3.11"
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = [
    "fastapi>=0.104.0",
    "sqlalchemy>=2.0.0",
    "pandas>=2.1.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "pydantic>=2.4.0",
    "structlog>=23.1.0",
    "python-dotenv>=1.0.0",
    "asyncio-mqtt>=0.13.0",
    "aiofiles>=23.2.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.21.0",
    "mypy>=1.5.0",
    "ruff>=0.0.290",
    "bandit>=1.7.5",
    "pre-commit>=3.4.0",
    "black>=23.7.0",
    "isort>=5.12.0",
]
test = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.11.0",
    "factory-boy>=3.3.0",
]
docs = [
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.2.0",
    "mkdocstrings[python]>=0.22.0",
]

[tool.ruff]
target-version = "py311"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*" = ["S101", "S106"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = ["tests"]
markers = [
    "slow: marks tests as slow (deselect with '-m "not slow"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
]

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\bProtocol\):",
    "@(abc\.)?abstractmethod",
]
```

### ğŸ” ä»£ç è´¨é‡æ£€æŸ¥æ¸…å•

#### æäº¤å‰æ£€æŸ¥ (Pre-commit)

```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.0.290
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.5.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]


```

#### å¼€å‘ç¯å¢ƒè®¾ç½®æ£€æŸ¥æ¸…å•

- [ ] Python 3.11+ å·²å®‰è£…
- [ ] è™šæ‹Ÿç¯å¢ƒå·²åˆ›å»ºå¹¶æ¿€æ´»
- [ ] ä¾èµ–åŒ…å·²å®‰è£…ï¼ˆ`pip install -e ".[dev]"`ï¼‰
- [ ] Pre-commit hooks å·²å®‰è£…ï¼ˆ`pre-commit install`ï¼‰
- [ ] IDE é…ç½®äº†ç±»å‹æ£€æŸ¥å’Œä»£ç æ ¼å¼åŒ–
- [ ] ç¯å¢ƒå˜é‡é…ç½®æ–‡ä»¶å·²åˆ›å»ºï¼ˆ`.env`ï¼‰

#### åŠŸèƒ½å¼€å‘æ£€æŸ¥æ¸…å•

- [ ] åŠŸèƒ½éœ€æ±‚æ˜ç¡®ï¼Œæœ‰è¯¦ç»†çš„è®¾è®¡æ–‡æ¡£
- [ ] åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ï¼ˆ`git checkout -b feature/xxx`ï¼‰
- [ ] ç¼–å†™æµ‹è¯•ç”¨ä¾‹ï¼ˆTDD æ–¹å¼ï¼‰
- [ ] å®ç°åŠŸèƒ½ä»£ç 
- [ ] ä»£ç é€šè¿‡æ‰€æœ‰è´¨é‡æ£€æŸ¥
- [ ] æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°è¦æ±‚ï¼ˆâ‰¥80%ï¼‰
- [ ] æ›´æ–°ç›¸å…³æ–‡æ¡£
- [ ] åˆ›å»º Pull Request

#### å‘å¸ƒå‰æ£€æŸ¥æ¸…å•

- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

- [ ] ä»£ç è¦†ç›–ç‡è¾¾æ ‡

- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡

- [ ] æ–‡æ¡£æ›´æ–°å®Œæ•´

- [ ] ç‰ˆæœ¬å·å·²æ›´æ–°

- [ ] å˜æ›´æ—¥å¿—å·²æ›´æ–°

- [ ] ç”Ÿäº§ç¯å¢ƒé…ç½®å·²å‡†å¤‡

### ğŸ› é—®é¢˜æ’æŸ¥æŒ‡å—

#### å¸¸è§é—®é¢˜è¯Šæ–­

1. **æ€§èƒ½é—®é¢˜**

   - æ£€æŸ¥æ—¥å¿—ä¸­çš„æ€§èƒ½ç›‘æ§æ•°æ®
   - ä½¿ç”¨ profiler åˆ†æç“¶é¢ˆ
   - æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
   - åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ

1. **æ•°æ®é—®é¢˜**

   - æ£€æŸ¥æ•°æ®éªŒè¯æ—¥å¿—
   - éªŒè¯æ•°æ®æºå®Œæ•´æ€§
   - æ£€æŸ¥æ•°æ®è½¬æ¢é€»è¾‘
   - åˆ†ææ•°æ®è´¨é‡æŠ¥å‘Š

1. **ç³»ç»Ÿé”™è¯¯**

   - æŸ¥çœ‹ç»“æ„åŒ–æ—¥å¿—
   - æ£€æŸ¥é”™è¯¯å †æ ˆä¿¡æ¯
   - éªŒè¯é…ç½®å‚æ•°
   - æ£€æŸ¥ä¾èµ–æœåŠ¡çŠ¶æ€

#### è°ƒè¯•å·¥å…·æ¨è

- **æ—¥å¿—åˆ†æ**ï¼šELK Stack, Grafana
- **æ€§èƒ½ç›‘æ§**ï¼šPrometheus + Grafana
- **ä»£ç è°ƒè¯•**ï¼špdb, ipdb, VS Code debugger
- **å†…å­˜åˆ†æ**ï¼šmemory_profiler, pympler
- **ç½‘ç»œè°ƒè¯•**ï¼šWireshark, tcpdump

### ğŸ“š æ–‡æ¡£ç»´æŠ¤è§„èŒƒ

#### æ–‡æ¡£ç±»å‹

- **API æ–‡æ¡£**ï¼šè‡ªåŠ¨ç”Ÿæˆï¼Œä¿æŒä¸ä»£ç åŒæ­¥
- **æ¶æ„æ–‡æ¡£**ï¼šç³»ç»Ÿè®¾è®¡å’ŒæŠ€æœ¯å†³ç­–
- **ç”¨æˆ·æ‰‹å†Œ**ï¼šåŠŸèƒ½ä½¿ç”¨è¯´æ˜
- **å¼€å‘æŒ‡å—**ï¼šå¼€å‘ç¯å¢ƒæ­å»ºå’Œè´¡çŒ®æŒ‡å—
- **æ•…éšœæ’é™¤**ï¼šå¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### æ–‡æ¡£æ›´æ–°æµç¨‹

- ä»£ç å˜æ›´æ—¶åŒæ­¥æ›´æ–°ç›¸å…³æ–‡æ¡£
- å®šæœŸå®¡æŸ¥æ–‡æ¡£çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç®¡ç†æ–‡æ¡£å˜æ›´
- å»ºç«‹æ–‡æ¡£åé¦ˆå’Œæ”¹è¿›æœºåˆ¶

### å¼€å‘å‰æ£€æŸ¥

- [ ] åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ï¼š`git checkout -b feature/åŠŸèƒ½åç§°`
- [ ] æ›´æ–°ä¾èµ–ï¼š`pip install -e .[dev]`
- [ ] è¿è¡Œè´¨é‡æ£€æŸ¥ï¼š`./scripts/quality_check.sh`

### å¼€å‘ä¸­æ£€æŸ¥

- [ ] éµå¾ªå‘½åè§„èŒƒ
- [ ] æ·»åŠ ç±»å‹æ³¨è§£
- [ ] ç¼–å†™æ–‡æ¡£å­—ç¬¦ä¸²
- [ ] æ·»åŠ é€‚å½“çš„æ—¥å¿—
- [ ] å¤„ç†å¼‚å¸¸æƒ…å†µ

### æäº¤å‰æ£€æŸ¥

- [ ] è¿è¡Œå®Œæ•´æµ‹è¯•ï¼š`pytest`
- [ ] æ£€æŸ¥ä»£ç è¦†ç›–ç‡ï¼š`pytest --cov`
- [ ] è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼š`pytest -m benchmark`
- [ ] æ›´æ–°æ–‡æ¡£
- [ ] æäº¤ä¿¡æ¯è§„èŒƒ

### ä»£ç å®¡æŸ¥æ£€æŸ¥

- [ ] ä»£ç é€»è¾‘æ­£ç¡®æ€§

- [ ] æ€§èƒ½å½±å“è¯„ä¼°

- [ ] å¯ç»´æŠ¤æ€§è¯„ä¼°

- [ ] æµ‹è¯•è¦†ç›–å……åˆ†æ€§

è¿™å¥—å¼€å‘è§„èŒƒç¡®ä¿äº†ä»£ç çš„é«˜è´¨é‡ã€å¯ç»´æŠ¤æ€§å’Œå›¢é˜Ÿåä½œæ•ˆç‡ï¼Œä¸ºæ³µç«™æ•°æ®ä¼˜åŒ–ç³»ç»Ÿçš„æˆåŠŸå¼€å‘æä¾›äº†åšå®çš„åŸºç¡€ã€‚
