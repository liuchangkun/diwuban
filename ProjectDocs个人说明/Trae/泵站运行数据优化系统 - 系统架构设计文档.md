# æ³µç«™è¿è¡Œæ•°æ®ä¼˜åŒ–ç³»ç»Ÿ - ç³»ç»Ÿæ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ”§ å¦‚ä½•ä½¿ç”¨æœ¬è§„èŒƒ

æœ¬è§„èŒƒé¢å‘ç³»ç»Ÿæ¶æ„å¸ˆã€å¼€å‘å·¥ç¨‹å¸ˆã€æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œæä¾›å®Œæ•´çš„ç³»ç»Ÿæ¶æ„è®¾è®¡æŒ‡å¯¼å’Œå®ç°è·¯å¾„ã€‚

### å¼€å‘è€…è§†è§’

- **æ–°å…¥èŒå·¥ç¨‹å¸ˆ**ï¼šæŒ‰ 2.1 èŠ‚é¡¹ç›®ç›®å½•ç»“æ„åˆå§‹åŒ–å¼€å‘ç¯å¢ƒ â†’ æŒ‰æŠ€æœ¯æ ˆæ¸…å•å®‰è£…ä¾èµ– â†’ é˜…è¯»æ ¸å¿ƒæœåŠ¡æ¶æ„ç†è§£ç³»ç»Ÿè®¾è®¡
- **æ¨¡å—å¼€å‘**ï¼šå‚è€ƒ 2.2 èŠ‚æœåŠ¡å±‚è®¾è®¡ï¼ŒæŒ‰æ¥å£è§„èŒƒå®ç°å„ä¸šåŠ¡æ¨¡å—
- **æ•°æ®åº“å¼€å‘**ï¼šé…åˆ [æ•°æ®åº“è®¾è®¡æ–‡æ¡£](database_design.md) å®Œæˆè¡¨ç»“æ„è®¾è®¡

### æµ‹è¯•è€…è§†è§’

- **é›†æˆæµ‹è¯•**ï¼šæŒ‰æœåŠ¡è¾¹ç•Œè®¾è®¡æµ‹è¯•ç”¨ä¾‹ï¼Œé‡ç‚¹éªŒè¯æ•°æ®æµå’ŒæœåŠ¡é—´é€šä¿¡
- **æ€§èƒ½æµ‹è¯•**ï¼šå‚è€ƒ 1.2 æŠ€æœ¯æ ˆæ€§èƒ½ç‰¹æ€§ï¼Œè®¾è®¡è´Ÿè½½æµ‹è¯•åœºæ™¯

### è¿ç»´è§†è§’

- **éƒ¨ç½²æ¶æ„**ï¼šæŒ‰åˆ†å±‚æ¶æ„è¿›è¡Œç¯å¢ƒè§„åˆ’ï¼Œç¡®ä¿å„å±‚ç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•
- **ç›‘æ§è®¾è®¡**ï¼šåŸºäºå¾®æœåŠ¡æ¶æ„ç‰¹ç‚¹è®¾è®¡ç›‘æ§æŒ‡æ ‡å’Œå‘Šè­¦ç­–ç•¥

______________________________________________________________________

## ğŸš€ å¼€å‘-æµ‹è¯•-ä¼˜åŒ–å¿«é€Ÿå¯¼èˆª

### å¼€å‘é˜¶æ®µ

1. **ç¯å¢ƒæ­å»º** â†’ [å¼€å‘æ ‡å‡†è§„èŒƒ](development_standards.md)
1. **é¡¹ç›®ç»“æ„** â†’ æœ¬æ–‡æ¡£ 2.1 èŠ‚
1. **æ•°æ®åº“è®¾è®¡** â†’ [æ•°æ®åº“è®¾è®¡æ–‡æ¡£](database_design.md)
1. **APIå¼€å‘** â†’ [APIæ¥å£æ–‡æ¡£](api_documentation.md)

### æµ‹è¯•é˜¶æ®µ

1. **æµ‹è¯•æ¡†æ¶** â†’ [æµ‹è¯•ä¸è´¨é‡ä¿è¯è§„èŒƒ](testing_quality_assurance_specifications.md)
1. **æ•°æ®éªŒè¯** â†’ [æ•°æ®å¤„ç†è§„èŒƒ](data_processing_specifications.md)
1. **ç®—æ³•æµ‹è¯•** â†’ [æµ‹è¯•ä¸è´¨é‡ä¿è¯è§„èŒƒ](testing_quality_assurance_specifications.md#4-%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E7%B2%BE%E5%BA%A6%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97)

### ä¼˜åŒ–é˜¶æ®µ

1. **æ€§èƒ½ç›‘æ§** â†’ [å®æ—¶å¤„ç†è§„èŒƒ](real_time_processing_specifications.md)
1. **ç®—æ³•ä¼˜åŒ–** â†’ [ä¼˜åŒ–ç®—æ³•è§„èŒƒ](optimization_algorithms_specifications.md)
1. **å·¥ç¨‹å‚æ•°** â†’ [æ°´æ³µå·¥ç¨‹å‚æ•°è®¡ç®—è§„èŒƒ](pump_engineering_parameter_calculation_specifications.md)

______________________________________________________________________

## 1. æ¶æ„æ¦‚è¿°

### 1.1 ç³»ç»Ÿæ¶æ„åŸåˆ™

- **æ¨¡å—åŒ–è®¾è®¡**: é«˜å†…èšã€ä½è€¦åˆçš„æ¨¡å—åŒ–æ¶æ„
- **åˆ†å±‚æ¶æ„**: æ¸…æ™°çš„åˆ†å±‚ç»“æ„ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **å¾®æœåŠ¡ç†å¿µ**: æœåŠ¡åŒ–çš„ç»„ä»¶è®¾è®¡ï¼Œæ”¯æŒç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•
- **æ•°æ®é©±åŠ¨**: ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ¶æ„è®¾è®¡
- **é«˜æ€§èƒ½**: æ”¯æŒå¤§æ•°æ®é‡å¤„ç†å’Œå®æ—¶è®¡ç®—
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ°´å¹³å’Œå‚ç›´æ‰©å±•
- **å®¹é”™æ€§**: å…·å¤‡æ•…éšœæ¢å¤å’Œé™çº§èƒ½åŠ›

### 1.2 æŠ€æœ¯æ ˆé€‰æ‹©

#### åç«¯æŠ€æœ¯æ ˆ

- **Webæ¡†æ¶**: FastAPI 0.104+ (é«˜æ€§èƒ½å¼‚æ­¥æ¡†æ¶)
- **ORM**: SQLAlchemy 2.0+ (ç°ä»£åŒ–ORM)
- **æ•°æ®åº“**: MySQL 8.0.12 (ä¸»æ•°æ®åº“) + Redis 7.0+ (ç¼“å­˜)
- **æ•°æ®å¤„ç†**: pandas 2.1+, NumPy 1.24+
- **æœºå™¨å­¦ä¹ **: scikit-learn 1.3+, TensorFlow 2.13+, PyTorch 2.0+
- **ä¼˜åŒ–ç®—æ³•**: SciPy 1.11+, DEAP 1.4+, Optuna 3.4+
- **ä»»åŠ¡é˜Ÿåˆ—**: Celery 5.3+ + Redis
- **é…ç½®ç®¡ç†**: Pydantic Settings 2.0+
- **æ—¥å¿—**: structlog 23.1+
- **ç›‘æ§**: Prometheus + Grafana
- **æµ‹è¯•**: pytest 7.4+, pytest-asyncio

#### å¼€å‘å·¥å…·

- **ä»£ç è´¨é‡**: ruff (linting + formatting), mypy (ç±»å‹æ£€æŸ¥)
- **ä¾èµ–ç®¡ç†**: Poetry 1.6+
- **éƒ¨ç½²**: æ ‡å‡†Pythonéƒ¨ç½²
- **CI/CD**: GitHub Actions
- **æ–‡æ¡£**: MkDocs + Material

### 1.3 ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å‰ç«¯å±•ç¤ºå±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web Dashboard  â”‚  Mobile App  â”‚  API Client  â”‚  Monitoring UI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        APIç½‘å…³å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     FastAPI Gateway     â”‚     é™æµç†”æ–­        â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ä¸šåŠ¡æœåŠ¡å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®ç®¡ç†æœåŠ¡   â”‚   æ›²çº¿æ‹ŸåˆæœåŠ¡ â”‚   ä¼˜åŒ–è®¡ç®—æœåŠ¡ â”‚   ç›‘æ§å‘Šè­¦æœåŠ¡ â”‚
â”‚                â”‚               â”‚               â”‚               â”‚
â”‚ â€¢ æ•°æ®å¯¼å…¥      â”‚ â€¢ ç‰¹æ€§æ›²çº¿æ‹Ÿåˆ â”‚ â€¢ å¤šç›®æ ‡ä¼˜åŒ–   â”‚ â€¢ å®æ—¶ç›‘æ§     â”‚
â”‚ â€¢ æ•°æ®è¡¥é½      â”‚ â€¢ æ¨¡å‹éªŒè¯     â”‚ â€¢ ç®—æ³•é›†æˆ     â”‚ â€¢ å¼‚å¸¸æ£€æµ‹     â”‚
â”‚ â€¢ è´¨é‡æ£€æŸ¥      â”‚ â€¢ ç²¾åº¦è¯„ä¼°     â”‚ â€¢ æ–¹æ¡ˆç”Ÿæˆ     â”‚ â€¢ æŠ¥è­¦æ¨é€     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®è®¿é—®å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   å…³ç³»æ•°æ®åº“     â”‚   ç¼“å­˜ç³»ç»Ÿ     â”‚   æ–‡ä»¶å­˜å‚¨     â”‚   æ¶ˆæ¯é˜Ÿåˆ—     â”‚
â”‚                â”‚               â”‚               â”‚               â”‚
â”‚ â€¢ MySQL 8.0    â”‚ â€¢ Redis 7.0   â”‚ â€¢ æœ¬åœ°æ–‡ä»¶     â”‚ â€¢ Celery      â”‚
â”‚ â€¢ è¿è¡Œæ•°æ®      â”‚ â€¢ ä¼šè¯ç¼“å­˜     â”‚ â€¢ CSVæ•°æ®     â”‚ â€¢ å¼‚æ­¥ä»»åŠ¡     â”‚
â”‚ â€¢ é…ç½®ä¿¡æ¯      â”‚ â€¢ è®¡ç®—ç¼“å­˜     â”‚ â€¢ æ¨¡å‹æ–‡ä»¶     â”‚ â€¢ å®šæ—¶ä»»åŠ¡     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åŸºç¡€è®¾æ–½å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   åº”ç”¨éƒ¨ç½²       â”‚   ç›‘æ§æ—¥å¿—     â”‚   æ•°æ®å¤„ç†     â”‚   ç½‘ç»œé€šä¿¡     â”‚
â”‚                â”‚               â”‚               â”‚               â”‚
â”‚ â€¢ Pythonåº”ç”¨   â”‚ â€¢ æ—¥å¿—è®°å½•     â”‚ â€¢ æ•°æ®éªŒè¯     â”‚ â€¢ HTTP/HTTPS  â”‚
â”‚ â€¢ è¿›ç¨‹ç®¡ç†      â”‚ â€¢ æ€§èƒ½ç›‘æ§     â”‚ â€¢ æ ¼å¼æ ¡éªŒ     â”‚ â€¢ WebSocket   â”‚
â”‚ â€¢ å¥åº·æ£€æŸ¥      â”‚ â€¢ é”™è¯¯è¿½è¸ª     â”‚ â€¢ è´¨é‡æ£€æŸ¥     â”‚ â€¢ REST API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 2.1 é¡¹ç›®ç›®å½•ç»“æ„

```
pump_station_optimization/
â”œâ”€â”€ app/                          # åº”ç”¨ä¸»ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # FastAPIåº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config/                   # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py           # åº”ç”¨é…ç½®
â”‚   â”‚   â”œâ”€â”€ database.py           # æ•°æ®åº“é…ç½®
â”‚   â”‚   â””â”€â”€ logging.py            # æ—¥å¿—é…ç½®
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py       # ä¾èµ–æ³¨å…¥
â”‚   â”‚   â”œâ”€â”€ exceptions.py         # å¼‚å¸¸å¤„ç†
â”‚   â”‚   â””â”€â”€ middleware.py         # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ api/                      # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/                   # APIç‰ˆæœ¬1
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/        # ç«¯ç‚¹å®šä¹‰
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ stations.py   # æ³µç«™ç®¡ç†
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ devices.py    # è®¾å¤‡ç®¡ç†
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data.py       # æ•°æ®ç®¡ç†
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ curves.py     # ç‰¹æ€§æ›²çº¿
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ optimization.py # ä¼˜åŒ–ç®—æ³•
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ monitoring.py # ç›‘æ§æ¥å£
â”‚   â”‚   â”‚   â””â”€â”€ router.py         # è·¯ç”±æ±‡æ€»
â”‚   â”‚   â””â”€â”€ websocket/            # WebSocketæ¥å£
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ manager.py        # è¿æ¥ç®¡ç†
â”‚   â”‚       â””â”€â”€ handlers.py       # æ¶ˆæ¯å¤„ç†
â”‚   â”œâ”€â”€ models/                   # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py               # åŸºç¡€æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ station.py            # æ³µç«™æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ device.py             # è®¾å¤‡æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ operation_data.py     # è¿è¡Œæ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ curve.py              # ç‰¹æ€§æ›²çº¿æ¨¡å‹
â”‚   â”‚   â””â”€â”€ optimization.py       # ä¼˜åŒ–ç»“æœæ¨¡å‹
â”‚   â”œâ”€â”€ schemas/                  # Pydanticæ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py               # åŸºç¡€æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ station.py            # æ³µç«™æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ device.py             # è®¾å¤‡æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ data.py               # æ•°æ®æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ curve.py              # æ›²çº¿æ¨¡å¼
â”‚   â”‚   â””â”€â”€ optimization.py       # ä¼˜åŒ–æ¨¡å¼
â”‚   â”œâ”€â”€ services/                 # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py               # åŸºç¡€æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ data_import.py        # æ•°æ®å¯¼å…¥æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ data_completion.py    # æ•°æ®è¡¥é½æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ curve_fitting.py      # æ›²çº¿æ‹ŸåˆæœåŠ¡
â”‚   â”‚   â”œâ”€â”€ optimization.py       # ä¼˜åŒ–æœåŠ¡
â”‚   â”‚   â””â”€â”€ monitoring.py         # ç›‘æ§æœåŠ¡
â”‚   â”œâ”€â”€ algorithms/               # ç®—æ³•æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ curve_fitting/        # æ›²çº¿æ‹Ÿåˆç®—æ³•
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py           # åŸºç¡€æ‹Ÿåˆç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ polynomial.py     # å¤šé¡¹å¼æ‹Ÿåˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ machine_learning.py # æœºå™¨å­¦ä¹ æ‹Ÿåˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ deep_learning.py  # æ·±åº¦å­¦ä¹ æ‹Ÿåˆ
â”‚   â”‚   â”‚   â””â”€â”€ ensemble.py       # é›†æˆæ–¹æ³•
â”‚   â”‚   â”œâ”€â”€ optimization/         # ä¼˜åŒ–ç®—æ³•
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py           # åŸºç¡€ä¼˜åŒ–ç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ genetic.py        # é—ä¼ ç®—æ³•
â”‚   â”‚   â”‚   â”œâ”€â”€ particle_swarm.py # ç²’å­ç¾¤ç®—æ³•
â”‚   â”‚   â”‚   â”œâ”€â”€ differential.py   # å·®åˆ†è¿›åŒ–
â”‚   â”‚   â”‚   â””â”€â”€ multi_objective.py # å¤šç›®æ ‡ä¼˜åŒ–
â”‚   â”‚   â””â”€â”€ data_processing/      # æ•°æ®å¤„ç†ç®—æ³•
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ interpolation.py  # æ’å€¼ç®—æ³•
â”‚   â”‚       â”œâ”€â”€ anomaly_detection.py # å¼‚å¸¸æ£€æµ‹
â”‚   â”‚       â””â”€â”€ quality_assessment.py # è´¨é‡è¯„ä¼°
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py           # æ•°æ®åº“å·¥å…·
â”‚   â”‚   â”œâ”€â”€ cache.py              # ç¼“å­˜å·¥å…·
â”‚   â”‚   â”œâ”€â”€ file_handler.py       # æ–‡ä»¶å¤„ç†
â”‚   â”‚   â”œâ”€â”€ math_utils.py         # æ•°å­¦å·¥å…·
â”‚   â”‚   â”œâ”€â”€ time_utils.py         # æ—¶é—´å·¥å…·
â”‚   â”‚   â””â”€â”€ validation.py         # éªŒè¯å·¥å…·
â”‚   â””â”€â”€ tasks/                    # å¼‚æ­¥ä»»åŠ¡
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ celery_app.py         # Celeryåº”ç”¨
â”‚       â”œâ”€â”€ data_import.py        # æ•°æ®å¯¼å…¥ä»»åŠ¡
â”‚       â”œâ”€â”€ curve_fitting.py      # æ›²çº¿æ‹Ÿåˆä»»åŠ¡
â”‚       â”œâ”€â”€ optimization.py       # ä¼˜åŒ–ä»»åŠ¡
â”‚       â””â”€â”€ monitoring.py         # ç›‘æ§ä»»åŠ¡
â”œâ”€â”€ tests/                        # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py               # æµ‹è¯•é…ç½®
â”‚   â”œâ”€â”€ unit/                     # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ integration/              # é›†æˆæµ‹è¯•
â”‚   â””â”€â”€ performance/              # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ scripts/                      # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ init_db.py                # æ•°æ®åº“åˆå§‹åŒ–
â”‚   â”œâ”€â”€ import_data.py            # æ•°æ®å¯¼å…¥è„šæœ¬
â”‚   â””â”€â”€ backup_db.py              # æ•°æ®åº“å¤‡ä»½
â”œâ”€â”€ docs/                         # æ–‡æ¡£ç›®å½•
â”œâ”€â”€ data/                         # æ•°æ®ç›®å½•
â”œâ”€â”€ logs/                         # æ—¥å¿—ç›®å½•
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ logging.conf
â”œâ”€â”€ .env.example                  # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ pyproject.toml                # é¡¹ç›®é…ç½®
â”œâ”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
â””â”€â”€ requirements.txt              # ä¾èµ–åˆ—è¡¨
```

### 2.2 æ ¸å¿ƒæœåŠ¡æ¶æ„

#### 2.2.1 æ•°æ®ç®¡ç†æœåŠ¡ (Data Management Service)

```python
# app/services/data_import.py
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.operation_data import OperationData
from app.schemas.data import DataImportConfig, ImportResult
from app.utils.file_handler import CSVProcessor
from app.utils.validation import DataValidator

class DataImportService:
    """æ•°æ®å¯¼å…¥æœåŠ¡

    è´Ÿè´£å¤„ç†CSVæ–‡ä»¶å¯¼å…¥ã€æ•°æ®éªŒè¯ã€æ‰¹é‡æ’å…¥ç­‰åŠŸèƒ½ã€‚
    æ”¯æŒå¤§æ–‡ä»¶åˆ†å—å¤„ç†å’Œå¹¶å‘å¯¼å…¥ã€‚
    """

    def __init__(self, db: AsyncSession, batch_size: int = 10000):
        self.db = db
        self.batch_size = batch_size
        self.csv_processor = CSVProcessor()
        self.validator = DataValidator()

    async def import_csv_file(
        self,
        file_path: Path,
        config: DataImportConfig
    ) -> ImportResult:
        """å¯¼å…¥CSVæ–‡ä»¶

        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            config: å¯¼å…¥é…ç½®

        Returns:
            ImportResult: å¯¼å…¥ç»“æœç»Ÿè®¡
        """
        try:
            # æ–‡ä»¶éªŒè¯
            await self._validate_file(file_path)

            # åˆ†å—è¯»å–å’Œå¤„ç†
            total_processed = 0
            total_errors = 0

            async for chunk in self.csv_processor.read_chunks(
                file_path,
                chunk_size=self.batch_size
            ):
                # æ•°æ®è½¬æ¢å’ŒéªŒè¯
                processed_data = await self._process_chunk(chunk, config)

                # æ‰¹é‡æ’å…¥
                result = await self._batch_insert(processed_data)

                total_processed += result.success_count
                total_errors += result.error_count

            return ImportResult(
                total_records=total_processed + total_errors,
                success_count=total_processed,
                error_count=total_errors,
                processing_time=time.time() - start_time
            )

        except Exception as e:
            logger.error(f"æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            raise

    async def _process_chunk(
        self,
        chunk: pd.DataFrame,
        config: DataImportConfig
    ) -> List[OperationData]:
        """å¤„ç†æ•°æ®å—"""
        processed_data = []

        for _, row in chunk.iterrows():
            try:
                # æ•°æ®è½¬æ¢
                data_point = self._transform_row(row, config)

                # æ•°æ®éªŒè¯
                if await self.validator.validate_data_point(data_point):
                    processed_data.append(data_point)

            except Exception as e:
                logger.warning(f"æ•°æ®è¡Œå¤„ç†å¤±è´¥: {e}")
                continue

        return processed_data
```

#### 2.2.2 ç‰¹æ€§æ›²çº¿æ‹ŸåˆæœåŠ¡ (Curve Fitting Service)

```python
# app/services/curve_fitting.py
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from app.algorithms.curve_fitting.base import BaseFittingAlgorithm
from app.algorithms.curve_fitting.ensemble import EnsembleFitting
from app.schemas.curve import CurveFittingRequest, FittingResult

class CurveFittingService:
    """ç‰¹æ€§æ›²çº¿æ‹ŸåˆæœåŠ¡

    é›†æˆå¤šç§æ‹Ÿåˆç®—æ³•ï¼Œæ”¯æŒä¼ ç»Ÿæ•°å­¦æ–¹æ³•ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ã€‚
    æä¾›æ¨¡å‹éªŒè¯ã€ç²¾åº¦è¯„ä¼°å’Œç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥ã€‚
    """

    def __init__(self):
        self.algorithms = {
            'polynomial': PolynomialFitting(),
            'svr': SVRFitting(),
            'random_forest': RandomForestFitting(),
            'mlp': MLPFitting(),
            'ensemble': EnsembleFitting()
        }

    async def fit_characteristic_curve(
        self,
        request: CurveFittingRequest
    ) -> FittingResult:
        """æ‹Ÿåˆç‰¹æ€§æ›²çº¿

        Args:
            request: æ‹Ÿåˆè¯·æ±‚å‚æ•°

        Returns:
            FittingResult: æ‹Ÿåˆç»“æœ
        """
        try:
            # æ•°æ®å‡†å¤‡
            X, y = await self._prepare_training_data(request)

            # æ•°æ®é¢„å¤„ç†
            X_processed, y_processed = await self._preprocess_data(X, y)

            # æ‰§è¡Œæ‹Ÿåˆ
            results = {}
            for method in request.fitting_methods:
                if method in self.algorithms:
                    algorithm = self.algorithms[method]
                    result = await algorithm.fit(X_processed, y_processed)
                    results[method] = result

            # æ¨¡å‹é›†æˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if request.ensemble.enabled and len(results) > 1:
                ensemble_result = await self._create_ensemble(results)
                results['ensemble'] = ensemble_result

            # é€‰æ‹©æœ€ä½³æ¨¡å‹
            best_result = self._select_best_model(results)

            # éªŒè¯å’Œè¯„ä¼°
            validation_metrics = await self._validate_model(
                best_result, X_processed, y_processed
            )

            return FittingResult(
                model_parameters=best_result.parameters,
                performance_metrics=validation_metrics,
                fitting_method=best_result.method,
                r_squared=validation_metrics.r_squared,
                rmse=validation_metrics.rmse,
                physics_consistency=validation_metrics.physics_score
            )

        except Exception as e:
            logger.error(f"æ›²çº¿æ‹Ÿåˆå¤±è´¥: {e}")
            raise

    async def _prepare_training_data(
        self,
        request: CurveFittingRequest
    ) -> tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # ä»æ•°æ®åº“è·å–è¿è¡Œæ•°æ®
        query = select(OperationData).where(
            OperationData.device_id == request.device_id,
            OperationData.data_time.between(
                request.data_source.time_range.start,
                request.data_source.time_range.end
            )
        )

        data = await self.db.execute(query)
        df = pd.DataFrame(data.fetchall())

        # æ•°æ®é€è§†å’Œç‰¹å¾å·¥ç¨‹
        pivot_df = df.pivot_table(
            index='data_time',
            columns='parameter_type',
            values='data_value'
        )

        # æ ¹æ®æ›²çº¿ç±»å‹é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡
        if request.curve_type == 'head_flow':
            X = pivot_df[['flow_rate']].values
            y = pivot_df['head'].values
        elif request.curve_type == 'efficiency_flow':
            X = pivot_df[['flow_rate']].values
            y = pivot_df['efficiency'].values
        # ... å…¶ä»–æ›²çº¿ç±»å‹

        return X, y
```

#### 2.2.3 ä¼˜åŒ–è®¡ç®—æœåŠ¡ (Optimization Service)

```python
# app/services/optimization.py
from typing import List, Dict, Any, Optional
import numpy as np
from scipy.optimize import differential_evolution
from deap import base, creator, tools, algorithms
from app.algorithms.optimization.base import BaseOptimizer
from app.algorithms.optimization.multi_objective import NSGAIIOptimizer
from app.schemas.optimization import OptimizationRequest, OptimizationResult

class OptimizationService:
    """ä¼˜åŒ–è®¡ç®—æœåŠ¡

    æä¾›å¤šç§ä¼˜åŒ–ç®—æ³•ï¼Œæ”¯æŒå•ç›®æ ‡å’Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚
    é›†æˆé—ä¼ ç®—æ³•ã€ç²’å­ç¾¤ç®—æ³•ã€å·®åˆ†è¿›åŒ–ç­‰ç»å…¸ç®—æ³•ã€‚
    """

    def __init__(self):
        self.optimizers = {
            'genetic_algorithm': GeneticAlgorithmOptimizer(),
            'particle_swarm': ParticleSwarmOptimizer(),
            'differential_evolution': DifferentialEvolutionOptimizer(),
            'nsga2': NSGAIIOptimizer(),
            'reinforcement_learning': RLOptimizer()
        }

    async def optimize_pump_operation(
        self,
        request: OptimizationRequest
    ) -> OptimizationResult:
        """ä¼˜åŒ–æ³µç»„è¿è¡Œ

        Args:
            request: ä¼˜åŒ–è¯·æ±‚å‚æ•°

        Returns:
            OptimizationResult: ä¼˜åŒ–ç»“æœ
        """
        try:
            # æ„å»ºä¼˜åŒ–é—®é¢˜
            problem = await self._build_optimization_problem(request)

            # æ‰§è¡Œä¼˜åŒ–ç®—æ³•
            results = {}
            for algorithm in request.algorithms:
                if algorithm in self.optimizers:
                    optimizer = self.optimizers[algorithm]
                    result = await optimizer.optimize(problem)
                    results[algorithm] = result

            # é€‰æ‹©æœ€ä½³è§£
            best_solution = self._select_best_solution(results)

            # è§£çš„éªŒè¯å’Œè¯„ä¼°
            validation_result = await self._validate_solution(
                best_solution, problem
            )

            # è®¡ç®—æ€§èƒ½æ”¹è¿›
            improvement = await self._calculate_improvement(
                best_solution, request.station_id
            )

            return OptimizationResult(
                best_solution=best_solution,
                objective_values=best_solution.objectives,
                algorithm_performance=results,
                performance_improvement=improvement,
                validation_metrics=validation_result
            )

        except Exception as e:
            logger.error(f"ä¼˜åŒ–è®¡ç®—å¤±è´¥: {e}")
            raise

    async def _build_optimization_problem(
        self,
        request: OptimizationRequest
    ) -> OptimizationProblem:
        """æ„å»ºä¼˜åŒ–é—®é¢˜"""
        # è·å–è®¾å¤‡ç‰¹æ€§æ›²çº¿
        curves = await self._get_characteristic_curves(request.station_id)

        # å®šä¹‰å†³ç­–å˜é‡
        variables = self._define_decision_variables(request)

        # å®šä¹‰ç›®æ ‡å‡½æ•°
        objectives = self._define_objectives(request.objectives, curves)

        # å®šä¹‰çº¦æŸæ¡ä»¶
        constraints = self._define_constraints(request.constraints)

        return OptimizationProblem(
            variables=variables,
            objectives=objectives,
            constraints=constraints,
            curves=curves
        )
```

### 2.3 æ•°æ®è®¿é—®å±‚è®¾è®¡

#### 2.3.1 æ•°æ®åº“è¿æ¥ç®¡ç†

```python
# app/config/database.py
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    async_sessionmaker
)
from sqlalchemy.pool import QueuePool
from app.config.settings import get_settings

settings = get_settings()

# å¼‚æ­¥æ•°æ®åº“å¼•æ“
engine = create_async_engine(
    settings.database_url,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=settings.debug
)

# ä¼šè¯å·¥å‚
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_db() -> AsyncSession:
    """è·å–æ•°æ®åº“ä¼šè¯"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

#### 2.3.2 ç¼“å­˜ç®¡ç†

```python
# app/utils/cache.py
import json
import pickle
from typing import Any, Optional, Union
from redis.asyncio import Redis
from app.config.settings import get_settings

settings = get_settings()

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨

    æä¾›Redisç¼“å­˜çš„ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒJSONå’ŒPickleåºåˆ—åŒ–ã€‚
    """

    def __init__(self):
        self.redis = Redis.from_url(
            settings.redis_url,
            decode_responses=False
        )

    async def get(
        self,
        key: str,
        serializer: str = 'json'
    ) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        try:
            value = await self.redis.get(key)
            if value is None:
                return None

            if serializer == 'json':
                return json.loads(value)
            elif serializer == 'pickle':
                return pickle.loads(value)
            else:
                return value.decode('utf-8')

        except Exception as e:
            logger.warning(f"ç¼“å­˜è·å–å¤±è´¥: {e}")
            return None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: int = 3600,
        serializer: str = 'json'
    ) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            if serializer == 'json':
                serialized_value = json.dumps(value, default=str)
            elif serializer == 'pickle':
                serialized_value = pickle.dumps(value)
            else:
                serialized_value = str(value)

            await self.redis.setex(key, ttl, serialized_value)
            return True

        except Exception as e:
            logger.warning(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            result = await self.redis.delete(key)
            return result > 0
        except Exception as e:
            logger.warning(f"ç¼“å­˜åˆ é™¤å¤±è´¥: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        try:
            result = await self.redis.exists(key)
            return result > 0
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")
            return False

# å…¨å±€ç¼“å­˜å®ä¾‹
cache = CacheManager()
```

### 2.4 å¼‚æ­¥ä»»åŠ¡æ¶æ„

#### 2.4.1 Celeryé…ç½®

```python
# app/tasks/celery_app.py
from celery import Celery
from app.config.settings import get_settings

settings = get_settings()

# Celeryåº”ç”¨å®ä¾‹
celery_app = Celery(
    "pump_station_optimization",
    broker=settings.redis_url,
    backend=settings.redis_url,
    include=[
        'app.tasks.data_import',
        'app.tasks.curve_fitting',
        'app.tasks.optimization',
        'app.tasks.monitoring'
    ]
)

# Celeryé…ç½®
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30åˆ†é’Ÿè¶…æ—¶
    task_soft_time_limit=25 * 60,  # 25åˆ†é’Ÿè½¯è¶…æ—¶
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
    beat_schedule={
        'data_quality_check': {
            'task': 'app.tasks.monitoring.check_data_quality',
            'schedule': 300.0,  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
        },
        'system_health_check': {
            'task': 'app.tasks.monitoring.check_system_health',
            'schedule': 60.0,  # æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
        },
    }
)
```

#### 2.4.2 æ•°æ®å¯¼å…¥ä»»åŠ¡

```python
# app/tasks/data_import.py
from celery import current_task
from app.tasks.celery_app import celery_app
from app.services.data_import import DataImportService
from app.schemas.data import DataImportConfig

@celery_app.task(bind=True)
def import_csv_data(
    self,
    file_path: str,
    station_id: int,
    device_id: int,
    import_config: dict
) -> dict:
    """å¼‚æ­¥å¯¼å…¥CSVæ•°æ®

    Args:
        file_path: CSVæ–‡ä»¶è·¯å¾„
        station_id: æ³µç«™ID
        device_id: è®¾å¤‡ID
        import_config: å¯¼å…¥é…ç½®

    Returns:
        dict: å¯¼å…¥ç»“æœ
    """
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        current_task.update_state(
            state='PROGRESS',
            meta={'status': 'å¼€å§‹å¯¼å…¥æ•°æ®', 'progress': 0}
        )

        # åˆ›å»ºå¯¼å…¥æœåŠ¡
        config = DataImportConfig(**import_config)
        service = DataImportService()

        # æ‰§è¡Œå¯¼å…¥
        result = service.import_csv_file_sync(
            Path(file_path),
            config,
            progress_callback=lambda p: current_task.update_state(
                state='PROGRESS',
                meta={'status': 'æ­£åœ¨å¯¼å…¥æ•°æ®', 'progress': p}
            )
        )

        return {
            'status': 'SUCCESS',
            'result': result.dict()
        }

    except Exception as e:
        logger.error(f"æ•°æ®å¯¼å…¥ä»»åŠ¡å¤±è´¥: {e}")
        current_task.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise
```

### 2.5 APIè®¾è®¡æ¶æ„

#### 2.5.1 FastAPIåº”ç”¨é…ç½®

```python
# app/main.py
from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from contextlib import asynccontextmanager
from app.config.settings import get_settings
from app.core.middleware import (
    LoggingMiddleware,
    RateLimitMiddleware
)
from app.api.v1.router import api_router
from app.api.websocket.manager import websocket_manager

settings = get_settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("åº”ç”¨å¯åŠ¨ä¸­...")

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
    await init_database()

    # åˆå§‹åŒ–ç¼“å­˜è¿æ¥
    await init_cache()

    # å¯åŠ¨WebSocketç®¡ç†å™¨
    await websocket_manager.start()

    logger.info("åº”ç”¨å¯åŠ¨å®Œæˆ")

    yield

    # å…³é—­æ—¶æ¸…ç†
    logger.info("åº”ç”¨å…³é—­ä¸­...")

    # å…³é—­æ•°æ®åº“è¿æ¥
    await close_database()

    # å…³é—­ç¼“å­˜è¿æ¥
    await close_cache()

    # åœæ­¢WebSocketç®¡ç†å™¨
    await websocket_manager.stop()

    logger.info("åº”ç”¨å…³é—­å®Œæˆ")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="æ³µç«™è¿è¡Œæ•°æ®ä¼˜åŒ–ç³»ç»Ÿ",
    description="é«˜æ€§èƒ½æ³µç«™æ•°æ®åˆ†æä¸ä¼˜åŒ–å¹³å°",
    version="1.0.0",
    docs_url="/docs" if settings.debug else None,
    redoc_url="/redoc" if settings.debug else None,
    lifespan=lifespan
)

# æ·»åŠ ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(LoggingMiddleware)
app.add_middleware(RateLimitMiddleware)

# æ³¨å†Œè·¯ç”±
app.include_router(api_router, prefix="/api/v1")

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0"
    }

# æŒ‡æ ‡ç«¯ç‚¹ï¼ˆPrometheusï¼‰
@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡"""
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    return Response(
        generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )
```

#### 2.5.2 ä¸­é—´ä»¶è®¾è®¡

```python
# app/core/middleware.py
import time
import uuid
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from app.utils.logging import get_logger
from app.utils.cache import cache

logger = get_logger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""

    async def dispatch(
        self,
        request: Request,
        call_next: Callable
    ) -> Response:
        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id

        # è®°å½•è¯·æ±‚å¼€å§‹
        start_time = time.time()

        logger.info(
            "è¯·æ±‚å¼€å§‹",
            extra={
                "request_id": request_id,
                "method": request.method,
                "url": str(request.url),
                "client_ip": request.client.host,
                "user_agent": request.headers.get("user-agent")
            }
        )

        # å¤„ç†è¯·æ±‚
        response = await call_next(request)

        # è®°å½•è¯·æ±‚ç»“æŸ
        process_time = time.time() - start_time

        logger.info(
            "è¯·æ±‚å®Œæˆ",
            extra={
                "request_id": request_id,
                "status_code": response.status_code,
                "process_time": process_time
            }
        )

        # æ·»åŠ å“åº”å¤´
        response.headers["X-Request-ID"] = request_id
        response.headers["X-Process-Time"] = str(process_time)

        return response

class RateLimitMiddleware(BaseHTTPMiddleware):
    """é™æµä¸­é—´ä»¶"""

    def __init__(self, app, calls: int = 100, period: int = 60):
        super().__init__(app)
        self.calls = calls
        self.period = period

    async def dispatch(
        self,
        request: Request,
        call_next: Callable
    ) -> Response:
        # è·å–å®¢æˆ·ç«¯IP
        client_ip = request.client.host

        # æ£€æŸ¥é™æµ
        if await self._is_rate_limited(client_ip):
            return Response(
                content="Rate limit exceeded",
                status_code=429,
                headers={"Retry-After": str(self.period)}
            )

        # è®°å½•è¯·æ±‚
        await self._record_request(client_ip)

        return await call_next(request)

    async def _is_rate_limited(self, client_ip: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™æµ"""
        key = f"rate_limit:{client_ip}"
        current_calls = await cache.get(key) or 0
        return current_calls >= self.calls

    async def _record_request(self, client_ip: str) -> None:
        """è®°å½•è¯·æ±‚æ¬¡æ•°"""
        key = f"rate_limit:{client_ip}"
        current_calls = await cache.get(key) or 0
        await cache.set(key, current_calls + 1, ttl=self.period)
```

### 2.6 ç›‘æ§å’Œæ—¥å¿—æ¶æ„

#### 2.6.1 ç»“æ„åŒ–æ—¥å¿—

```python
# app/config/logging.py
import structlog
from typing import Any, Dict
from app.config.settings import get_settings

settings = get_settings()

def configure_logging():
    """é…ç½®ç»“æ„åŒ–æ—¥å¿—"""

    # é…ç½®structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer() if settings.log_format == "json"
            else structlog.dev.ConsoleRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

def get_logger(name: str) -> structlog.BoundLogger:
    """è·å–ç»“æ„åŒ–æ—¥å¿—å™¨"""
    return structlog.get_logger(name)
```

#### 2.6.2 æ€§èƒ½ç›‘æ§

```python
# app/utils/monitoring.py
import time
import psutil
from typing import Dict, Any
from prometheus_client import Counter, Histogram, Gauge
from app.utils.logging import get_logger

logger = get_logger(__name__)

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

SYSTEM_MEMORY_USAGE = Gauge(
    'system_memory_usage_bytes',
    'System memory usage'
)

SYSTEM_CPU_USAGE = Gauge(
    'system_cpu_usage_percent',
    'System CPU usage percentage'
)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = time.time()

    def record_request(
        self,
        method: str,
        endpoint: str,
        status: int,
        duration: float
    ):
        """è®°å½•HTTPè¯·æ±‚æŒ‡æ ‡"""
        REQUEST_COUNT.labels(
            method=method,
            endpoint=endpoint,
            status=status
        ).inc()

        REQUEST_DURATION.labels(
            method=method,
            endpoint=endpoint
        ).observe(duration)

    def update_system_metrics(self):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        SYSTEM_MEMORY_USAGE.set(memory.used)

        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        SYSTEM_CPU_USAGE.set(cpu_percent)

        logger.debug(
            "ç³»ç»ŸæŒ‡æ ‡æ›´æ–°",
            memory_used=memory.used,
            memory_percent=memory.percent,
            cpu_percent=cpu_percent
        )

    def get_uptime(self) -> float:
        """è·å–è¿è¡Œæ—¶é—´"""
        return time.time() - self.start_time

# å…¨å±€ç›‘æ§å®ä¾‹
monitor = PerformanceMonitor()
```

## 3. éƒ¨ç½²æ¶æ„

### 3.1 åº”ç”¨éƒ¨ç½²

åº”ç”¨é‡‡ç”¨æ ‡å‡†Pythonéƒ¨ç½²æ–¹å¼ï¼š

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨åº”ç”¨
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### 3.2 æœåŠ¡é…ç½®

```yaml
# config/services.yml
services:
  web:
    port: 8000
    environment:
      - DATABASE_URL=mysql+aiomysql://root:password@localhost:3306/pump_station_optimization
      - REDIS_URL=redis://localhost:6379/0
      - LOG_LEVEL=INFO

  mysql:
    port: 3306
    database: pump_station_optimization

  redis:
    port: 6379
    persistence: true
```

### 3.3 ç”Ÿäº§ç¯å¢ƒé…ç½®

ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®ï¼š
cpus: '2.0'
memory: 4G
reservations:
cpus: '1.0'
memory: 2G
restart_policy:
condition: on-failure
delay: 5s
max_attempts: 3
environment:
\- ENV=production
\- LOG_LEVEL=WARNING
\- WORKERS=4
healthcheck:
test: \["CMD", "curl", "-f", "http://localhost:8000/health"\]
interval: 30s
timeout: 10s
retries: 3
start_period: 40s

worker:
image: pump-station-optimization:latest
deploy:
replicas: 2
resources:
limits:
cpus: '4.0'
memory: 8G
reservations:
cpus: '2.0'
memory: 4G
command: >
celery -A app.tasks.celery_app worker
--loglevel=warning
--concurrency=4
--max-tasks-per-child=1000

mysql:
image: mysql:8.0
deploy:
resources:
limits:
cpus: '2.0'
memory: 4G
reservations:
cpus: '1.0'
memory: 2G
command: >
--default-authentication-plugin=mysql_native_password
--innodb-buffer-pool-size=2G
--innodb-log-file-size=512M
--max-connections=500
--query-cache-size=256M

```



---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-28
**ç»´æŠ¤å›¢é˜Ÿ**: æ³µç«™ä¼˜åŒ–ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ
**å®¡æ ¸çŠ¶æ€**: å¾…å®¡æ ¸
```
