# 泵站运行数据优化系统 - 系统架构设计文档

## 🔧 如何使用本规范

本规范面向系统架构师、开发工程师、测试工程师，提供完整的系统架构设计指导和实现路径。

### 开发者视角

- **新入职工程师**：按 2.1 节项目目录结构初始化开发环境 → 按技术栈清单安装依赖 → 阅读核心服务架构理解系统设计
- **模块开发**：参考 2.2 节服务层设计，按接口规范实现各业务模块
- **数据库开发**：配合 [数据库设计文档](database_design.md) 完成表结构设计

### 测试者视角

- **集成测试**：按服务边界设计测试用例，重点验证数据流和服务间通信
- **性能测试**：参考 1.2 技术栈性能特性，设计负载测试场景

### 运维视角

- **部署架构**：按分层架构进行环境规划，确保各层独立部署和扩展
- **监控设计**：基于微服务架构特点设计监控指标和告警策略

______________________________________________________________________

## 🚀 开发-测试-优化快速导航

### 开发阶段

1. **环境搭建** → [开发标准规范](development_standards.md)
1. **项目结构** → 本文档 2.1 节
1. **数据库设计** → [数据库设计文档](database_design.md)
1. **API开发** → [API接口文档](api_documentation.md)

### 测试阶段

1. **测试框架** → [测试与质量保证规范](testing_quality_assurance_specifications.md)
1. **数据验证** → [数据处理规范](data_processing_specifications.md)
1. **算法测试** → [测试与质量保证规范](testing_quality_assurance_specifications.md#4-%E6%9B%B2%E7%BA%BF%E6%8B%9F%E5%90%88%E7%B2%BE%E5%BA%A6%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A8%A1%E5%9D%97)

### 优化阶段

1. **性能监控** → [实时处理规范](real_time_processing_specifications.md)
1. **算法优化** → [优化算法规范](optimization_algorithms_specifications.md)
1. **工程参数** → [水泵工程参数计算规范](pump_engineering_parameter_calculation_specifications.md)

______________________________________________________________________

## 1. 架构概述

### 1.1 系统架构原则

- **模块化设计**: 高内聚、低耦合的模块化架构
- **分层架构**: 清晰的分层结构，便于维护和扩展
- **微服务理念**: 服务化的组件设计，支持独立部署和扩展
- **数据驱动**: 以数据为中心的架构设计
- **高性能**: 支持大数据量处理和实时计算
- **可扩展性**: 支持水平和垂直扩展
- **容错性**: 具备故障恢复和降级能力

### 1.2 技术栈选择

#### 后端技术栈

- **Web框架**: FastAPI 0.104+ (高性能异步框架)
- **ORM**: SQLAlchemy 2.0+ (现代化ORM)
- **数据库**: MySQL 8.0.12 (主数据库) + Redis 7.0+ (缓存)
- **数据处理**: pandas 2.1+, NumPy 1.24+
- **机器学习**: scikit-learn 1.3+, TensorFlow 2.13+, PyTorch 2.0+
- **优化算法**: SciPy 1.11+, DEAP 1.4+, Optuna 3.4+
- **任务队列**: Celery 5.3+ + Redis
- **配置管理**: Pydantic Settings 2.0+
- **日志**: structlog 23.1+
- **监控**: Prometheus + Grafana
- **测试**: pytest 7.4+, pytest-asyncio

#### 开发工具

- **代码质量**: ruff (linting + formatting), mypy (类型检查)
- **依赖管理**: Poetry 1.6+
- **部署**: 标准Python部署
- **CI/CD**: GitHub Actions
- **文档**: MkDocs + Material

### 1.3 系统架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                        前端展示层                                │
├─────────────────────────────────────────────────────────────────┤
│  Web Dashboard  │  Mobile App  │  API Client  │  Monitoring UI  │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                        API网关层                                 │
├─────────────────────────────────────────────────────────────────┤
│     FastAPI Gateway     │     限流熔断        │                  │
└─────────────────────────────────────────────────────────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                        业务服务层                                │
├─────────────────┬───────────────┬───────────────┬───────────────┤
│   数据管理服务   │   曲线拟合服务 │   优化计算服务 │   监控告警服务 │
│                │               │               │               │
│ • 数据导入      │ • 特性曲线拟合 │ • 多目标优化   │ • 实时监控     │
│ • 数据补齐      │ • 模型验证     │ • 算法集成     │ • 异常检测     │
│ • 质量检查      │ • 精度评估     │ • 方案生成     │ • 报警推送     │
└─────────────────┴───────────────┴───────────────┴───────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                        数据访问层                                │
├─────────────────┬───────────────┬───────────────┬───────────────┤
│   关系数据库     │   缓存系统     │   文件存储     │   消息队列     │
│                │               │               │               │
│ • MySQL 8.0    │ • Redis 7.0   │ • 本地文件     │ • Celery      │
│ • 运行数据      │ • 会话缓存     │ • CSV数据     │ • 异步任务     │
│ • 配置信息      │ • 计算缓存     │ • 模型文件     │ • 定时任务     │
└─────────────────┴───────────────┴───────────────┴───────────────┘
                                │
                                ▼
┌─────────────────────────────────────────────────────────────────┐
│                        基础设施层                                │
├─────────────────┬───────────────┬───────────────┬───────────────┤
│   应用部署       │   监控日志     │   数据处理     │   网络通信     │
│                │               │               │               │
│ • Python应用   │ • 日志记录     │ • 数据验证     │ • HTTP/HTTPS  │
│ • 进程管理      │ • 性能监控     │ • 格式校验     │ • WebSocket   │
│ • 健康检查      │ • 错误追踪     │ • 质量检查     │ • REST API    │
└─────────────────┴───────────────┴───────────────┴───────────────┘
```

## 2. 核心模块设计

### 2.1 项目目录结构

```
pump_station_optimization/
├── app/                          # 应用主目录
│   ├── __init__.py
│   ├── main.py                   # FastAPI应用入口
│   ├── config/                   # 配置管理
│   │   ├── __init__.py
│   │   ├── settings.py           # 应用配置
│   │   ├── database.py           # 数据库配置
│   │   └── logging.py            # 日志配置
│   ├── core/                     # 核心模块
│   │   ├── __init__.py
│   │   ├── dependencies.py       # 依赖注入
│   │   ├── exceptions.py         # 异常处理
│   │   └── middleware.py         # 中间件
│   ├── api/                      # API路由
│   │   ├── __init__.py
│   │   ├── v1/                   # API版本1
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/        # 端点定义
│   │   │   │   ├── __init__.py
│   │   │   │   ├── stations.py   # 泵站管理
│   │   │   │   ├── devices.py    # 设备管理
│   │   │   │   ├── data.py       # 数据管理
│   │   │   │   ├── curves.py     # 特性曲线
│   │   │   │   ├── optimization.py # 优化算法
│   │   │   │   └── monitoring.py # 监控接口
│   │   │   └── router.py         # 路由汇总
│   │   └── websocket/            # WebSocket接口
│   │       ├── __init__.py
│   │       ├── manager.py        # 连接管理
│   │       └── handlers.py       # 消息处理
│   ├── models/                   # 数据模型
│   │   ├── __init__.py
│   │   ├── base.py               # 基础模型
│   │   ├── station.py            # 泵站模型
│   │   ├── device.py             # 设备模型
│   │   ├── operation_data.py     # 运行数据模型
│   │   ├── curve.py              # 特性曲线模型
│   │   └── optimization.py       # 优化结果模型
│   ├── schemas/                  # Pydantic模式
│   │   ├── __init__.py
│   │   ├── base.py               # 基础模式
│   │   ├── station.py            # 泵站模式
│   │   ├── device.py             # 设备模式
│   │   ├── data.py               # 数据模式
│   │   ├── curve.py              # 曲线模式
│   │   └── optimization.py       # 优化模式
│   ├── services/                 # 业务服务
│   │   ├── __init__.py
│   │   ├── base.py               # 基础服务
│   │   ├── data_import.py        # 数据导入服务
│   │   ├── data_completion.py    # 数据补齐服务
│   │   ├── curve_fitting.py      # 曲线拟合服务
│   │   ├── optimization.py       # 优化服务
│   │   └── monitoring.py         # 监控服务
│   ├── algorithms/               # 算法模块
│   │   ├── __init__.py
│   │   ├── curve_fitting/        # 曲线拟合算法
│   │   │   ├── __init__.py
│   │   │   ├── base.py           # 基础拟合类
│   │   │   ├── polynomial.py     # 多项式拟合
│   │   │   ├── machine_learning.py # 机器学习拟合
│   │   │   ├── deep_learning.py  # 深度学习拟合
│   │   │   └── ensemble.py       # 集成方法
│   │   ├── optimization/         # 优化算法
│   │   │   ├── __init__.py
│   │   │   ├── base.py           # 基础优化类
│   │   │   ├── genetic.py        # 遗传算法
│   │   │   ├── particle_swarm.py # 粒子群算法
│   │   │   ├── differential.py   # 差分进化
│   │   │   └── multi_objective.py # 多目标优化
│   │   └── data_processing/      # 数据处理算法
│   │       ├── __init__.py
│   │       ├── interpolation.py  # 插值算法
│   │       ├── anomaly_detection.py # 异常检测
│   │       └── quality_assessment.py # 质量评估
│   ├── utils/                    # 工具模块
│   │   ├── __init__.py
│   │   ├── database.py           # 数据库工具
│   │   ├── cache.py              # 缓存工具
│   │   ├── file_handler.py       # 文件处理
│   │   ├── math_utils.py         # 数学工具
│   │   ├── time_utils.py         # 时间工具
│   │   └── validation.py         # 验证工具
│   └── tasks/                    # 异步任务
│       ├── __init__.py
│       ├── celery_app.py         # Celery应用
│       ├── data_import.py        # 数据导入任务
│       ├── curve_fitting.py      # 曲线拟合任务
│       ├── optimization.py       # 优化任务
│       └── monitoring.py         # 监控任务
├── tests/                        # 测试目录
│   ├── __init__.py
│   ├── conftest.py               # 测试配置
│   ├── unit/                     # 单元测试
│   ├── integration/              # 集成测试
│   └── performance/              # 性能测试
├── scripts/                      # 脚本目录
│   ├── init_db.py                # 数据库初始化
│   ├── import_data.py            # 数据导入脚本
│   └── backup_db.py              # 数据库备份
├── docs/                         # 文档目录
├── data/                         # 数据目录
├── logs/                         # 日志目录
├── config/                       # 配置文件
│   ├── settings.py
│   ├── database.py
│   └── logging.conf
├── .env.example                  # 环境变量示例
├── pyproject.toml                # 项目配置
├── README.md                     # 项目说明
└── requirements.txt              # 依赖列表
```

### 2.2 核心服务架构

#### 2.2.1 数据管理服务 (Data Management Service)

```python
# app/services/data_import.py
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.operation_data import OperationData
from app.schemas.data import DataImportConfig, ImportResult
from app.utils.file_handler import CSVProcessor
from app.utils.validation import DataValidator

class DataImportService:
    """数据导入服务

    负责处理CSV文件导入、数据验证、批量插入等功能。
    支持大文件分块处理和并发导入。
    """

    def __init__(self, db: AsyncSession, batch_size: int = 10000):
        self.db = db
        self.batch_size = batch_size
        self.csv_processor = CSVProcessor()
        self.validator = DataValidator()

    async def import_csv_file(
        self,
        file_path: Path,
        config: DataImportConfig
    ) -> ImportResult:
        """导入CSV文件

        Args:
            file_path: CSV文件路径
            config: 导入配置

        Returns:
            ImportResult: 导入结果统计
        """
        try:
            # 文件验证
            await self._validate_file(file_path)

            # 分块读取和处理
            total_processed = 0
            total_errors = 0

            async for chunk in self.csv_processor.read_chunks(
                file_path,
                chunk_size=self.batch_size
            ):
                # 数据转换和验证
                processed_data = await self._process_chunk(chunk, config)

                # 批量插入
                result = await self._batch_insert(processed_data)

                total_processed += result.success_count
                total_errors += result.error_count

            return ImportResult(
                total_records=total_processed + total_errors,
                success_count=total_processed,
                error_count=total_errors,
                processing_time=time.time() - start_time
            )

        except Exception as e:
            logger.error(f"数据导入失败: {e}")
            raise

    async def _process_chunk(
        self,
        chunk: pd.DataFrame,
        config: DataImportConfig
    ) -> List[OperationData]:
        """处理数据块"""
        processed_data = []

        for _, row in chunk.iterrows():
            try:
                # 数据转换
                data_point = self._transform_row(row, config)

                # 数据验证
                if await self.validator.validate_data_point(data_point):
                    processed_data.append(data_point)

            except Exception as e:
                logger.warning(f"数据行处理失败: {e}")
                continue

        return processed_data
```

#### 2.2.2 特性曲线拟合服务 (Curve Fitting Service)

```python
# app/services/curve_fitting.py
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from app.algorithms.curve_fitting.base import BaseFittingAlgorithm
from app.algorithms.curve_fitting.ensemble import EnsembleFitting
from app.schemas.curve import CurveFittingRequest, FittingResult

class CurveFittingService:
    """特性曲线拟合服务

    集成多种拟合算法，支持传统数学方法、机器学习和深度学习。
    提供模型验证、精度评估和物理一致性检查。
    """

    def __init__(self):
        self.algorithms = {
            'polynomial': PolynomialFitting(),
            'svr': SVRFitting(),
            'random_forest': RandomForestFitting(),
            'mlp': MLPFitting(),
            'ensemble': EnsembleFitting()
        }

    async def fit_characteristic_curve(
        self,
        request: CurveFittingRequest
    ) -> FittingResult:
        """拟合特性曲线

        Args:
            request: 拟合请求参数

        Returns:
            FittingResult: 拟合结果
        """
        try:
            # 数据准备
            X, y = await self._prepare_training_data(request)

            # 数据预处理
            X_processed, y_processed = await self._preprocess_data(X, y)

            # 执行拟合
            results = {}
            for method in request.fitting_methods:
                if method in self.algorithms:
                    algorithm = self.algorithms[method]
                    result = await algorithm.fit(X_processed, y_processed)
                    results[method] = result

            # 模型集成（如果启用）
            if request.ensemble.enabled and len(results) > 1:
                ensemble_result = await self._create_ensemble(results)
                results['ensemble'] = ensemble_result

            # 选择最佳模型
            best_result = self._select_best_model(results)

            # 验证和评估
            validation_metrics = await self._validate_model(
                best_result, X_processed, y_processed
            )

            return FittingResult(
                model_parameters=best_result.parameters,
                performance_metrics=validation_metrics,
                fitting_method=best_result.method,
                r_squared=validation_metrics.r_squared,
                rmse=validation_metrics.rmse,
                physics_consistency=validation_metrics.physics_score
            )

        except Exception as e:
            logger.error(f"曲线拟合失败: {e}")
            raise

    async def _prepare_training_data(
        self,
        request: CurveFittingRequest
    ) -> tuple[np.ndarray, np.ndarray]:
        """准备训练数据"""
        # 从数据库获取运行数据
        query = select(OperationData).where(
            OperationData.device_id == request.device_id,
            OperationData.data_time.between(
                request.data_source.time_range.start,
                request.data_source.time_range.end
            )
        )

        data = await self.db.execute(query)
        df = pd.DataFrame(data.fetchall())

        # 数据透视和特征工程
        pivot_df = df.pivot_table(
            index='data_time',
            columns='parameter_type',
            values='data_value'
        )

        # 根据曲线类型选择特征和目标
        if request.curve_type == 'head_flow':
            X = pivot_df[['flow_rate']].values
            y = pivot_df['head'].values
        elif request.curve_type == 'efficiency_flow':
            X = pivot_df[['flow_rate']].values
            y = pivot_df['efficiency'].values
        # ... 其他曲线类型

        return X, y
```

#### 2.2.3 优化计算服务 (Optimization Service)

```python
# app/services/optimization.py
from typing import List, Dict, Any, Optional
import numpy as np
from scipy.optimize import differential_evolution
from deap import base, creator, tools, algorithms
from app.algorithms.optimization.base import BaseOptimizer
from app.algorithms.optimization.multi_objective import NSGAIIOptimizer
from app.schemas.optimization import OptimizationRequest, OptimizationResult

class OptimizationService:
    """优化计算服务

    提供多种优化算法，支持单目标和多目标优化。
    集成遗传算法、粒子群算法、差分进化等经典算法。
    """

    def __init__(self):
        self.optimizers = {
            'genetic_algorithm': GeneticAlgorithmOptimizer(),
            'particle_swarm': ParticleSwarmOptimizer(),
            'differential_evolution': DifferentialEvolutionOptimizer(),
            'nsga2': NSGAIIOptimizer(),
            'reinforcement_learning': RLOptimizer()
        }

    async def optimize_pump_operation(
        self,
        request: OptimizationRequest
    ) -> OptimizationResult:
        """优化泵组运行

        Args:
            request: 优化请求参数

        Returns:
            OptimizationResult: 优化结果
        """
        try:
            # 构建优化问题
            problem = await self._build_optimization_problem(request)

            # 执行优化算法
            results = {}
            for algorithm in request.algorithms:
                if algorithm in self.optimizers:
                    optimizer = self.optimizers[algorithm]
                    result = await optimizer.optimize(problem)
                    results[algorithm] = result

            # 选择最佳解
            best_solution = self._select_best_solution(results)

            # 解的验证和评估
            validation_result = await self._validate_solution(
                best_solution, problem
            )

            # 计算性能改进
            improvement = await self._calculate_improvement(
                best_solution, request.station_id
            )

            return OptimizationResult(
                best_solution=best_solution,
                objective_values=best_solution.objectives,
                algorithm_performance=results,
                performance_improvement=improvement,
                validation_metrics=validation_result
            )

        except Exception as e:
            logger.error(f"优化计算失败: {e}")
            raise

    async def _build_optimization_problem(
        self,
        request: OptimizationRequest
    ) -> OptimizationProblem:
        """构建优化问题"""
        # 获取设备特性曲线
        curves = await self._get_characteristic_curves(request.station_id)

        # 定义决策变量
        variables = self._define_decision_variables(request)

        # 定义目标函数
        objectives = self._define_objectives(request.objectives, curves)

        # 定义约束条件
        constraints = self._define_constraints(request.constraints)

        return OptimizationProblem(
            variables=variables,
            objectives=objectives,
            constraints=constraints,
            curves=curves
        )
```

### 2.3 数据访问层设计

#### 2.3.1 数据库连接管理

```python
# app/config/database.py
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    async_sessionmaker
)
from sqlalchemy.pool import QueuePool
from app.config.settings import get_settings

settings = get_settings()

# 异步数据库引擎
engine = create_async_engine(
    settings.database_url,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=settings.debug
)

# 会话工厂
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_db() -> AsyncSession:
    """获取数据库会话"""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()
```

#### 2.3.2 缓存管理

```python
# app/utils/cache.py
import json
import pickle
from typing import Any, Optional, Union
from redis.asyncio import Redis
from app.config.settings import get_settings

settings = get_settings()

class CacheManager:
    """缓存管理器

    提供Redis缓存的统一接口，支持JSON和Pickle序列化。
    """

    def __init__(self):
        self.redis = Redis.from_url(
            settings.redis_url,
            decode_responses=False
        )

    async def get(
        self,
        key: str,
        serializer: str = 'json'
    ) -> Optional[Any]:
        """获取缓存值"""
        try:
            value = await self.redis.get(key)
            if value is None:
                return None

            if serializer == 'json':
                return json.loads(value)
            elif serializer == 'pickle':
                return pickle.loads(value)
            else:
                return value.decode('utf-8')

        except Exception as e:
            logger.warning(f"缓存获取失败: {e}")
            return None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: int = 3600,
        serializer: str = 'json'
    ) -> bool:
        """设置缓存值"""
        try:
            if serializer == 'json':
                serialized_value = json.dumps(value, default=str)
            elif serializer == 'pickle':
                serialized_value = pickle.dumps(value)
            else:
                serialized_value = str(value)

            await self.redis.setex(key, ttl, serialized_value)
            return True

        except Exception as e:
            logger.warning(f"缓存设置失败: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """删除缓存"""
        try:
            result = await self.redis.delete(key)
            return result > 0
        except Exception as e:
            logger.warning(f"缓存删除失败: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """检查缓存是否存在"""
        try:
            result = await self.redis.exists(key)
            return result > 0
        except Exception as e:
            logger.warning(f"缓存检查失败: {e}")
            return False

# 全局缓存实例
cache = CacheManager()
```

### 2.4 异步任务架构

#### 2.4.1 Celery配置

```python
# app/tasks/celery_app.py
from celery import Celery
from app.config.settings import get_settings

settings = get_settings()

# Celery应用实例
celery_app = Celery(
    "pump_station_optimization",
    broker=settings.redis_url,
    backend=settings.redis_url,
    include=[
        'app.tasks.data_import',
        'app.tasks.curve_fitting',
        'app.tasks.optimization',
        'app.tasks.monitoring'
    ]
)

# Celery配置
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30分钟超时
    task_soft_time_limit=25 * 60,  # 25分钟软超时
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
    beat_schedule={
        'data_quality_check': {
            'task': 'app.tasks.monitoring.check_data_quality',
            'schedule': 300.0,  # 每5分钟执行一次
        },
        'system_health_check': {
            'task': 'app.tasks.monitoring.check_system_health',
            'schedule': 60.0,  # 每分钟执行一次
        },
    }
)
```

#### 2.4.2 数据导入任务

```python
# app/tasks/data_import.py
from celery import current_task
from app.tasks.celery_app import celery_app
from app.services.data_import import DataImportService
from app.schemas.data import DataImportConfig

@celery_app.task(bind=True)
def import_csv_data(
    self,
    file_path: str,
    station_id: int,
    device_id: int,
    import_config: dict
) -> dict:
    """异步导入CSV数据

    Args:
        file_path: CSV文件路径
        station_id: 泵站ID
        device_id: 设备ID
        import_config: 导入配置

    Returns:
        dict: 导入结果
    """
    try:
        # 更新任务状态
        current_task.update_state(
            state='PROGRESS',
            meta={'status': '开始导入数据', 'progress': 0}
        )

        # 创建导入服务
        config = DataImportConfig(**import_config)
        service = DataImportService()

        # 执行导入
        result = service.import_csv_file_sync(
            Path(file_path),
            config,
            progress_callback=lambda p: current_task.update_state(
                state='PROGRESS',
                meta={'status': '正在导入数据', 'progress': p}
            )
        )

        return {
            'status': 'SUCCESS',
            'result': result.dict()
        }

    except Exception as e:
        logger.error(f"数据导入任务失败: {e}")
        current_task.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise
```

### 2.5 API设计架构

#### 2.5.1 FastAPI应用配置

```python
# app/main.py
from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from contextlib import asynccontextmanager
from app.config.settings import get_settings
from app.core.middleware import (
    LoggingMiddleware,
    RateLimitMiddleware
)
from app.api.v1.router import api_router
from app.api.websocket.manager import websocket_manager

settings = get_settings()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    # 启动时初始化
    logger.info("应用启动中...")

    # 初始化数据库连接池
    await init_database()

    # 初始化缓存连接
    await init_cache()

    # 启动WebSocket管理器
    await websocket_manager.start()

    logger.info("应用启动完成")

    yield

    # 关闭时清理
    logger.info("应用关闭中...")

    # 关闭数据库连接
    await close_database()

    # 关闭缓存连接
    await close_cache()

    # 停止WebSocket管理器
    await websocket_manager.stop()

    logger.info("应用关闭完成")

# 创建FastAPI应用
app = FastAPI(
    title="泵站运行数据优化系统",
    description="高性能泵站数据分析与优化平台",
    version="1.0.0",
    docs_url="/docs" if settings.debug else None,
    redoc_url="/redoc" if settings.debug else None,
    lifespan=lifespan
)

# 添加中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(LoggingMiddleware)
app.add_middleware(RateLimitMiddleware)

# 注册路由
app.include_router(api_router, prefix="/api/v1")

# 健康检查端点
@app.get("/health")
async def health_check():
    """健康检查"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0"
    }

# 指标端点（Prometheus）
@app.get("/metrics")
async def metrics():
    """Prometheus指标"""
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    return Response(
        generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )
```

#### 2.5.2 中间件设计

```python
# app/core/middleware.py
import time
import uuid
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from app.utils.logging import get_logger
from app.utils.cache import cache

logger = get_logger(__name__)

class LoggingMiddleware(BaseHTTPMiddleware):
    """请求日志中间件"""

    async def dispatch(
        self,
        request: Request,
        call_next: Callable
    ) -> Response:
        # 生成请求ID
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id

        # 记录请求开始
        start_time = time.time()

        logger.info(
            "请求开始",
            extra={
                "request_id": request_id,
                "method": request.method,
                "url": str(request.url),
                "client_ip": request.client.host,
                "user_agent": request.headers.get("user-agent")
            }
        )

        # 处理请求
        response = await call_next(request)

        # 记录请求结束
        process_time = time.time() - start_time

        logger.info(
            "请求完成",
            extra={
                "request_id": request_id,
                "status_code": response.status_code,
                "process_time": process_time
            }
        )

        # 添加响应头
        response.headers["X-Request-ID"] = request_id
        response.headers["X-Process-Time"] = str(process_time)

        return response

class RateLimitMiddleware(BaseHTTPMiddleware):
    """限流中间件"""

    def __init__(self, app, calls: int = 100, period: int = 60):
        super().__init__(app)
        self.calls = calls
        self.period = period

    async def dispatch(
        self,
        request: Request,
        call_next: Callable
    ) -> Response:
        # 获取客户端IP
        client_ip = request.client.host

        # 检查限流
        if await self._is_rate_limited(client_ip):
            return Response(
                content="Rate limit exceeded",
                status_code=429,
                headers={"Retry-After": str(self.period)}
            )

        # 记录请求
        await self._record_request(client_ip)

        return await call_next(request)

    async def _is_rate_limited(self, client_ip: str) -> bool:
        """检查是否超出限流"""
        key = f"rate_limit:{client_ip}"
        current_calls = await cache.get(key) or 0
        return current_calls >= self.calls

    async def _record_request(self, client_ip: str) -> None:
        """记录请求次数"""
        key = f"rate_limit:{client_ip}"
        current_calls = await cache.get(key) or 0
        await cache.set(key, current_calls + 1, ttl=self.period)
```

### 2.6 监控和日志架构

#### 2.6.1 结构化日志

```python
# app/config/logging.py
import structlog
from typing import Any, Dict
from app.config.settings import get_settings

settings = get_settings()

def configure_logging():
    """配置结构化日志"""

    # 配置structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer() if settings.log_format == "json"
            else structlog.dev.ConsoleRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

def get_logger(name: str) -> structlog.BoundLogger:
    """获取结构化日志器"""
    return structlog.get_logger(name)
```

#### 2.6.2 性能监控

```python
# app/utils/monitoring.py
import time
import psutil
from typing import Dict, Any
from prometheus_client import Counter, Histogram, Gauge
from app.utils.logging import get_logger

logger = get_logger(__name__)

# Prometheus指标
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

SYSTEM_MEMORY_USAGE = Gauge(
    'system_memory_usage_bytes',
    'System memory usage'
)

SYSTEM_CPU_USAGE = Gauge(
    'system_cpu_usage_percent',
    'System CPU usage percentage'
)

class PerformanceMonitor:
    """性能监控器"""

    def __init__(self):
        self.start_time = time.time()

    def record_request(
        self,
        method: str,
        endpoint: str,
        status: int,
        duration: float
    ):
        """记录HTTP请求指标"""
        REQUEST_COUNT.labels(
            method=method,
            endpoint=endpoint,
            status=status
        ).inc()

        REQUEST_DURATION.labels(
            method=method,
            endpoint=endpoint
        ).observe(duration)

    def update_system_metrics(self):
        """更新系统指标"""
        # 内存使用
        memory = psutil.virtual_memory()
        SYSTEM_MEMORY_USAGE.set(memory.used)

        # CPU使用率
        cpu_percent = psutil.cpu_percent(interval=1)
        SYSTEM_CPU_USAGE.set(cpu_percent)

        logger.debug(
            "系统指标更新",
            memory_used=memory.used,
            memory_percent=memory.percent,
            cpu_percent=cpu_percent
        )

    def get_uptime(self) -> float:
        """获取运行时间"""
        return time.time() - self.start_time

# 全局监控实例
monitor = PerformanceMonitor()
```

## 3. 部署架构

### 3.1 应用部署

应用采用标准Python部署方式：

```bash
# 安装依赖
pip install -r requirements.txt

# 启动应用
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### 3.2 服务配置

```yaml
# config/services.yml
services:
  web:
    port: 8000
    environment:
      - DATABASE_URL=mysql+aiomysql://root:password@localhost:3306/pump_station_optimization
      - REDIS_URL=redis://localhost:6379/0
      - LOG_LEVEL=INFO

  mysql:
    port: 3306
    database: pump_station_optimization

  redis:
    port: 6379
    persistence: true
```

### 3.3 生产环境配置

生产环境部署配置：
cpus: '2.0'
memory: 4G
reservations:
cpus: '1.0'
memory: 2G
restart_policy:
condition: on-failure
delay: 5s
max_attempts: 3
environment:
\- ENV=production
\- LOG_LEVEL=WARNING
\- WORKERS=4
healthcheck:
test: \["CMD", "curl", "-f", "http://localhost:8000/health"\]
interval: 30s
timeout: 10s
retries: 3
start_period: 40s

worker:
image: pump-station-optimization:latest
deploy:
replicas: 2
resources:
limits:
cpus: '4.0'
memory: 8G
reservations:
cpus: '2.0'
memory: 4G
command: >
celery -A app.tasks.celery_app worker
--loglevel=warning
--concurrency=4
--max-tasks-per-child=1000

mysql:
image: mysql:8.0
deploy:
resources:
limits:
cpus: '2.0'
memory: 4G
reservations:
cpus: '1.0'
memory: 2G
command: >
--default-authentication-plugin=mysql_native_password
--innodb-buffer-pool-size=2G
--innodb-log-file-size=512M
--max-connections=500
--query-cache-size=256M

```



---

**文档版本**: v1.0
**最后更新**: 2025-01-28
**维护团队**: 泵站优化系统开发团队
**审核状态**: 待审核
```
