______________________________________________________________________

## description: 数据库表数据操作规范（增删改查安全性、性能优化、事务管理、审计日志、数据验证）

# 数据库表数据操作规范

## 数据插入（INSERT）规范

### 单条插入操作

```sql
-- 标准插入模板
DELIMITER $$
CREATE PROCEDURE sp_insert_aligned_data(
    IN p_station_id INT,
    IN p_device_id INT,
    IN p_standard_time DATETIME,
    IN p_data_json JSON,
    IN p_data_quality TINYINT DEFAULT 1,
    OUT p_insert_id BIGINT,
    OUT p_result_code INT,
    OUT p_result_message VARCHAR(255)
)
BEGIN
    DECLARE v_data_hash VARCHAR(32);
    DECLARE v_duplicate_count INT DEFAULT 0;

    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        GET DIAGNOSTICS CONDITION 1
            p_result_code = MYSQL_ERRNO,
            p_result_message = MESSAGE_TEXT;
        ROLLBACK;
        SET p_insert_id = 0;
    END;

    START TRANSACTION;

    -- 数据验证
    IF p_station_id IS NULL OR p_device_id IS NULL OR p_standard_time IS NULL THEN
        SET p_result_code = 3001;
        SET p_result_message = '必填字段不能为空';
        ROLLBACK;
    ELSEIF NOT JSON_VALID(p_data_json) THEN
        SET p_result_code = 3002;
        SET p_result_message = '无效的JSON数据格式';
        ROLLBACK;
    ELSEIF p_data_quality NOT BETWEEN 1 AND 4 THEN
        SET p_result_code = 3003;
        SET p_result_message = '数据质量值必须在1-4之间';
        ROLLBACK;
    ELSE
        -- 检查外键约束
        IF NOT EXISTS (SELECT 1 FROM stations WHERE id = p_station_id AND is_active = TRUE) THEN
            SET p_result_code = 3004;
            SET p_result_message = CONCAT('站点不存在或已停用: ', p_station_id);
            ROLLBACK;
        ELSEIF NOT EXISTS (SELECT 1 FROM devices WHERE id = p_device_id AND is_active = TRUE) THEN
            SET p_result_code = 3005;
            SET p_result_message = CONCAT('设备不存在或已停用: ', p_device_id);
            ROLLBACK;
        ELSE
            -- 生成数据哈希
            SET v_data_hash = MD5(CONCAT(p_device_id, '_', p_standard_time, '_', p_data_json));

            -- 检查重复数据
            SELECT COUNT(*) INTO v_duplicate_count
            FROM aligned_data
            WHERE device_id = p_device_id
              AND standard_time = p_standard_time;

            IF v_duplicate_count > 0 THEN
                -- 使用幂等合并策略
                UPDATE aligned_data
                SET data_json = JSON_MERGE_PATCH(data_json, p_data_json),
                    data_quality = LEAST(data_quality, p_data_quality),
                    data_hash = v_data_hash,
                    updated_at = CURRENT_TIMESTAMP
                WHERE device_id = p_device_id
                  AND standard_time = p_standard_time;

                SET p_insert_id = 0; -- 表示更新而非插入
                SET p_result_code = 0;
                SET p_result_message = '数据已存在，执行合并更新';
            ELSE
                -- 执行插入
                INSERT INTO aligned_data (
                    station_id, device_id, standard_time,
                    data_json, data_quality, data_hash
                ) VALUES (
                    p_station_id, p_device_id, p_standard_time,
                    p_data_json, p_data_quality, v_data_hash
                );

                SET p_insert_id = LAST_INSERT_ID();
                SET p_result_code = 0;
                SET p_result_message = '数据插入成功';
            END IF;

            COMMIT;
        END IF;
    END IF;
END$$
DELIMITER ;
```

### 批量插入操作

```sql
-- 批量插入存储过程
DELIMITER $$
CREATE PROCEDURE sp_batch_insert_aligned_data(
    IN p_data_array JSON,
    IN p_batch_size INT DEFAULT 1000,
    OUT p_result_summary JSON
)
BEGIN
    DECLARE v_total_count INT DEFAULT 0;
    DECLARE v_success_count INT DEFAULT 0;
    DECLARE v_error_count INT DEFAULT 0;
    DECLARE v_update_count INT DEFAULT 0;
    DECLARE v_idx INT DEFAULT 0;
    DECLARE v_batch_start INT DEFAULT 0;
    DECLARE v_errors JSON DEFAULT JSON_ARRAY();

    -- 获取数组长度
    SET v_total_count = JSON_LENGTH(p_data_array);

    -- 分批处理
    WHILE v_batch_start < v_total_count DO
        START TRANSACTION;

        SET v_idx = v_batch_start;
        WHILE v_idx < LEAST(v_batch_start + p_batch_size, v_total_count) DO
            BEGIN
                DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
                BEGIN
                    GET DIAGNOSTICS CONDITION 1
                        @error_code = MYSQL_ERRNO,
                        @error_message = MESSAGE_TEXT;
                    SET v_errors = JSON_ARRAY_APPEND(v_errors, '$',
                        JSON_OBJECT('index', v_idx, 'error_code', @error_code, 'error_message', @error_message));
                    SET v_error_count = v_error_count + 1;
                END;

                -- 解析单条数据
                SET @record = JSON_EXTRACT(p_data_array, CONCAT('$[', v_idx, ']'));
                SET @station_id = JSON_UNQUOTE(JSON_EXTRACT(@record, '$.station_id'));
                SET @device_id = JSON_UNQUOTE(JSON_EXTRACT(@record, '$.device_id'));
                SET @standard_time = JSON_UNQUOTE(JSON_EXTRACT(@record, '$.standard_time'));
                SET @data_json = JSON_EXTRACT(@record, '$.data_json');
                SET @data_quality = JSON_UNQUOTE(JSON_EXTRACT(@record, '$.data_quality'));

                -- 生成哈希
                SET @data_hash = MD5(CONCAT(@device_id, '_', @standard_time, '_', @data_json));

                -- 插入或更新
                INSERT INTO aligned_data (
                    station_id, device_id, standard_time,
                    data_json, data_quality, data_hash
                ) VALUES (
                    @station_id, @device_id, @standard_time,
                    @data_json, IFNULL(@data_quality, 1), @data_hash
                ) ON DUPLICATE KEY UPDATE
                    data_json = JSON_MERGE_PATCH(aligned_data.data_json, VALUES(data_json)),
                    data_quality = LEAST(aligned_data.data_quality, VALUES(data_quality)),
                    data_hash = VALUES(data_hash),
                    updated_at = CURRENT_TIMESTAMP;

                IF ROW_COUNT() = 1 THEN
                    SET v_success_count = v_success_count + 1;
                ELSE
                    SET v_update_count = v_update_count + 1;
                END IF;

                SET v_idx = v_idx + 1;
            END;
        END WHILE;

        COMMIT;
        SET v_batch_start = v_batch_start + p_batch_size;
    END WHILE;

    -- 返回处理结果
    SET p_result_summary = JSON_OBJECT(
        'total_count', v_total_count,
        'success_count', v_success_count,
        'update_count', v_update_count,
        'error_count', v_error_count,
        'errors', v_errors,
        'processed_at', NOW()
    );
END$$
DELIMITER ;
```

## 数据更新（UPDATE）规范

### 安全更新操作

```sql
-- 安全更新存储过程
DELIMITER $$
CREATE PROCEDURE sp_update_aligned_data(
    IN p_record_id BIGINT,
    IN p_data_json JSON,
    IN p_data_quality TINYINT,
    IN p_update_reason VARCHAR(255),
    OUT p_result_code INT,
    OUT p_result_message VARCHAR(255)
)
BEGIN
    DECLARE v_old_data JSON;
    DECLARE v_old_quality TINYINT;
    DECLARE v_record_exists INT DEFAULT 0;
    DECLARE v_data_hash VARCHAR(32);

    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        GET DIAGNOSTICS CONDITION 1
            p_result_code = MYSQL_ERRNO,
            p_result_message = MESSAGE_TEXT;
        ROLLBACK;
    END;

    START TRANSACTION;

    -- 检查记录是否存在并获取原始数据
    SELECT COUNT(*), data_json, data_quality
    INTO v_record_exists, v_old_data, v_old_quality
    FROM aligned_data
    WHERE id = p_record_id;

    IF v_record_exists = 0 THEN
        SET p_result_code = 4001;
        SET p_result_message = CONCAT('记录不存在: ID=', p_record_id);
        ROLLBACK;
    ELSEIF NOT JSON_VALID(p_data_json) THEN
        SET p_result_code = 4002;
        SET p_result_message = '无效的JSON数据格式';
        ROLLBACK;
    ELSEIF p_data_quality NOT BETWEEN 1 AND 4 THEN
        SET p_result_code = 4003;
        SET p_result_message = '数据质量值必须在1-4之间';
        ROLLBACK;
    ELSE
        -- 生成新的数据哈希
        SELECT CONCAT(device_id, '_', standard_time, '_', p_data_json)
        INTO @hash_source
        FROM aligned_data WHERE id = p_record_id;

        SET v_data_hash = MD5(@hash_source);

        -- 记录变更历史
        INSERT INTO data_change_log (
            table_name, record_id, operation_type,
            old_values, new_values, change_reason, changed_by
        ) VALUES (
            'aligned_data', p_record_id, 'UPDATE',
            JSON_OBJECT('data_json', v_old_data, 'data_quality', v_old_quality),
            JSON_OBJECT('data_json', p_data_json, 'data_quality', p_data_quality),
            p_update_reason, USER()
        );

        -- 执行更新
        UPDATE aligned_data
        SET data_json = p_data_json,
            data_quality = p_data_quality,
            data_hash = v_data_hash,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = p_record_id;

        SET p_result_code = 0;
        SET p_result_message = '数据更新成功';
        COMMIT;
    END IF;
END$$
DELIMITER ;

-- 批量更新存储过程
DELIMITER $$
CREATE PROCEDURE sp_batch_update_data_quality(
    IN p_device_id INT,
    IN p_start_time DATETIME,
    IN p_end_time DATETIME,
    IN p_old_quality TINYINT,
    IN p_new_quality TINYINT,
    IN p_update_reason VARCHAR(255),
    OUT p_affected_rows INT
)
BEGIN
    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        SET p_affected_rows = -1;
    END;

    START TRANSACTION;

    -- 记录批量变更日志
    INSERT INTO batch_change_log (
        operation_type, table_name, filter_conditions,
        old_value, new_value, change_reason, changed_by
    ) VALUES (
        'BATCH_UPDATE', 'aligned_data',
        JSON_OBJECT('device_id', p_device_id, 'time_range',
                   CONCAT(p_start_time, ' to ', p_end_time)),
        p_old_quality, p_new_quality, p_update_reason, USER()
    );

    -- 执行批量更新
    UPDATE aligned_data
    SET data_quality = p_new_quality,
        updated_at = CURRENT_TIMESTAMP
    WHERE device_id = p_device_id
      AND standard_time BETWEEN p_start_time AND p_end_time
      AND data_quality = p_old_quality;

    SET p_affected_rows = ROW_COUNT();
    COMMIT;
END$$
DELIMITER ;
```

## 数据删除（DELETE）规范

### 软删除机制

```sql
-- 添加软删除字段
ALTER TABLE aligned_data
ADD COLUMN is_deleted BOOLEAN DEFAULT FALSE,
ADD COLUMN deleted_at TIMESTAMP NULL,
ADD COLUMN deleted_by VARCHAR(100) NULL,
ADD COLUMN delete_reason VARCHAR(255) NULL;

-- 软删除存储过程
DELIMITER $$
CREATE PROCEDURE sp_soft_delete_aligned_data(
    IN p_record_id BIGINT,
    IN p_delete_reason VARCHAR(255),
    OUT p_result_code INT,
    OUT p_result_message VARCHAR(255)
)
BEGIN
    DECLARE v_record_exists INT DEFAULT 0;
    DECLARE v_already_deleted BOOLEAN DEFAULT FALSE;
    DECLARE v_backup_data JSON;

    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        GET DIAGNOSTICS CONDITION 1
            p_result_code = MYSQL_ERRNO,
            p_result_message = MESSAGE_TEXT;
        ROLLBACK;
    END;

    START TRANSACTION;

    -- 检查记录状态
    SELECT COUNT(*), is_deleted,
           JSON_OBJECT('station_id', station_id, 'device_id', device_id,
                      'standard_time', standard_time, 'data_json', data_json)
    INTO v_record_exists, v_already_deleted, v_backup_data
    FROM aligned_data
    WHERE id = p_record_id;

    IF v_record_exists = 0 THEN
        SET p_result_code = 5001;
        SET p_result_message = CONCAT('记录不存在: ID=', p_record_id);
        ROLLBACK;
    ELSEIF v_already_deleted THEN
        SET p_result_code = 5002;
        SET p_result_message = '记录已被删除';
        ROLLBACK;
    ELSE
        -- 备份到删除日志
        INSERT INTO data_deletion_log (
            table_name, record_id, deleted_data,
            delete_reason, deleted_by
        ) VALUES (
            'aligned_data', p_record_id, v_backup_data,
            p_delete_reason, USER()
        );

        -- 执行软删除
        UPDATE aligned_data
        SET is_deleted = TRUE,
            deleted_at = CURRENT_TIMESTAMP,
            deleted_by = USER(),
            delete_reason = p_delete_reason
        WHERE id = p_record_id;

        SET p_result_code = 0;
        SET p_result_message = '记录删除成功';
        COMMIT;
    END IF;
END$$
DELIMITER ;

-- 批量软删除存储过程
DELIMITER $$
CREATE PROCEDURE sp_batch_soft_delete(
    IN p_filter_conditions JSON,
    IN p_delete_reason VARCHAR(255),
    IN p_max_records INT DEFAULT 1000,
    OUT p_deleted_count INT
)
BEGIN
    DECLARE v_device_id INT;
    DECLARE v_start_time DATETIME;
    DECLARE v_end_time DATETIME;
    DECLARE v_data_quality TINYINT;

    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        ROLLBACK;
        SET p_deleted_count = -1;
    END;

    START TRANSACTION;

    -- 解析过滤条件
    SET v_device_id = JSON_UNQUOTE(JSON_EXTRACT(p_filter_conditions, '$.device_id'));
    SET v_start_time = JSON_UNQUOTE(JSON_EXTRACT(p_filter_conditions, '$.start_time'));
    SET v_end_time = JSON_UNQUOTE(JSON_EXTRACT(p_filter_conditions, '$.end_time'));
    SET v_data_quality = JSON_UNQUOTE(JSON_EXTRACT(p_filter_conditions, '$.data_quality'));

    -- 记录批量删除日志
    INSERT INTO batch_change_log (
        operation_type, table_name, filter_conditions,
        change_reason, changed_by
    ) VALUES (
        'BATCH_SOFT_DELETE', 'aligned_data', p_filter_conditions,
        p_delete_reason, USER()
    );

    -- 执行批量软删除
    UPDATE aligned_data
    SET is_deleted = TRUE,
        deleted_at = CURRENT_TIMESTAMP,
        deleted_by = USER(),
        delete_reason = p_delete_reason
    WHERE is_deleted = FALSE
      AND (v_device_id IS NULL OR device_id = v_device_id)
      AND (v_start_time IS NULL OR standard_time >= v_start_time)
      AND (v_end_time IS NULL OR standard_time <= v_end_time)
      AND (v_data_quality IS NULL OR data_quality = v_data_quality)
    LIMIT p_max_records;

    SET p_deleted_count = ROW_COUNT();
    COMMIT;
END$$
DELIMITER ;
```

### 硬删除（物理删除）

```sql
-- 硬删除存储过程（谨慎使用）
DELIMITER $$
CREATE PROCEDURE sp_hard_delete_old_data(
    IN p_retention_days INT DEFAULT 365,
    IN p_max_records INT DEFAULT 10000,
    IN p_confirm_delete BOOLEAN DEFAULT FALSE,
    OUT p_result_summary JSON
)
BEGIN
    DECLARE v_cutoff_date DATETIME;
    DECLARE v_candidate_count INT DEFAULT 0;
    DECLARE v_deleted_count INT DEFAULT 0;
    DECLARE v_archived_count INT DEFAULT 0;

    DECLARE EXIT HANDLER FOR SQLEXCEPTION
    BEGIN
        GET DIAGNOSTICS CONDITION 1
            @error_code = MYSQL_ERRNO,
            @error_message = MESSAGE_TEXT;
        ROLLBACK;
        SET p_result_summary = JSON_OBJECT(
            'success', FALSE,
            'error_code', @error_code,
            'error_message', @error_message
        );
    END;

    SET v_cutoff_date = DATE_SUB(NOW(), INTERVAL p_retention_days DAY);

    START TRANSACTION;

    -- 统计待删除记录数
    SELECT COUNT(*) INTO v_candidate_count
    FROM aligned_data
    WHERE standard_time < v_cutoff_date
      AND (is_deleted = TRUE OR data_quality = 4);

    IF NOT p_confirm_delete THEN
        -- 仅返回统计信息，不执行删除
        SET p_result_summary = JSON_OBJECT(
            'action', 'PREVIEW_ONLY',
            'candidate_count', v_candidate_count,
            'cutoff_date', v_cutoff_date,
            'max_records', p_max_records
        );
        ROLLBACK;
    ELSE
        -- 先归档到历史表
        INSERT INTO aligned_data_archive
        SELECT *, NOW() as archived_at
        FROM aligned_data
        WHERE standard_time < v_cutoff_date
          AND (is_deleted = TRUE OR data_quality = 4)
        LIMIT p_max_records;

        SET v_archived_count = ROW_COUNT();

        -- 执行物理删除
        DELETE FROM aligned_data
        WHERE standard_time < v_cutoff_date
          AND (is_deleted = TRUE OR data_quality = 4)
        LIMIT p_max_records;

        SET v_deleted_count = ROW_COUNT();

        -- 记录删除操作
        INSERT INTO maintenance_log (
            operation_type, operation_details, records_affected
        ) VALUES (
            'HARD_DELETE_OLD_DATA',
            JSON_OBJECT('retention_days', p_retention_days, 'cutoff_date', v_cutoff_date),
            v_deleted_count
        );

        SET p_result_summary = JSON_OBJECT(
            'action', 'HARD_DELETE_COMPLETED',
            'archived_count', v_archived_count,
            'deleted_count', v_deleted_count,
            'remaining_candidates', v_candidate_count - v_deleted_count
        );

        COMMIT;
    END IF;
END$$
DELIMITER ;
```

## 数据查询（SELECT）优化

### 安全查询模板

```sql
-- 分页查询存储过程
DELIMITER $$
CREATE PROCEDURE sp_query_aligned_data_paginated(
    IN p_filters JSON,
    IN p_page_size INT DEFAULT 20,
    IN p_page_number INT DEFAULT 1,
    IN p_sort_field VARCHAR(50) DEFAULT 'standard_time',
    IN p_sort_direction ENUM('ASC', 'DESC') DEFAULT 'DESC',
    OUT p_total_count INT
)
BEGIN
    DECLARE v_offset INT;
    DECLARE v_device_id INT DEFAULT NULL;
    DECLARE v_station_id INT DEFAULT NULL;
    DECLARE v_start_time DATETIME DEFAULT NULL;
    DECLARE v_end_time DATETIME DEFAULT NULL;
    DECLARE v_min_quality TINYINT DEFAULT NULL;
    DECLARE v_include_deleted BOOLEAN DEFAULT FALSE;

    -- 解析过滤条件
    IF p_filters IS NOT NULL THEN
        SET v_device_id = JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.device_id'));
        SET v_station_id = JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.station_id'));
        SET v_start_time = JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.start_time'));
        SET v_end_time = JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.end_time'));
        SET v_min_quality = JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.min_quality'));
        SET v_include_deleted = IFNULL(JSON_UNQUOTE(JSON_EXTRACT(p_filters, '$.include_deleted')), FALSE);
    END IF;

    SET v_offset = (p_page_number - 1) * p_page_size;

    -- 构建动态查询（使用预处理语句防止SQL注入）
    SET @sql = CONCAT(
        'SELECT COUNT(*) FROM aligned_data ad ',
        'JOIN devices d ON ad.device_id = d.id ',
        'JOIN stations s ON ad.station_id = s.id ',
        'WHERE 1=1'
    );

    IF v_device_id IS NOT NULL THEN
        SET @sql = CONCAT(@sql, ' AND ad.device_id = ', v_device_id);
    END IF;

    IF v_station_id IS NOT NULL THEN
        SET @sql = CONCAT(@sql, ' AND ad.station_id = ', v_station_id);
    END IF;

    IF v_start_time IS NOT NULL THEN
        SET @sql = CONCAT(@sql, ' AND ad.standard_time >= ''', v_start_time, '''');
    END IF;

    IF v_end_time IS NOT NULL THEN
        SET @sql = CONCAT(@sql, ' AND ad.standard_time <= ''', v_end_time, '''');
    END IF;

    IF v_min_quality IS NOT NULL THEN
        SET @sql = CONCAT(@sql, ' AND ad.data_quality >= ', v_min_quality);
    END IF;

    IF NOT v_include_deleted THEN
        SET @sql = CONCAT(@sql, ' AND ad.is_deleted = FALSE');
    END IF;

    -- 执行计数查询
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;

    -- 构建数据查询
    SET @sql = CONCAT(
        'SELECT ad.id, ad.station_id, s.name as station_name, ',
        'ad.device_id, d.name as device_name, d.device_code, ',
        'ad.standard_time, ad.data_json, ad.data_quality, ',
        'ad.created_at, ad.updated_at ',
        'FROM aligned_data ad ',
        'JOIN devices d ON ad.device_id = d.id ',
        'JOIN stations s ON ad.station_id = s.id ',
        'WHERE 1=1'
    );

    -- 添加相同的过滤条件...
    -- (省略重复代码)

    SET @sql = CONCAT(@sql, ' ORDER BY ad.', p_sort_field, ' ', p_sort_direction);
    SET @sql = CONCAT(@sql, ' LIMIT ', p_page_size, ' OFFSET ', v_offset);

    -- 执行数据查询
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
END$$
DELIMITER ;
```

## 审计日志系统

### 变更日志表

```sql
-- 数据变更日志表
CREATE TABLE data_change_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(100) NOT NULL,
    record_id BIGINT NOT NULL,
    operation_type ENUM('INSERT', 'UPDATE', 'DELETE') NOT NULL,
    old_values JSON,
    new_values JSON,
    change_reason VARCHAR(255),
    changed_by VARCHAR(100) NOT NULL,
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address VARCHAR(45),
    user_agent VARCHAR(255),
    INDEX idx_table_record (table_name, record_id),
    INDEX idx_changed_by_time (changed_by, changed_at),
    INDEX idx_operation_time (operation_type, changed_at)
);

-- 批量变更日志表
CREATE TABLE batch_change_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    operation_type VARCHAR(50) NOT NULL,
    table_name VARCHAR(100) NOT NULL,
    filter_conditions JSON,
    old_value VARCHAR(255),
    new_value VARCHAR(255),
    affected_records INT DEFAULT 0,
    change_reason VARCHAR(255),
    changed_by VARCHAR(100) NOT NULL,
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_operation_time (operation_type, changed_at),
    INDEX idx_table_time (table_name, changed_at)
);

-- 删除日志表
CREATE TABLE data_deletion_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(100) NOT NULL,
    record_id BIGINT NOT NULL,
    deleted_data JSON NOT NULL,
    delete_reason VARCHAR(255),
    deleted_by VARCHAR(100) NOT NULL,
    deleted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_table_record (table_name, record_id),
    INDEX idx_deleted_by_time (deleted_by, deleted_at)
);

-- 维护操作日志表
CREATE TABLE maintenance_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    operation_type VARCHAR(50) NOT NULL,
    operation_details JSON,
    records_affected INT DEFAULT 0,
    execution_time_ms INT,
    status ENUM('SUCCESS', 'FAILED', 'PARTIAL') DEFAULT 'SUCCESS',
    error_message TEXT,
    executed_by VARCHAR(100) NOT NULL,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_operation_time (operation_type, executed_at),
    INDEX idx_status_time (status, executed_at)
);
```

### 审计触发器

```sql
-- 数据变更审计触发器
DELIMITER $$
CREATE TRIGGER tr_aligned_data_audit_insert
    AFTER INSERT ON aligned_data
    FOR EACH ROW
BEGIN
    INSERT INTO data_change_log (
        table_name, record_id, operation_type, new_values, changed_by
    ) VALUES (
        'aligned_data', NEW.id, 'INSERT',
        JSON_OBJECT(
            'station_id', NEW.station_id,
            'device_id', NEW.device_id,
            'standard_time', NEW.standard_time,
            'data_json', NEW.data_json,
            'data_quality', NEW.data_quality
        ),
        USER()
    );
END$$

CREATE TRIGGER tr_aligned_data_audit_update
    AFTER UPDATE ON aligned_data
    FOR EACH ROW
BEGIN
    INSERT INTO data_change_log (
        table_name, record_id, operation_type, old_values, new_values, changed_by
    ) VALUES (
        'aligned_data', NEW.id, 'UPDATE',
        JSON_OBJECT(
            'data_json', OLD.data_json,
            'data_quality', OLD.data_quality,
            'is_deleted', OLD.is_deleted
        ),
        JSON_OBJECT(
            'data_json', NEW.data_json,
            'data_quality', NEW.data_quality,
            'is_deleted', NEW.is_deleted
        ),
        USER()
    );
END$$

CREATE TRIGGER tr_aligned_data_audit_delete
    BEFORE DELETE ON aligned_data
    FOR EACH ROW
BEGIN
    INSERT INTO data_change_log (
        table_name, record_id, operation_type, old_values, changed_by
    ) VALUES (
        'aligned_data', OLD.id, 'DELETE',
        JSON_OBJECT(
            'station_id', OLD.station_id,
            'device_id', OLD.device_id,
            'standard_time', OLD.standard_time,
            'data_json', OLD.data_json,
            'data_quality', OLD.data_quality
        ),
        USER()
    );
END$$
DELIMITER ;
```

## 数据验证和约束

### 数据有效性检查

```sql
-- 数据验证函数
DELIMITER $$
CREATE FUNCTION fn_validate_data_json(p_device_type VARCHAR(20), p_data_json JSON)
RETURNS JSON
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE v_result JSON DEFAULT JSON_OBJECT('valid', TRUE, 'errors', JSON_ARRAY());
    DECLARE v_required_fields JSON;
    DECLARE v_field_count INT DEFAULT 0;
    DECLARE v_errors JSON DEFAULT JSON_ARRAY();

    -- 根据设备类型获取必需字段
    CASE p_device_type
        WHEN 'pump' THEN
            SET v_required_fields = JSON_ARRAY('frequency', 'power', 'current_a');
        WHEN 'sensor' THEN
            SET v_required_fields = JSON_ARRAY('pressure', 'flow_rate');
        ELSE
            SET v_required_fields = JSON_ARRAY();
    END CASE;

    -- 检查必需字段
    SET v_field_count = 0;
    WHILE v_field_count < JSON_LENGTH(v_required_fields) DO
        SET @field_name = JSON_UNQUOTE(JSON_EXTRACT(v_required_fields, CONCAT('$[', v_field_count, ']')));

        IF JSON_EXTRACT(p_data_json, CONCAT('$.', @field_name)) IS NULL THEN
            SET v_errors = JSON_ARRAY_APPEND(v_errors, '$',
                CONCAT('Missing required field: ', @field_name));
        END IF;

        SET v_field_count = v_field_count + 1;
    END WHILE;

    -- 检查数值范围
    IF JSON_EXTRACT(p_data_json, '$.frequency') IS NOT NULL THEN
        SET @freq_value = CAST(JSON_UNQUOTE(JSON_EXTRACT(p_data_json, '$.frequency')) AS DECIMAL(10,3));
        IF @freq_value < 0 OR @freq_value > 100 THEN
            SET v_errors = JSON_ARRAY_APPEND(v_errors, '$', 'Frequency out of range (0-100)');
        END IF;
    END IF;

    IF JSON_EXTRACT(p_data_json, '$.power') IS NOT NULL THEN
        SET @power_value = CAST(JSON_UNQUOTE(JSON_EXTRACT(p_data_json, '$.power')) AS DECIMAL(15,3));
        IF @power_value < 0 THEN
            SET v_errors = JSON_ARRAY_APPEND(v_errors, '$', 'Power cannot be negative');
        END IF;
    END IF;

    -- 设置结果
    IF JSON_LENGTH(v_errors) > 0 THEN
        SET v_result = JSON_SET(v_result, '$.valid', FALSE);
        SET v_result = JSON_SET(v_result, '$.errors', v_errors);
    END IF;

    RETURN v_result;
END$$
DELIMITER ;
```

## 性能监控和优化

### 操作性能监控

```sql
-- 操作性能监控表
CREATE TABLE operation_performance_log (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    operation_type VARCHAR(50) NOT NULL,
    table_name VARCHAR(100) NOT NULL,
    records_count INT NOT NULL,
    execution_time_ms INT NOT NULL,
    cpu_usage_pct DECIMAL(5,2),
    memory_usage_mb INT,
    io_wait_ms INT,
    executed_by VARCHAR(100) NOT NULL,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_operation_time (operation_type, executed_at),
    INDEX idx_performance (execution_time_ms, records_count)
);

-- 性能监控存储过程
DELIMITER $$
CREATE PROCEDURE sp_monitor_operation_performance(
    IN p_operation_type VARCHAR(50),
    IN p_table_name VARCHAR(100),
    IN p_records_count INT,
    IN p_start_time TIMESTAMP,
    IN p_end_time TIMESTAMP
)
BEGIN
    DECLARE v_execution_time_ms INT;

    SET v_execution_time_ms = TIMESTAMPDIFF(MICROSECOND, p_start_time, p_end_time) / 1000;

    INSERT INTO operation_performance_log (
        operation_type, table_name, records_count,
        execution_time_ms, executed_by
    ) VALUES (
        p_operation_type, p_table_name, p_records_count,
        v_execution_time_ms, USER()
    );

    -- 性能告警检查
    IF v_execution_time_ms > 5000 AND p_records_count < 1000 THEN
        INSERT INTO performance_alerts (
            alert_type, message, severity, details
        ) VALUES (
            'SLOW_OPERATION',
            CONCAT('Slow ', p_operation_type, ' operation detected'),
            'HIGH',
            JSON_OBJECT(
                'operation_type', p_operation_type,
                'table_name', p_table_name,
                'execution_time_ms', v_execution_time_ms,
                'records_count', p_records_count
            )
        );
    END IF;
END$$
DELIMITER ;
```

## 使用指南和最佳实践

### 操作清单

- [ ] 部署所有数据操作存储过程
- [ ] 创建审计日志表和触发器
- [ ] 设置数据验证函数
- [ ] 配置性能监控
- [ ] 测试批量操作性能
- [ ] 验证软删除机制
- [ ] 设置定期维护任务
- [ ] 配置操作权限
- [ ] 创建操作手册
- [ ] 建立故障恢复流程

### 安全建议

1. **参数验证**：所有输入参数必须验证
1. **事务管理**：使用事务确保数据一致性
1. **审计记录**：记录所有数据变更操作
1. **权限控制**：按角色分配操作权限
1. **软删除优先**：优先使用软删除机制
1. **批量限制**：限制批量操作的记录数量
1. **性能监控**：监控操作性能和资源使用
1. **备份恢复**：定期备份和恢复测试
