______________________________________________________________________

## description: 数据库高级功能规范（存储过程、触发器、视图、时间对齐后处理、数据一致性保证）

# 数据库高级功能规范

## 时间对齐后处理机制

### 对齐质量检查触发器

```sql
-- 创建对齐质量检查触发器
DELIMITER $$
CREATE TRIGGER tr_aligned_data_quality_check
    AFTER INSERT ON aligned_data
    FOR EACH ROW
BEGIN
    DECLARE field_count INT DEFAULT 0;
    DECLARE expected_fields INT DEFAULT 0;
    DECLARE quality_score DECIMAL(3,2) DEFAULT 1.0;

    -- 统计JSON字段数量
    SET field_count = JSON_LENGTH(NEW.data_json);

    -- 根据设备类型获取期望字段数
    SELECT expected_field_count INTO expected_fields
    FROM device_field_expectations
    WHERE device_id = NEW.device_id;

    -- 计算质量分数
    IF expected_fields > 0 THEN
        SET quality_score = LEAST(1.0, field_count / expected_fields);
    END IF;

    -- 更新质量分数
    UPDATE aligned_data
    SET data_quality = CASE
        WHEN quality_score >= 0.9 THEN 1  -- 优质
        WHEN quality_score >= 0.7 THEN 2  -- 良好
        WHEN quality_score >= 0.5 THEN 3  -- 一般
        ELSE 4  -- 较差
    END
    WHERE id = NEW.id;

    -- 记录质量异常
    IF quality_score < 0.5 THEN
        INSERT INTO data_quality_alerts (
            device_id, standard_time, quality_score,
            field_count, expected_count, alert_type
        ) VALUES (
            NEW.device_id, NEW.standard_time, quality_score,
            field_count, expected_fields, 'LOW_QUALITY'
        );
    END IF;
END$$
DELIMITER ;
```

### 时间窗口完整性检查

```sql
-- 创建时间窗口完整性检查存储过程
DELIMITER $$
CREATE PROCEDURE sp_check_time_window_completeness(
    IN p_station_id INT,
    IN p_start_time DATETIME,
    IN p_end_time DATETIME,
    OUT p_completeness_ratio DECIMAL(5,4),
    OUT p_missing_intervals JSON
)
BEGIN
    DECLARE v_expected_points INT DEFAULT 0;
    DECLARE v_actual_points INT DEFAULT 0;
    DECLARE v_missing_intervals JSON DEFAULT JSON_ARRAY();

    -- 计算期望的数据点数量（按秒计算）
    SET v_expected_points = TIMESTAMPDIFF(SECOND, p_start_time, p_end_time) + 1;

    -- 计算实际数据点数量
    SELECT COUNT(DISTINCT standard_time) INTO v_actual_points
    FROM aligned_data
    WHERE station_id = p_station_id
      AND standard_time BETWEEN p_start_time AND p_end_time;

    -- 计算完整性比率
    SET p_completeness_ratio = v_actual_points / v_expected_points;

    -- 查找缺失的时间间隔
    WITH RECURSIVE time_series AS (
        SELECT p_start_time as time_point
        UNION ALL
        SELECT DATE_ADD(time_point, INTERVAL 1 SECOND)
        FROM time_series
        WHERE time_point < p_end_time
    ),
    missing_times AS (
        SELECT ts.time_point
        FROM time_series ts
        LEFT JOIN aligned_data ad ON ad.standard_time = ts.time_point
                                   AND ad.station_id = p_station_id
        WHERE ad.standard_time IS NULL
    )
    SELECT JSON_ARRAYAGG(time_point) INTO p_missing_intervals
    FROM missing_times;

END$$
DELIMITER ;
```

## 数据对齐存储过程

### 设备间时间同步

```sql
-- 设备间时间同步存储过程
DELIMITER $$
CREATE PROCEDURE sp_sync_device_timestamps(
    IN p_station_id INT,
    IN p_time_window_start DATETIME,
    IN p_time_window_end DATETIME,
    IN p_tolerance_seconds INT DEFAULT 5
)
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE v_standard_time DATETIME;
    DECLARE v_device_count INT;

    -- 游标声明
    DECLARE time_cursor CURSOR FOR
        SELECT DISTINCT standard_time
        FROM aligned_data
        WHERE station_id = p_station_id
          AND standard_time BETWEEN p_time_window_start AND p_time_window_end
        ORDER BY standard_time;

    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;

    -- 创建临时表存储同步结果
    CREATE TEMPORARY TABLE temp_sync_results (
        standard_time DATETIME,
        device_count INT,
        sync_quality ENUM('PERFECT', 'GOOD', 'POOR', 'MISSING'),
        aligned_devices JSON
    );

    OPEN time_cursor;

    read_loop: LOOP
        FETCH time_cursor INTO v_standard_time;
        IF done THEN
            LEAVE read_loop;
        END IF;

        -- 统计该时间点的设备数量
        SELECT COUNT(DISTINCT device_id),
               JSON_ARRAYAGG(device_id)
        INTO v_device_count, @aligned_devices
        FROM aligned_data
        WHERE station_id = p_station_id
          AND standard_time = v_standard_time;

        -- 评估同步质量
        INSERT INTO temp_sync_results VALUES (
            v_standard_time,
            v_device_count,
            CASE
                WHEN v_device_count >= (SELECT COUNT(*) FROM devices WHERE station_id = p_station_id) THEN 'PERFECT'
                WHEN v_device_count >= (SELECT COUNT(*) FROM devices WHERE station_id = p_station_id) * 0.8 THEN 'GOOD'
                WHEN v_device_count >= (SELECT COUNT(*) FROM devices WHERE station_id = p_station_id) * 0.5 THEN 'POOR'
                ELSE 'MISSING'
            END,
            @aligned_devices
        );

    END LOOP;

    CLOSE time_cursor;

    -- 返回同步结果
    SELECT * FROM temp_sync_results ORDER BY standard_time;

    DROP TEMPORARY TABLE temp_sync_results;
END$$
DELIMITER ;
```

### 数据插值和补齐

```sql
-- 数据插值存储过程
DELIMITER $$
CREATE PROCEDURE sp_interpolate_missing_data(
    IN p_device_id INT,
    IN p_field_key VARCHAR(100),
    IN p_start_time DATETIME,
    IN p_end_time DATETIME,
    IN p_interpolation_method ENUM('LINEAR', 'CUBIC', 'FORWARD_FILL', 'BACKWARD_FILL') DEFAULT 'LINEAR'
)
BEGIN
    DECLARE v_prev_time DATETIME DEFAULT NULL;
    DECLARE v_prev_value DECIMAL(20,6) DEFAULT NULL;
    DECLARE v_next_time DATETIME DEFAULT NULL;
    DECLARE v_next_value DECIMAL(20,6) DEFAULT NULL;
    DECLARE v_interpolated_value DECIMAL(20,6);
    DECLARE v_time_diff_total BIGINT;
    DECLARE v_time_diff_current BIGINT;

    -- 查找缺失数据的时间点
    CREATE TEMPORARY TABLE temp_missing_times AS
    WITH RECURSIVE time_series AS (
        SELECT p_start_time as time_point
        UNION ALL
        SELECT DATE_ADD(time_point, INTERVAL 1 SECOND)
        FROM time_series
        WHERE time_point < p_end_time
    )
    SELECT ts.time_point
    FROM time_series ts
    LEFT JOIN aligned_data ad ON ad.standard_time = ts.time_point
                               AND ad.device_id = p_device_id
                               AND JSON_EXTRACT(ad.data_json, CONCAT('$.', p_field_key)) IS NOT NULL
    WHERE ad.standard_time IS NULL;

    -- 处理每个缺失时间点
    block: BEGIN
        DECLARE done INT DEFAULT FALSE;
        DECLARE v_missing_time DATETIME;

        DECLARE missing_cursor CURSOR FOR
            SELECT time_point FROM temp_missing_times ORDER BY time_point;

        DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;

        OPEN missing_cursor;

        interpolate_loop: LOOP
            FETCH missing_cursor INTO v_missing_time;
            IF done THEN
                LEAVE interpolate_loop;
            END IF;

            -- 查找前一个有效值
            SELECT standard_time,
                   CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, CONCAT('$.', p_field_key))) AS DECIMAL(20,6))
            INTO v_prev_time, v_prev_value
            FROM aligned_data
            WHERE device_id = p_device_id
              AND standard_time < v_missing_time
              AND JSON_EXTRACT(data_json, CONCAT('$.', p_field_key)) IS NOT NULL
            ORDER BY standard_time DESC
            LIMIT 1;

            -- 查找后一个有效值
            SELECT standard_time,
                   CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, CONCAT('$.', p_field_key))) AS DECIMAL(20,6))
            INTO v_next_time, v_next_value
            FROM aligned_data
            WHERE device_id = p_device_id
              AND standard_time > v_missing_time
              AND JSON_EXTRACT(data_json, CONCAT('$.', p_field_key)) IS NOT NULL
            ORDER BY standard_time ASC
            LIMIT 1;

            -- 执行插值
            IF v_prev_time IS NOT NULL AND v_next_time IS NOT NULL THEN
                CASE p_interpolation_method
                    WHEN 'LINEAR' THEN
                        SET v_time_diff_total = TIMESTAMPDIFF(SECOND, v_prev_time, v_next_time);
                        SET v_time_diff_current = TIMESTAMPDIFF(SECOND, v_prev_time, v_missing_time);
                        SET v_interpolated_value = v_prev_value +
                            (v_next_value - v_prev_value) * (v_time_diff_current / v_time_diff_total);

                    WHEN 'FORWARD_FILL' THEN
                        SET v_interpolated_value = v_prev_value;

                    WHEN 'BACKWARD_FILL' THEN
                        SET v_interpolated_value = v_next_value;

                    ELSE -- 默认线性插值
                        SET v_time_diff_total = TIMESTAMPDIFF(SECOND, v_prev_time, v_next_time);
                        SET v_time_diff_current = TIMESTAMPDIFF(SECOND, v_prev_time, v_missing_time);
                        SET v_interpolated_value = v_prev_value +
                            (v_next_value - v_prev_value) * (v_time_diff_current / v_time_diff_total);
                END CASE;

                -- 插入插值数据
                INSERT INTO aligned_data (station_id, device_id, standard_time, data_json, data_quality, data_hash)
                SELECT station_id, p_device_id, v_missing_time,
                       JSON_SET('{}', CONCAT('$.', p_field_key), v_interpolated_value),
                       2, -- 标记为插值数据
                       MD5(CONCAT(p_device_id, '_', v_missing_time, '_interpolated'))
                FROM devices WHERE id = p_device_id
                ON DUPLICATE KEY UPDATE
                    data_json = JSON_SET(data_json, CONCAT('$.', p_field_key), v_interpolated_value),
                    data_quality = LEAST(data_quality, 2), -- 保持较低的质量等级
                    updated_at = CURRENT_TIMESTAMP;
            END IF;

        END LOOP;

        CLOSE missing_cursor;
    END block;

    DROP TEMPORARY TABLE temp_missing_times;
END$$
DELIMITER ;
```

## 数据完整性视图

### 实时数据质量视图

```sql
-- 创建实时数据质量监控视图
CREATE VIEW v_realtime_data_quality AS
SELECT
    s.name as station_name,
    d.name as device_name,
    d.device_type,
    ad.standard_time,
    ad.data_quality,
    JSON_LENGTH(ad.data_json) as field_count,
    CASE
        WHEN ad.data_quality = 1 THEN '优质'
        WHEN ad.data_quality = 2 THEN '良好'
        WHEN ad.data_quality = 3 THEN '一般'
        WHEN ad.data_quality = 4 THEN '较差'
        ELSE '未知'
    END as quality_desc,
    -- 计算数据完整性百分比
    ROUND(
        JSON_LENGTH(ad.data_json) /
        COALESCE(dfe.expected_field_count, 1) * 100, 2
    ) as completeness_pct,
    ad.created_at,
    ad.updated_at
FROM aligned_data ad
JOIN devices d ON ad.device_id = d.id
JOIN stations s ON ad.station_id = s.id
LEFT JOIN device_field_expectations dfe ON d.id = dfe.device_id
WHERE ad.standard_time >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
ORDER BY ad.standard_time DESC, s.name, d.name;
```

### 时间对齐统计视图

```sql
-- 创建时间对齐统计视图
CREATE VIEW v_time_alignment_stats AS
SELECT
    s.id as station_id,
    s.name as station_name,
    DATE(ad.standard_time) as data_date,
    HOUR(ad.standard_time) as data_hour,
    COUNT(DISTINCT ad.device_id) as active_devices,
    COUNT(DISTINCT CONCAT(ad.device_id, '_', ad.standard_time)) as total_records,
    COUNT(DISTINCT ad.standard_time) as unique_timestamps,
    -- 计算设备同步率
    ROUND(
        COUNT(DISTINCT CONCAT(ad.device_id, '_', ad.standard_time)) /
        (COUNT(DISTINCT ad.standard_time) * COUNT(DISTINCT ad.device_id)) * 100, 2
    ) as sync_rate_pct,
    -- 平均数据质量
    ROUND(AVG(ad.data_quality), 2) as avg_quality,
    -- 数据延迟统计
    ROUND(AVG(TIMESTAMPDIFF(SECOND, ad.standard_time, ad.created_at)), 2) as avg_delay_seconds,
    MIN(ad.standard_time) as first_timestamp,
    MAX(ad.standard_time) as last_timestamp
FROM aligned_data ad
JOIN stations s ON ad.station_id = s.id
WHERE ad.standard_time >= DATE_SUB(NOW(), INTERVAL 7 DAY)
GROUP BY s.id, s.name, DATE(ad.standard_time), HOUR(ad.standard_time)
ORDER BY data_date DESC, data_hour DESC, station_name;
```

### 缺失数据分析视图

```sql
-- 创建缺失数据分析视图
CREATE VIEW v_missing_data_analysis AS
SELECT
    s.name as station_name,
    d.name as device_name,
    d.device_type,
    DATE(expected_times.time_point) as missing_date,
    HOUR(expected_times.time_point) as missing_hour,
    COUNT(*) as missing_count,
    GROUP_CONCAT(
        DATE_FORMAT(expected_times.time_point, '%H:%i:%s')
        ORDER BY expected_times.time_point
        SEPARATOR ', '
    ) as missing_times_sample,
    -- 计算缺失率
    ROUND(COUNT(*) / 3600 * 100, 2) as missing_rate_pct_hourly
FROM (
    -- 生成期望的时间序列（最近24小时，每秒一个点）
    WITH RECURSIVE time_series AS (
        SELECT DATE_SUB(DATE_SUB(NOW(), INTERVAL 24 HOUR), INTERVAL SECOND(NOW()) SECOND) as time_point
        UNION ALL
        SELECT DATE_ADD(time_point, INTERVAL 1 SECOND)
        FROM time_series
        WHERE time_point < DATE_SUB(NOW(), INTERVAL 1 SECOND)
    )
    SELECT time_point FROM time_series
) expected_times
CROSS JOIN devices d
JOIN stations s ON d.station_id = s.id
LEFT JOIN aligned_data ad ON ad.device_id = d.id
                           AND ad.standard_time = expected_times.time_point
WHERE ad.standard_time IS NULL
  AND d.is_active = 1
GROUP BY s.name, d.name, d.device_type, DATE(expected_times.time_point), HOUR(expected_times.time_point)
HAVING missing_count > 10  -- 只显示缺失超过10个数据点的情况
ORDER BY missing_date DESC, missing_hour DESC, missing_count DESC;
```

## 性能优化触发器

### JSON字段索引维护

```sql
-- 为常用JSON字段创建虚拟列和索引
ALTER TABLE aligned_data
ADD COLUMN frequency_value DECIMAL(10,3)
AS (CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.frequency')) AS DECIMAL(10,3))) VIRTUAL,
ADD COLUMN power_value DECIMAL(15,3)
AS (CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.power')) AS DECIMAL(15,3))) VIRTUAL,
ADD COLUMN pressure_value DECIMAL(10,3)
AS (CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.pressure')) AS DECIMAL(10,3))) VIRTUAL;

-- 创建虚拟列索引
CREATE INDEX idx_frequency ON aligned_data (device_id, frequency_value, standard_time);
CREATE INDEX idx_power ON aligned_data (device_id, power_value, standard_time);
CREATE INDEX idx_pressure ON aligned_data (device_id, pressure_value, standard_time);
```

### 数据归档触发器

```sql
-- 创建数据归档触发器
DELIMITER $$
CREATE TRIGGER tr_data_archive_check
    AFTER INSERT ON aligned_data
    FOR EACH ROW
BEGIN
    DECLARE archive_threshold DATETIME;

    -- 设置归档阈值（例如30天前的数据）
    SET archive_threshold = DATE_SUB(NOW(), INTERVAL 30 DAY);

    -- 检查是否需要归档
    IF NEW.standard_time < archive_threshold THEN
        INSERT INTO aligned_data_archive
        SELECT * FROM aligned_data
        WHERE standard_time < archive_threshold
        ON DUPLICATE KEY UPDATE updated_at = VALUES(updated_at);

        -- 标记为待删除
        UPDATE aligned_data
        SET data_quality = data_quality | 128  -- 设置归档标志位
        WHERE standard_time < archive_threshold;
    END IF;
END$$
DELIMITER ;
```

## 数据一致性函数

### 跨设备数据校验

```sql
-- 创建跨设备数据一致性校验函数
DELIMITER $$
CREATE FUNCTION fn_check_cross_device_consistency(
    p_station_id INT,
    p_timestamp DATETIME,
    p_tolerance_pct DECIMAL(5,2) DEFAULT 5.0
) RETURNS JSON
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE v_result JSON DEFAULT JSON_OBJECT();
    DECLARE v_inconsistencies JSON DEFAULT JSON_ARRAY();

    -- 检查同一时间点不同设备的相似字段是否一致
    -- 例如：同一泵站的不同泵的压力值应该相近

    -- 压力一致性检查
    SET @pressure_check = (
        SELECT JSON_ARRAYAGG(
            JSON_OBJECT(
                'device_id', device_id,
                'pressure', CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.pressure')) AS DECIMAL(10,3)),
                'deviation_pct', ABS(
                    CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.pressure')) AS DECIMAL(10,3)) - @avg_pressure
                ) / @avg_pressure * 100
            )
        )
        FROM aligned_data
        CROSS JOIN (
            SELECT AVG(CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.pressure')) AS DECIMAL(10,3))) as avg_pressure
            FROM aligned_data
            WHERE station_id = p_station_id
              AND standard_time = p_timestamp
              AND JSON_EXTRACT(data_json, '$.pressure') IS NOT NULL
        ) avg_calc
        WHERE station_id = p_station_id
          AND standard_time = p_timestamp
          AND JSON_EXTRACT(data_json, '$.pressure') IS NOT NULL
          AND ABS(
              CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, '$.pressure')) AS DECIMAL(10,3)) - avg_calc.avg_pressure
          ) / avg_calc.avg_pressure * 100 > p_tolerance_pct
    );

    SET v_result = JSON_SET(v_result, '$.pressure_inconsistencies', IFNULL(@pressure_check, JSON_ARRAY()));

    RETURN v_result;
END$$
DELIMITER ;
```

## 监控和告警存储过程

### 实时异常检测

```sql
-- 创建实时异常检测存储过程
DELIMITER $$
CREATE PROCEDURE sp_realtime_anomaly_detection(
    IN p_lookback_minutes INT DEFAULT 60,
    IN p_threshold_multiplier DECIMAL(3,1) DEFAULT 3.0
)
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE v_device_id INT;
    DECLARE v_field_key VARCHAR(100);
    DECLARE v_current_value DECIMAL(20,6);
    DECLARE v_avg_value DECIMAL(20,6);
    DECLARE v_std_value DECIMAL(20,6);
    DECLARE v_threshold_upper DECIMAL(20,6);
    DECLARE v_threshold_lower DECIMAL(20,6);

    -- 游标：获取最近的数据进行异常检测
    DECLARE anomaly_cursor CURSOR FOR
        SELECT
            ad.device_id,
            jt.field_key,
            CAST(jt.field_value AS DECIMAL(20,6)) as current_value
        FROM aligned_data ad
        JOIN JSON_TABLE(
            ad.data_json,
            '$.*' COLUMNS (
                field_key VARCHAR(100) PATH '$',
                field_value VARCHAR(255) PATH '$'
            )
        ) jt
        WHERE ad.standard_time >= DATE_SUB(NOW(), INTERVAL 5 MINUTE)
          AND jt.field_value REGEXP '^[0-9]+\.?[0-9]*$';

    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;

    -- 创建临时表存储异常结果
    CREATE TEMPORARY TABLE temp_anomalies (
        device_id INT,
        field_key VARCHAR(100),
        current_value DECIMAL(20,6),
        expected_range_min DECIMAL(20,6),
        expected_range_max DECIMAL(20,6),
        anomaly_score DECIMAL(5,2),
        anomaly_type ENUM('HIGH', 'LOW', 'SPIKE', 'DROP')
    );

    OPEN anomaly_cursor;

    detection_loop: LOOP
        FETCH anomaly_cursor INTO v_device_id, v_field_key, v_current_value;
        IF done THEN
            LEAVE detection_loop;
        END IF;

        -- 计算历史统计值
        SELECT
            AVG(CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, CONCAT('$.', v_field_key))) AS DECIMAL(20,6))),
            STDDEV(CAST(JSON_UNQUOTE(JSON_EXTRACT(data_json, CONCAT('$.', v_field_key))) AS DECIMAL(20,6)))
        INTO v_avg_value, v_std_value
        FROM aligned_data
        WHERE device_id = v_device_id
          AND standard_time BETWEEN DATE_SUB(NOW(), INTERVAL p_lookback_minutes MINUTE)
                                AND DATE_SUB(NOW(), INTERVAL 5 MINUTE)
          AND JSON_EXTRACT(data_json, CONCAT('$.', v_field_key)) IS NOT NULL;

        -- 计算阈值
        SET v_threshold_upper = v_avg_value + (p_threshold_multiplier * IFNULL(v_std_value, 0));
        SET v_threshold_lower = v_avg_value - (p_threshold_multiplier * IFNULL(v_std_value, 0));

        -- 检测异常
        IF v_current_value > v_threshold_upper OR v_current_value < v_threshold_lower THEN
            INSERT INTO temp_anomalies VALUES (
                v_device_id,
                v_field_key,
                v_current_value,
                v_threshold_lower,
                v_threshold_upper,
                ABS(v_current_value - v_avg_value) / GREATEST(IFNULL(v_std_value, 1), 0.001),
                CASE
                    WHEN v_current_value > v_threshold_upper THEN 'HIGH'
                    ELSE 'LOW'
                END
            );
        END IF;

    END LOOP;

    CLOSE anomaly_cursor;

    -- 返回异常结果并插入告警表
    INSERT INTO data_anomaly_alerts (
        device_id, field_key, current_value, expected_min, expected_max,
        anomaly_score, anomaly_type, detected_at
    )
    SELECT
        device_id, field_key, current_value, expected_range_min, expected_range_max,
        anomaly_score, anomaly_type, NOW()
    FROM temp_anomalies;

    -- 返回结果
    SELECT
        d.name as device_name,
        ta.*
    FROM temp_anomalies ta
    JOIN devices d ON ta.device_id = d.id
    ORDER BY ta.anomaly_score DESC;

    DROP TEMPORARY TABLE temp_anomalies;
END$$
DELIMITER ;
```

## 配置表和初始化

### 创建支持表

```sql
-- 设备字段期望表
CREATE TABLE device_field_expectations (
    id INT PRIMARY KEY AUTO_INCREMENT,
    device_id INT NOT NULL,
    expected_field_count INT NOT NULL,
    required_fields JSON,
    optional_fields JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    FOREIGN KEY (device_id) REFERENCES devices(id),
    UNIQUE KEY uk_device_expectations (device_id)
);

-- 数据质量告警表
CREATE TABLE data_quality_alerts (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    device_id INT NOT NULL,
    standard_time DATETIME NOT NULL,
    quality_score DECIMAL(3,2),
    field_count INT,
    expected_count INT,
    alert_type ENUM('LOW_QUALITY', 'MISSING_FIELDS', 'INVALID_DATA'),
    resolved_at DATETIME NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_device_time (device_id, standard_time),
    INDEX idx_alert_type_time (alert_type, created_at),
    FOREIGN KEY (device_id) REFERENCES devices(id)
);

-- 数据异常告警表
CREATE TABLE data_anomaly_alerts (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    device_id INT NOT NULL,
    field_key VARCHAR(100) NOT NULL,
    current_value DECIMAL(20,6),
    expected_min DECIMAL(20,6),
    expected_max DECIMAL(20,6),
    anomaly_score DECIMAL(5,2),
    anomaly_type ENUM('HIGH', 'LOW', 'SPIKE', 'DROP'),
    resolved_at DATETIME NULL,
    detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_device_field_time (device_id, field_key, detected_at),
    INDEX idx_anomaly_type_time (anomaly_type, detected_at),
    FOREIGN KEY (device_id) REFERENCES devices(id)
);

-- 数据归档表（与aligned_data结构相同）
CREATE TABLE aligned_data_archive LIKE aligned_data;
ALTER TABLE aligned_data_archive
ADD COLUMN archived_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;
```

## 性能监控和维护

### 定期维护作业

```sql
-- 创建定期清理作业
DELIMITER $$
CREATE EVENT ev_cleanup_old_alerts
ON SCHEDULE EVERY 1 DAY
STARTS CURRENT_TIMESTAMP
DO BEGIN
    -- 清理30天前的已解决告警
    DELETE FROM data_quality_alerts
    WHERE resolved_at IS NOT NULL
      AND resolved_at < DATE_SUB(NOW(), INTERVAL 30 DAY);

    DELETE FROM data_anomaly_alerts
    WHERE resolved_at IS NOT NULL
      AND resolved_at < DATE_SUB(NOW(), INTERVAL 30 DAY);

    -- 清理90天前的未解决告警（避免表过大）
    DELETE FROM data_quality_alerts
    WHERE created_at < DATE_SUB(NOW(), INTERVAL 90 DAY);

    DELETE FROM data_anomaly_alerts
    WHERE detected_at < DATE_SUB(NOW(), INTERVAL 90 DAY);
END$$
DELIMITER ;

-- 创建统计信息更新作业
DELIMITER $$
CREATE EVENT ev_update_table_stats
ON SCHEDULE EVERY 6 HOUR
STARTS CURRENT_TIMESTAMP
DO BEGIN
    ANALYZE TABLE aligned_data;
    ANALYZE TABLE data_quality_alerts;
    ANALYZE TABLE data_anomaly_alerts;
END$$
DELIMITER ;
```

## 使用指南

### 部署检查清单

- [ ] 创建所有支持表（device_field_expectations, data_quality_alerts, data_anomaly_alerts）
- [ ] 部署所有存储过程和函数
- [ ] 创建触发器（谨慎使用，可能影响插入性能）
- [ ] 创建视图并验证查询性能
- [ ] 配置定期维护作业
- [ ] 测试异常检测和告警机制
- [ ] 验证数据归档流程
- [ ] 设置监控指标和报表
- [ ] 配置权限和安全策略
- [ ] 编写操作文档和故障处理手册
