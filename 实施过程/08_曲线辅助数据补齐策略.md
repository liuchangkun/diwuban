# æ³µç»„ç‰¹æ€§æ›²çº¿æ‹Ÿåˆ - æ›²çº¿è¾…åŠ©æ•°æ®è¡¥é½ç­–ç•¥

## ğŸš€ åˆ©ç”¨æ‹Ÿåˆæ›²çº¿ä¼˜åŒ–æ•°æ®è¡¥é½

### æ›²çº¿è¾…åŠ©æ•°æ®è¡¥é½ç­–ç•¥

```python
# app/services/curve_enhanced_completion.py

class CurveEnhancedCompletion:
    """åŸºäºæ‹Ÿåˆæ›²çº¿çš„å¢å¼ºæ•°æ®è¡¥é½"""
    
    def __init__(self, curve_fitting_service):
        self.curve_service = curve_fitting_service
        self.completion_strategies = {
            'curve_based': self._curve_based_completion,
            'hybrid': self._hybrid_completion,
            'multi_curve': self._multi_curve_completion
        }
    
    async def enhance_completion_with_curves(self, 
                                           station_id: str,
                                           missing_data: Dict,
                                           curve_models: Dict) -> Dict:
        """ä½¿ç”¨æ‹Ÿåˆæ›²çº¿å¢å¼ºæ•°æ®è¡¥é½ç²¾åº¦"""
        
        enhanced_data = {}
        
        for metric_type, missing_points in missing_data.items():
            
            # 1. é€‰æ‹©ç›¸å…³æ›²çº¿æ¨¡å‹
            relevant_curves = self._select_relevant_curves(metric_type, curve_models)
            
            if not relevant_curves:
                # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                enhanced_data[metric_type] = await self._traditional_completion(missing_points)
                continue
            
            # 2. åŸºäºæ›²çº¿çš„æ™ºèƒ½è¡¥é½
            completed_points = []
            
            for point in missing_points:
                # æ”¶é›†ç›¸å…³æŒ‡æ ‡çš„å·²çŸ¥å€¼
                known_metrics = await self._get_known_metrics_at_time(
                    station_id, point['timestamp']
                )
                
                # ä½¿ç”¨æ›²çº¿æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼
                predicted_value = await self._predict_from_curves(
                    metric_type, known_metrics, relevant_curves
                )
                
                # ç½®ä¿¡åº¦è¯„ä¼°
                confidence = self._calculate_prediction_confidence(
                    predicted_value, known_metrics, relevant_curves
                )
                
                completed_points.append({
                    'timestamp': point['timestamp'],
                    'value': predicted_value,
                    'confidence': confidence,
                    'method': 'CURVE_ENHANCED',
                    'source_curves': [curve.curve_type for curve in relevant_curves]
                })
            
            enhanced_data[metric_type] = completed_points
        
        return enhanced_data
    
    def _select_relevant_curves(self, metric_type: str, curve_models: Dict) -> List:
        """é€‰æ‹©ä¸ç›®æ ‡æŒ‡æ ‡ç›¸å…³çš„æ›²çº¿æ¨¡å‹"""
        relevant_curves = []
        
        # åŸºäºæŒ‡æ ‡ç±»å‹é€‰æ‹©ç›¸å…³æ›²çº¿
        curve_mapping = {
            'pump_flow_rate': ['HQ', 'EtaQ', 'PQ', 'NQ', 'fQ'],
            'pump_head': ['QH', 'PH', 'NH', 'fH'],
            'pump_efficiency': ['QEta', 'PEta', 'NEta', 'fEta'],
            'pump_active_power': ['QP', 'HP', 'NP', 'fP']
        }
        
        target_curves = curve_mapping.get(metric_type, [])
        
        for curve_type in target_curves:
            if curve_type in curve_models and curve_models[curve_type].quality_score > 0.7:
                relevant_curves.append(curve_models[curve_type])
        
        return relevant_curves
    
    async def _predict_from_curves(self, metric_type: str, 
                                 known_metrics: Dict, 
                                 curves: List) -> float:
        """åŸºäºæ›²çº¿æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼"""
        
        predictions = []
        weights = []
        
        for curve in curves:
            try:
                # æ ¹æ®æ›²çº¿ç±»å‹è·å–è¾“å…¥å˜é‡
                input_value = self._get_curve_input(curve.curve_type, known_metrics)
                
                if input_value is not None:
                    prediction = curve.predict(input_value)
                    confidence = curve.get_prediction_confidence(input_value)
                    
                    predictions.append(prediction)
                    weights.append(confidence * curve.quality_score)
                
            except Exception as e:
                self.logger.warning(f"æ›²çº¿ {curve.curve_type} é¢„æµ‹å¤±è´¥: {e}")
                continue
        
        if not predictions:
            return None
        
        # åŠ æƒå¹³å‡é¢„æµ‹ç»“æœ
        weighted_prediction = np.average(predictions, weights=weights)
        
        return weighted_prediction
    
    def _get_curve_input(self, curve_type: str, known_metrics: Dict) -> Optional[float]:
        """æ ¹æ®æ›²çº¿ç±»å‹è·å–è¾“å…¥å˜é‡"""
        
        input_mapping = {
            'HQ': 'pump_flow_rate',      # æ‰¬ç¨‹-æµé‡æ›²çº¿
            'EtaQ': 'pump_flow_rate',    # æ•ˆç‡-æµé‡æ›²çº¿
            'PQ': 'pump_flow_rate',      # åŠŸç‡-æµé‡æ›²çº¿
            'QH': 'pump_head',           # æµé‡-æ‰¬ç¨‹æ›²çº¿
            'QP': 'pump_active_power',   # æµé‡-åŠŸç‡æ›²çº¿
            'fQ': 'pump_frequency',      # é¢‘ç‡-æµé‡æ›²çº¿
            'fH': 'pump_frequency',      # é¢‘ç‡-æ‰¬ç¨‹æ›²çº¿
            'fP': 'pump_frequency'       # é¢‘ç‡-åŠŸç‡æ›²çº¿
        }
        
        input_metric = input_mapping.get(curve_type)
        return known_metrics.get(input_metric) if input_metric else None
    
    def _calculate_prediction_confidence(self, predicted_value: float, 
                                       known_metrics: Dict, 
                                       curves: List) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        
        if not curves:
            return 0.0
        
        # åŸºäºæ›²çº¿è´¨é‡å’Œä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        quality_scores = [curve.quality_score for curve in curves]
        avg_quality = np.mean(quality_scores)
        
        # æ£€æŸ¥é¢„æµ‹å€¼çš„ç‰©ç†åˆç†æ€§
        physical_validity = self._validate_physical_reasonableness(
            predicted_value, known_metrics
        )
        
        # ç»¼åˆç½®ä¿¡åº¦
        confidence = avg_quality * physical_validity * len(curves) / 5.0
        
        return min(1.0, confidence)
```

### å¤šç­–ç•¥èåˆè¡¥é½

```python
class HybridCompletionStrategy:
    """æ··åˆè¡¥é½ç­–ç•¥"""
    
    def __init__(self):
        self.strategies = {
            'FFILL': self._ffill_completion,
            'MEAN': self._mean_completion,
            'REG': self._regression_completion,
            'CURVE_ENHANCED': self._curve_enhanced_completion
        }
        
        self.strategy_weights = {
            'FFILL': 0.9,           # çŸ­æ—¶é—´ç¼ºå¤±ä¼˜å…ˆ
            'MEAN': 0.7,            # ä¸­ç­‰æ—¶é—´ç¼ºå¤±
            'REG': 0.6,             # é•¿æ—¶é—´ç¼ºå¤±
            'CURVE_ENHANCED': 0.95  # æœ‰æ›²çº¿æ—¶ä¼˜å…ˆ
        }
    
    async def adaptive_hybrid_completion(self, missing_context: Dict, 
                                       available_curves: Dict) -> Dict:
        """è‡ªé€‚åº”æ··åˆè¡¥é½ç­–ç•¥"""
        
        # 1. è¯„ä¼°æ¯ç§ç­–ç•¥çš„é€‚ç”¨æ€§
        strategy_scores = await self._evaluate_strategy_applicability(
            missing_context, available_curves
        )
        
        # 2. é€‰æ‹©æœ€ä¼˜ç­–ç•¥ç»„åˆ
        selected_strategies = self._select_optimal_strategies(strategy_scores)
        
        # 3. å¹¶è¡Œæ‰§è¡Œå¤šç§ç­–ç•¥
        completion_results = await self._execute_strategies_parallel(
            selected_strategies, missing_context, available_curves
        )
        
        # 4. èåˆç»“æœ
        final_result = await self._fuse_completion_results(completion_results)
        
        return final_result
    
    async def _evaluate_strategy_applicability(self, context: Dict, curves: Dict) -> Dict:
        """è¯„ä¼°ç­–ç•¥é€‚ç”¨æ€§"""
        
        scores = {}
        
        # FFILLç­–ç•¥è¯„åˆ†
        ffill_score = self._evaluate_ffill_applicability(context)
        scores['FFILL'] = ffill_score
        
        # MEANç­–ç•¥è¯„åˆ†
        mean_score = self._evaluate_mean_applicability(context)
        scores['MEAN'] = mean_score
        
        # REGç­–ç•¥è¯„åˆ†
        reg_score = self._evaluate_regression_applicability(context)
        scores['REG'] = reg_score
        
        # CURVE_ENHANCEDç­–ç•¥è¯„åˆ†
        curve_score = self._evaluate_curve_applicability(context, curves)
        scores['CURVE_ENHANCED'] = curve_score
        
        return scores
    
    def _evaluate_ffill_applicability(self, context: Dict) -> float:
        """è¯„ä¼°FFILLç­–ç•¥é€‚ç”¨æ€§"""
        
        gap_hours = context.get('gap_hours', 0)
        data_quality = context.get('data_quality', 0)
        context_availability = context.get('context_availability', False)
        
        # çŸ­æ—¶é—´ç¼ºå¤±ä¸”æœ‰ä¸Šä¸‹æ–‡æ—¶é€‚ç”¨æ€§é«˜
        if gap_hours <= 2 and context_availability and data_quality > 0.8:
            return 0.95
        elif gap_hours <= 6 and context_availability:
            return 0.75
        elif gap_hours <= 12:
            return 0.5
        else:
            return 0.2
    
    def _evaluate_curve_applicability(self, context: Dict, curves: Dict) -> float:
        """è¯„ä¼°æ›²çº¿å¢å¼ºç­–ç•¥é€‚ç”¨æ€§"""
        
        if not curves:
            return 0.0
        
        # æ£€æŸ¥å¯ç”¨æ›²çº¿çš„è´¨é‡
        curve_qualities = [curve.quality_score for curve in curves.values()]
        avg_quality = np.mean(curve_qualities) if curve_qualities else 0
        
        # æ£€æŸ¥ç›¸å…³æŒ‡æ ‡çš„å¯ç”¨æ€§
        related_metrics_available = context.get('related_metrics_count', 0) / 5.0  # å‡è®¾æœ€å¤š5ä¸ªç›¸å…³æŒ‡æ ‡
        
        # ç»¼åˆè¯„åˆ†
        applicability = avg_quality * related_metrics_available * len(curve_qualities) / 3.0
        
        return min(1.0, applicability)
```

### è¡¥é½è´¨é‡è¯„ä¼°ä¸ä¼˜åŒ–

```python
class CompletionQualityAssessment:
    """è¡¥é½è´¨é‡è¯„ä¼°ä¸ä¼˜åŒ–"""
    
    def __init__(self):
        self.quality_metrics = {
            'accuracy': self._calculate_accuracy,
            'consistency': self._calculate_consistency,
            'physical_validity': self._validate_physical_constraints,
            'temporal_coherence': self._check_temporal_coherence
        }
    
    async def assess_completion_quality(self, completed_data: Dict, 
                                      reference_data: Dict = None) -> Dict:
        """è¯„ä¼°è¡¥é½è´¨é‡"""
        
        quality_report = {}
        
        for metric_type, data_points in completed_data.items():
            
            # 1. è®¡ç®—å„é¡¹è´¨é‡æŒ‡æ ‡
            quality_scores = {}
            
            for metric_name, metric_func in self.quality_metrics.items():
                try:
                    score = await metric_func(data_points, reference_data)
                    quality_scores[metric_name] = score
                except Exception as e:
                    quality_scores[metric_name] = 0.0
                    self.logger.warning(f"è´¨é‡æŒ‡æ ‡ {metric_name} è®¡ç®—å¤±è´¥: {e}")
            
            # 2. ç»¼åˆè´¨é‡è¯„åˆ†
            overall_quality = self._calculate_overall_quality(quality_scores)
            
            # 3. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            quality_report[metric_type] = {
                'individual_scores': quality_scores,
                'overall_quality': overall_quality,
                'quality_grade': self._assign_quality_grade(overall_quality),
                'recommendations': self._generate_improvement_recommendations(quality_scores)
            }
        
        return quality_report
    
    async def _calculate_accuracy(self, completed_data: List, reference_data: Dict = None) -> float:
        """è®¡ç®—è¡¥é½ç²¾åº¦"""
        
        if not reference_data:
            # æ— å‚è€ƒæ•°æ®æ—¶ï¼Œä½¿ç”¨å†…éƒ¨ä¸€è‡´æ€§è¯„ä¼°
            return self._estimate_accuracy_from_consistency(completed_data)
        
        # æœ‰å‚è€ƒæ•°æ®æ—¶ï¼Œè®¡ç®—å®é™…ç²¾åº¦
        accurate_points = 0
        total_points = 0
        
        for point in completed_data:
            timestamp = point['timestamp']
            predicted_value = point['value']
            
            if timestamp in reference_data:
                actual_value = reference_data[timestamp]
                relative_error = abs(predicted_value - actual_value) / max(abs(actual_value), 1e-6)
                
                if relative_error < 0.1:  # 10%è¯¯å·®é˜ˆå€¼
                    accurate_points += 1
                
                total_points += 1
        
        return accurate_points / total_points if total_points > 0 else 0.0
    
    async def _calculate_consistency(self, completed_data: List) -> float:
        """è®¡ç®—è¡¥é½ä¸€è‡´æ€§"""
        
        if len(completed_data) < 3:
            return 1.0  # æ•°æ®ç‚¹å¤ªå°‘ï¼Œé»˜è®¤ä¸€è‡´
        
        # æ£€æŸ¥å€¼çš„å˜åŒ–è¶‹åŠ¿ä¸€è‡´æ€§
        values = [point['value'] for point in completed_data]
        
        # è®¡ç®—ä¸€é˜¶å·®åˆ†çš„å¹³æ»‘åº¦
        first_diff = np.diff(values)
        smoothness = 1.0 - (np.std(first_diff) / (np.mean(np.abs(first_diff)) + 1e-6))
        
        return max(0.0, min(1.0, smoothness))
    
    def _assign_quality_grade(self, overall_quality: float) -> str:
        """åˆ†é…è´¨é‡ç­‰çº§"""
        
        if overall_quality >= 0.90:
            return 'ä¼˜ç§€ - é«˜è´¨é‡è¡¥é½'
        elif overall_quality >= 0.80:
            return 'è‰¯å¥½ - è´¨é‡åˆæ ¼'
        elif overall_quality >= 0.70:
            return 'å¯æ¥å— - åŸºæœ¬å¯ç”¨'
        elif overall_quality >= 0.60:
            return 'ä¸€èˆ¬ - éœ€è¦æ”¹è¿›'
        else:
            return 'å·® - ä¸å»ºè®®ä½¿ç”¨'
    
    def _generate_improvement_recommendations(self, quality_scores: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        if quality_scores.get('accuracy', 1.0) < 0.8:
            recommendations.append("ç²¾åº¦åä½ï¼Œå»ºè®®ä½¿ç”¨æ›´é«˜çº§çš„è¡¥é½ç®—æ³•")
        
        if quality_scores.get('consistency', 1.0) < 0.7:
            recommendations.append("ä¸€è‡´æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†è´¨é‡")
        
        if quality_scores.get('physical_validity', 1.0) < 0.9:
            recommendations.append("ç‰©ç†çº¦æŸéªŒè¯å¤±è´¥ï¼Œå»ºè®®å¢å¼ºçº¦æŸæ¡ä»¶")
        
        if quality_scores.get('temporal_coherence', 1.0) < 0.8:
            recommendations.append("æ—¶é—´è¿è´¯æ€§ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–æ—¶é—´åºåˆ—å¤„ç†")
        
        return recommendations
```

### ç²¾åº¦æå‡éªŒè¯

#### ğŸ“Š å…·ä½“å®æ–½æ¡†æ¶
```python
class IntegratedCompletionFramework:
    def __init__(self):
        self.strategy_performance = {
            'FFILL': {'success_rate': 0.95, 'avg_confidence': 0.88},
            'MEAN': {'success_rate': 0.85, 'avg_confidence': 0.72},
            'REG': {'success_rate': 0.75, 'avg_confidence': 0.65},
            'CURVE_ENHANCED': {'success_rate': 0.92, 'avg_confidence': 0.89}
        }
    
    async def adaptive_completion(self, missing_data_context: Dict) -> str:
        """è‡ªé€‚åº”è¡¥é½ç­–ç•¥é€‰æ‹©"""
        
        # 1. ç¯å¢ƒè¯„ä¼°
        context_score = self._evaluate_context(missing_data_context)
        
        # 2. å†å²æ€§èƒ½åŠ æƒ
        weighted_strategies = self._apply_historical_weights(context_score)
        
        # 3. é€‰æ‹©æœ€ä¼˜ç­–ç•¥
        selected_strategy = max(weighted_strategies, key=weighted_strategies.get)
        
        return selected_strategy
```

### æ•°æ®é©±åŠ¨é€‰æ‹©

#### ğŸ¯ æ•°æ®é©±åŠ¨é€‰æ‹©
- **æ•°æ®é‡å¤§ï¼ˆ>200æ ·æœ¬ï¼‰**: ä¼˜å…ˆç¥ç»ç½‘ç»œå’Œéšæœºæ£®æ—
- **æ•°æ®é‡ä¸­ç­‰ï¼ˆ50-200æ ·æœ¬ï¼‰**: é«˜æ–¯è¿‡ç¨‹å’Œå¤šé¡¹å¼æ‹Ÿåˆ
- **æ•°æ®é‡å°ï¼ˆ<50æ ·æœ¬ï¼‰**: æ ·æ¡å’Œçº¿æ€§æ‹Ÿåˆ
- **æ•°æ®ä¸è¶³**: ä½¿ç”¨é»˜è®¤ç†è®ºæ›²çº¿

#### ğŸ¯ ç²¾åº¦æå‡ç›®æ ‡
- **æ‰¬ç¨‹RMSE**: ä»4.0mé™ä½åˆ°1.5mï¼ˆæå‡62.5%ï¼‰
- **æ•ˆç‡MAE**: ä»8%é™ä½åˆ°3%ï¼ˆæå‡62.5%ï¼‰
- **æµé‡è¯¯å·®**: ä»12%é™ä½åˆ°4%ï¼ˆæå‡66.7%ï¼‰
- **æ•´ä½“ç²¾åº¦**: ç»¼åˆç²¾åº¦æå‡60%ä»¥ä¸Š

### å¹¶è¡Œè®¡ç®—ä¼˜åŒ–

#### ğŸ“Š å¹¶è¡Œè®¡ç®—ä¼˜åŒ–
- **å¹¶è¡Œè®¡ç®—**: å……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼Œæé«˜ä¼˜åŒ–æ•ˆç‡
- **é›†æˆå­¦ä¹ **: èåˆå¤šç§ç®—æ³•ä¼˜åŠ¿ï¼Œæé«˜é¢„æµ‹ç²¾åº¦
- **åœ¨çº¿æ ¡å‡†**: ç»“åˆRLSç®—æ³•å®ç°å‚æ•°è‡ªé€‚åº”æ›´æ–°

#### ğŸ›¡ï¸ é²æ£’æ€§ä¿è¯
- **é™çº§æœºåˆ¶**: é€šè¿‡é™çº§æœºåˆ¶ç¡®ä¿ç³»ç»Ÿçš„é²æ£’æ€§
- **å¤šæ›²çº¿éªŒè¯**: H-Qã€Î·-Qã€P-Qæ›²çº¿ç›¸äº’éªŒè¯
- **ç‰©ç†çº¦æŸ**: ç¡®ä¿æ‰€æœ‰ç»“æœç¬¦åˆç‰©ç†å®šå¾‹

### ç›‘æ§ä¸æ—¥å¿—

#### ğŸ“Š å…³é”®æŒ‡æ ‡ç›‘æ§
```python
class PerformanceMetrics:
    key_metrics = {
        'completion_success_rate': 0.95,  # è¡¥é½æˆåŠŸç‡
        'fitting_accuracy': 0.85,         # æ‹Ÿåˆç²¾åº¦
        'dynamic_adjustment_frequency': 0.1, # åŠ¨æ€è°ƒæ•´é¢‘ç‡
        'system_response_time': 30,       # ç³»ç»Ÿå“åº”æ—¶é—´(ç§’)
        'overall_improvement': 0.6        # æ•´ä½“æ”¹å–„å¹…åº¦
    }
```

è¿™æ ·çš„è®¾è®¡æ—¢ä¿è¯äº†ç³»ç»Ÿçš„é²æ£’æ€§ï¼ˆé€šè¿‡é™çº§æœºåˆ¶ï¼‰ï¼Œåˆå……åˆ†å‘æŒ¥äº†å„ç§ç®—æ³•çš„ä¼˜åŠ¿ï¼ˆé€šè¿‡å¹¶è¡Œä¼˜åŒ–å’Œæ™ºèƒ½èåˆï¼‰ï¼Œæœ€ç»ˆå®ç°**60%ä»¥ä¸Šçš„ç²¾åº¦æå‡**ç›®æ ‡ã€‚

è¿™ä¸ªæ›²çº¿è¾…åŠ©æ•°æ®è¡¥é½ç­–ç•¥æ¨¡å—æä¾›äº†ï¼š

1. **åŸºäºæ›²çº¿çš„æ™ºèƒ½è¡¥é½** - åˆ©ç”¨æ‹Ÿåˆæ›²çº¿é¢„æµ‹ç¼ºå¤±å€¼ï¼Œæ˜¾è‘—æå‡è¡¥é½ç²¾åº¦
2. **å¤šç­–ç•¥èåˆæœºåˆ¶** - è‡ªé€‚åº”é€‰æ‹©å’Œèåˆå¤šç§è¡¥é½ç­–ç•¥
3. **è´¨é‡è¯„ä¼°ä½“ç³»** - å…¨é¢è¯„ä¼°è¡¥é½è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®
4. **ç²¾åº¦æå‡éªŒè¯** - é‡åŒ–éªŒè¯60%ä»¥ä¸Šçš„ç²¾åº¦æå‡æ•ˆæœ
5. **å¹¶è¡Œä¼˜åŒ–æ¡†æ¶** - å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºæé«˜è¡¥é½æ•ˆç‡

é€šè¿‡è¿™äº›å…ˆè¿›çš„è¡¥é½ç­–ç•¥ï¼Œç³»ç»Ÿèƒ½å¤Ÿåœ¨å¤æ‚å·¥å†µä¸‹ä»ä¿æŒé«˜è´¨é‡çš„æ•°æ®è¡¥é½æ•ˆæœã€‚