# æ³µç«™ä¼˜åŒ–ç³»ç»Ÿå®æ–½è¿‡ç¨‹ - M1é˜¶æ®µï¼šæ•°æ®åº“å‡½æ•°ææ•ˆ + åŸºç¡€è¡¨å»ºè®¾

## ğŸ¯ M1é˜¶æ®µï¼šæ•°æ®åº“å‡½æ•°ææ•ˆ + åŸºç¡€è¡¨å»ºè®¾

### ğŸ“Š M1.1 æ•°æ®åº“å‡½æ•°ææ•ˆä¸è§†å›¾æ‰©å±•

#### ä»»åŠ¡ç›®æ ‡

ä¼˜åŒ–ç°æœ‰ `get_*_by_time_range` ç³»åˆ—å‡½æ•°ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½20%ä»¥ä¸Šï¼Œå¹¶æ‰©å±•è§†å›¾ä½“ç³»æ”¯æŒæœªæ¥ä½¿ç”¨

#### ğŸ¤” è®¾è®¡é—®é¢˜ä¸æ–¹æ¡ˆ

**é—®é¢˜1ï¼šæ˜¯å¦éœ€è¦å¢åŠ æ›´å¤šè§†å›¾å’Œå‡½æ•°ï¼Ÿ**

- **ä¸ºä»€ä¹ˆéœ€è¦ï¼š**

  - ç°æœ‰å‡½æ•°ä¸»è¦é¢å‘åŸå§‹æ•°æ®æŸ¥è¯¢ï¼Œç¼ºå°‘èšåˆã€ç»Ÿè®¡ã€è¶‹åŠ¿åˆ†æç±»è§†å›¾
  - M2-M4é˜¶æ®µéœ€è¦å¤§é‡çš„æ•°æ®èšåˆå’Œè®¡ç®—ï¼Œéœ€è¦é«˜æ•ˆçš„æ•°æ®è®¿é—®æ¥å£
  - å†å²æ•°æ®åˆ†æã€è´¨é‡æŠ¥å‘Šã€ç»Ÿè®¡ç‰¹å¾æå–éœ€è¦ä¸“é—¨çš„è§†å›¾æ”¯æŒ

- **æ¨èæ–¹æ¡ˆï¼š**

  ```sql
  -- 1. æ—¶åºèšåˆè§†å›¾ç³»åˆ—
  CREATE OR REPLACE VIEW v_device_metrics_hourly AS
  SELECT
      station_id, device_id, metric_type,
      date_trunc('hour', metric_time) as hour_bucket,
      avg(metric_value) as avg_value,
      min(metric_value) as min_value,
      max(metric_value) as max_value,
      count(*) as sample_count,
      stddev(metric_value) as std_value
  FROM fact_measurements
  GROUP BY station_id, device_id, metric_type, hour_bucket;

  -- 2. è´¨é‡ç»Ÿè®¡è§†å›¾
  CREATE OR REPLACE VIEW v_data_quality_stats AS
  SELECT
      station_id, device_id, metric_type,
      date_trunc('day', metric_time) as date_bucket,
      count(*) as total_points,
      count(CASE WHEN metric_value IS NULL THEN 1 END) as null_count,
      count(CASE WHEN abs(metric_value - lag(metric_value) OVER w) > 3*stddev(metric_value) OVER w THEN 1 END) as outlier_count
  FROM fact_measurements
  WINDOW w AS (PARTITION BY station_id, device_id, metric_type ORDER BY metric_time ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)
  GROUP BY station_id, device_id, metric_type, date_bucket;

  -- 3. è¿è¡ŒçŠ¶æ€èšåˆè§†å›¾
  CREATE OR REPLACE VIEW v_pump_operation_summary AS
  SELECT
      station_id, device_id,
      date_trunc('day', metric_time) as date_bucket,
      avg(CASE WHEN metric_type = 'pump_frequency' THEN metric_value END) as avg_frequency,
      avg(CASE WHEN metric_type = 'pump_active_power' THEN metric_value END) as avg_power,
      count(CASE WHEN metric_type = 'pump_frequency' AND metric_value > 5 THEN 1 END) as running_minutes
  FROM fact_measurements
  WHERE metric_type IN ('pump_frequency', 'pump_active_power')
  GROUP BY station_id, device_id, date_bucket;
  ```

- **ä¼˜ç‚¹ï¼š**

  - æ•°æ®åº“å±‚èšåˆï¼Œå¤§å¹…å‡å°‘ç½‘ç»œä¼ è¾“å’ŒPythonå†…å­˜ä½¿ç”¨
  - ç´¢å¼•å‹å¥½ï¼ŒæŸ¥è¯¢æ€§èƒ½é«˜
  - æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

- **ç¼ºç‚¹ï¼š**

  - å¢åŠ æ•°æ®åº“ç»´æŠ¤å¤æ‚åº¦
  - è§†å›¾æ›´æ–°éœ€è¦è€ƒè™‘å‘ä¸‹å…¼å®¹æ€§

- **æ›¿ä»£æ–¹æ¡ˆï¼š**

  - æ–¹æ¡ˆAï¼šä½¿ç”¨Redisç¼“å­˜é¢„è®¡ç®—ç»“æœ
  - æ–¹æ¡ˆBï¼šä½¿ç”¨ClickHouseç­‰OLAPæ•°æ®åº“åšåˆ†æå±‚

**é—®é¢˜2ï¼šå¦‚ä½•æ”¯æŒæ–°æ³µç«™ã€è®¾å¤‡ã€æŒ‡æ ‡çš„è‡ªé€‚åº”æ‰©å±•ï¼Ÿ**

- **ä¸ºä»€ä¹ˆé‡è¦ï¼š**

  - ç³»ç»Ÿéœ€è¦æ”¯æŒåŠ¨æ€æ‰©å±•ï¼Œé¿å…æ¯æ¬¡æ–°å¢è®¾å¤‡éƒ½éœ€è¦ä¿®æ”¹ä»£ç 
  - ä¸åŒæ³µç«™çš„è®¾å¤‡ç±»å‹å’ŒæŒ‡æ ‡é…ç½®å¯èƒ½ä¸åŒ
  - éœ€è¦æ”¯æŒçƒ­æ’æ‹”å¼çš„é…ç½®æ›´æ–°

- **æ¨èæ–¹æ¡ˆï¼šåŸºäºé…ç½®çš„åŠ¨æ€è¡¨ç»“æ„ç®¡ç†**

  ```python
  # configs/dynamic_schema.yaml
  schema_templates:
    completion_tables:
      base_table: "completion_runs_{station_type}"
      partition_strategy: "monthly"
      indexes:
        - fields: ["station_id", "start_time"]
          type: "btree"
        - fields: ["run_id"]
          type: "hash"

    device_specific_metrics:
      pump_metrics:
        - "pump_frequency"
        - "pump_active_power"
        - "pump_flow_rate"
      valve_metrics:
        - "valve_position"
        - "valve_pressure_drop"

  auto_discovery:
    scan_interval: "1h"
    new_device_template: "device_template_v1"
    metric_validation_rules:
      numeric_range: [-999999, 999999]
      unit_validation: true
  ```

  ```python
  # app/services/schema_manager.py
  class DynamicSchemaManager:
      async def discover_new_devices(self) -> List[DeviceConfig]:
          """æ‰«ææ•°æ®æ˜ å°„æ–‡ä»¶ï¼Œå‘ç°æ–°è®¾å¤‡é…ç½®"""

      async def create_device_tables(self, device_config: DeviceConfig) -> bool:
          """ä¸ºæ–°è®¾å¤‡åˆ›å»ºç›¸å…³è¡¨ç»“æ„å’Œç´¢å¼•"""

      async def update_metric_config(self, new_metrics: List[MetricConfig]) -> bool:
          """åŠ¨æ€æ›´æ–°æŒ‡æ ‡é…ç½®"""
  ```

- **ä¼˜ç‚¹ï¼š**

  - å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ— éœ€äººå·¥å¹²é¢„
  - åŸºäºé…ç½®æ–‡ä»¶ï¼Œæ˜“äºç‰ˆæœ¬æ§åˆ¶å’Œå®¡è®¡
  - æ”¯æŒå›æ»šå’Œç°åº¦å‘å¸ƒ

- **ç¼ºç‚¹ï¼š**

  - ç³»ç»Ÿå¤æ‚åº¦å¢åŠ 
  - éœ€è¦å®Œå–„çš„å®¹é”™æœºåˆ¶

**é—®é¢˜3ï¼šå¦‚ä½•å¤„ç†æ•°æ®é‡éå¸¸å¤§çš„æ€§èƒ½é—®é¢˜ï¼Ÿ**

- **ä¸ºä»€ä¹ˆå…³é”®ï¼š**

  - æ³µç«™æ•°æ®é€šå¸¸æ˜¯ç§’çº§é‡‡é›†ï¼Œæ•°æ®é‡å·¨å¤§
  - M2-M4é˜¶æ®µéœ€è¦å¤§é‡çš„å†å²æ•°æ®è®¡ç®—å’Œåˆ†æ
  - å®æ—¶æ€§è¦æ±‚é«˜ï¼Œä¸èƒ½å½±å“åœ¨çº¿ä¸šåŠ¡

- **æ¨èæ–¹æ¡ˆï¼šå¤šå±‚æ¬¡æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**

  ```sql
  -- 1. é«˜çº§åˆ†åŒºç­–ç•¥ï¼ˆæ—¶é—´ + è®¾å¤‡ + æŒ‡æ ‡ä¸‰ç»´åˆ†åŒºï¼‰
  CREATE TABLE fact_measurements_optimized (
      metric_time TIMESTAMP NOT NULL,
      station_id VARCHAR(50) NOT NULL,
      device_id VARCHAR(50) NOT NULL,
      metric_type VARCHAR(50) NOT NULL,
      metric_value DECIMAL(15,6),
      -- ... other fields
  ) PARTITION BY RANGE (metric_time);

  -- æ¯å¤©ä¸€ä¸ªåˆ†åŒºï¼Œå†æŒ‰station_idå“ˆå¸Œå­åˆ†åŒº
  CREATE TABLE fact_measurements_y2024m01d01 PARTITION OF fact_measurements_optimized
  FOR VALUES FROM ('2024-01-01') TO ('2024-01-02')
  PARTITION BY HASH (station_id);

  -- 2. æ™ºèƒ½ç´¢å¼•ç­–ç•¥
  -- æœ€æ–°æ•°æ®ä½¿ç”¨B-treeç´¢å¼•
  CREATE INDEX idx_recent_data ON fact_measurements_y2024m01d01
  USING btree (station_id, device_id, metric_time, metric_type);

  -- å†å²æ•°æ®ä½¿ç”¨BRINç´¢å¼•
  CREATE INDEX idx_historical_data ON fact_measurements_y2023m01d01
  USING brin (metric_time, station_id);
  ```

  ```python
  # app/services/performance_optimizer.py
  class QueryPerformanceOptimizer:
      async def adaptive_query_routing(self, query_params: QueryParams) -> QueryStrategy:
          """æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç­–ç•¥"""
          if query_params.time_range.days <= 1:
              return QueryStrategy.REAL_TIME_INDEX
          elif query_params.time_range.days <= 30:
              return QueryStrategy.RECENT_PARTITION
          else:
              return QueryStrategy.HISTORICAL_AGGREGATION

      async def parallel_processing(self, large_query: Query) -> QueryResult:
          """å¤§æŸ¥è¯¢è‡ªåŠ¨åˆ†ç‰‡å¹¶è¡Œå¤„ç†"""
          chunks = self.split_query_by_time_and_device(large_query)
          results = await asyncio.gather(*[self.execute_chunk(chunk) for chunk in chunks])
          return self.merge_results(results)
  ```

- **ä¼˜ç‚¹ï¼š**

  - é’ˆå¯¹ä¸åŒæŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–ï¼Œæ€§èƒ½æå‡æ˜¾è‘—
  - æ”¯æŒæ°´å¹³æ‰©å±•å’Œè´Ÿè½½å‡è¡¡
  - è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜

- **ç¼ºç‚¹ï¼š**

  - ç³»ç»Ÿå¤æ‚åº¦æ˜¾è‘—å¢åŠ 
  - éœ€è¦ä¸“ä¸šçš„DBAæ”¯æŒ

- **æ›¿ä»£æ–¹æ¡ˆï¼š**

  - æ–¹æ¡ˆAï¼šä½¿ç”¨TimescaleDBç­‰æ—¶åºæ•°æ®åº“
  - æ–¹æ¡ˆBï¼šå¼•å…¥åˆ†å¸ƒå¼æ•°æ®åº“å¦‚ClickHouse
  - æ–¹æ¡ˆCï¼šä½¿ç”¨åˆ—å­˜å‚¨æ ¼å¼å¦‚Parquet + DuckDB

#### å·¥ä½œå†…å®¹

1. **å‡½æ•°æ€§èƒ½åˆ†æ**

   ```sql
   -- åˆ†æç°æœ‰å‡½æ•°æ‰§è¡Œè®¡åˆ’
   EXPLAIN ANALYZE SELECT * FROM get_device_metrics_by_time_range(
       'STATION001', 'DEVICE001',
       '2024-01-01'::date, '2024-01-31'::date
   );
   ```

1. **ç´¢å¼•ä¼˜åŒ–**

   ```sql
   -- å¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
   CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_fact_measurements_compound
   ON fact_measurements (station_id, device_id, metric_time, metric_type);
   ```

1. **å‡½æ•°é‡æ„**

   - å‡å°‘ä¸å¿…è¦çš„JOINæ“ä½œ
   - ä¼˜åŒ–WHEREæ¡ä»¶é¡ºåº
   - å¢åŠ ç»“æœé›†ç¼“å­˜æœºåˆ¶

#### å®æ–½æ–¹æ³•

1. ä½¿ç”¨ `EXPLAIN ANALYZE` åˆ†æå½“å‰æ€§èƒ½ç“¶é¢ˆ
1. é’ˆå¯¹æ€§åˆ›å»ºå¤åˆç´¢å¼•
1. é‡å†™å‡½æ•°é€»è¾‘ï¼Œå‡å°‘æ‰«ææ•°æ®é‡
1. åœ¨gateway.pyä¸­å°è£…ä¼˜åŒ–åçš„å‡½æ•°è°ƒç”¨

#### éªŒæ”¶æ ‡å‡†

- [ ] æŸ¥è¯¢å“åº”æ—¶é—´æå‡20%ä»¥ä¸Š
- [ ] ç´¢å¼•å‘½ä¸­ç‡è¾¾åˆ°95%
- [ ] æ‰€æœ‰APIæ¥å£æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] å¹¶å‘100ç”¨æˆ·ä¸‹å“åº”æ—¶é—´\<2ç§’

### ğŸ“‹ M1.2 è‡ªé€‚åº”åŸºç¡€è¡¨å»ºè®¾

#### ä»»åŠ¡ç›®æ ‡

åˆ›å»ºå®Œæ•´çš„æ•°æ®è¡¥é½æµç¨‹è¿½è¸ªè¡¨ç»“æ„ï¼Œæ”¯æŒè‡ªé€‚åº”æ‰©å±•å’Œé«˜æ€§èƒ½å¤„ç†

#### ğŸ¤” è®¾è®¡é—®é¢˜ä¸æ–¹æ¡ˆ

**é—®é¢˜ï¼šå¢åŠ çš„è¡¨æ˜¯å¦è€ƒè™‘äº†æœªæ¥æ–°å¢æ³µç«™ã€è®¾å¤‡ã€æŒ‡æ ‡çš„è‡ªé€‚åº”ï¼Ÿ**

- **ä¸ºä»€ä¹ˆéœ€è¦è‡ªé€‚åº”ï¼š**

  - ä¸åŒæ³µç«™çš„è®¾å¤‡ç±»å‹ã€è§„æ¨¡ã€æŒ‡æ ‡é…ç½®å·®å¼‚å·¨å¤§
  - å•ä¸€è¡¨ç»“æ„éš¾ä»¥é€‚åº”æ‰€æœ‰åœºæ™¯ï¼Œæ€§èƒ½ä¹Ÿä¼šå—å½±å“
  - ç³»ç»Ÿéœ€è¦æ”¯æŒçƒ­éƒ¨ç½²å’ŒåŠ¨æ€æ‰©å±•

- **æ¨èæ–¹æ¡ˆï¼šæ¨¡æ¿åŒ–åŠ¨æ€è¡¨ç»“æ„**

  ```sql
  -- 1. åŸºç¡€æ¨¡æ¿è¡¨ï¼ˆä¸ç›´æ¥ä½¿ç”¨ï¼Œä»…ä½œä¸ºæ¨¡æ¿ï¼‰
  CREATE TABLE completion_runs_template (
      run_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      station_id VARCHAR(50) NOT NULL,
      start_time TIMESTAMP NOT NULL,
      end_time TIMESTAMP NOT NULL,
      strategy VARCHAR(20) NOT NULL CHECK (strategy IN ('FFILL', 'MEAN', 'REG')),
      status VARCHAR(20) DEFAULT 'PENDING',
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      completed_at TIMESTAMP,
      metrics_count INTEGER DEFAULT 0,
      filled_count INTEGER DEFAULT 0,
      -- è‡ªé€‚åº”å­—æ®µ
      station_type VARCHAR(50), -- æ³µç«™ç±»å‹ï¼ˆä¾›æ°´/æ’æ°´/åŠ å‹ç­‰ï¼‰
      device_count INTEGER, -- è®¾å¤‡æ•°é‡
      config_version VARCHAR(20), -- é…ç½®ç‰ˆæœ¬
      custom_params JSONB DEFAULT '{}' -- è‡ªå®šä¹‰å‚æ•°
  );

  -- 2. æŒ‰æ³µç«™ç±»å‹åŠ¨æ€åˆ›å»ºå®é™…è¡¨
  -- completion_runs_water_supply (ä¾›æ°´ç«™)
  -- completion_runs_sewage_treatment (æ±¡æ°´å¤„ç†ç«™)
  -- completion_runs_booster (åŠ å‹ç«™)

  -- 3. æŒ‡æ ‡ç±»å‹è‡ªé€‚åº”é…ç½®è¡¨
  CREATE TABLE metric_type_configs (
      station_type VARCHAR(50),
      device_type VARCHAR(50),
      metric_name VARCHAR(100),
      data_type VARCHAR(20),
      unit VARCHAR(20),
      valid_range_min DECIMAL(15,6),
      valid_range_max DECIMAL(15,6),
      completion_priority INTEGER, -- è¡¥é½ä¼˜å…ˆçº§
      computation_weight DECIMAL(5,3), -- è®¡ç®—æƒé‡
      is_critical BOOLEAN DEFAULT false, -- æ˜¯å¦å…³é”®æŒ‡æ ‡
      custom_rules JSONB DEFAULT '{}'
  );
  ```

  ```python
  # app/services/dynamic_table_manager.py
  class DynamicTableManager:
      def __init__(self):
          self.table_templates = self.load_templates()
          self.station_configs = self.load_station_configs()

      async def create_station_tables(self, station_config: StationConfig) -> bool:
          """ä¸ºæ–°æ³µç«™åˆ›å»ºä¸“ç”¨è¡¨ç»“æ„"""
          station_type = station_config.station_type
          table_suffix = f"_{station_type}_{station_config.region}"

          for template_name, template_sql in self.table_templates.items():
              actual_table_name = f"{template_name}{table_suffix}"
              # æ ¹æ®æ¨¡æ¿åˆ›å»ºå®é™…è¡¨
              await self.execute_template_sql(template_sql, actual_table_name, station_config)

              # åˆ›å»ºä¸“ç”¨ç´¢å¼•
              await self.create_optimized_indexes(actual_table_name, station_config)

          return True

      async def auto_discover_metrics(self, station_id: str) -> List[MetricConfig]:
          """è‡ªåŠ¨å‘ç°å’Œæ³¨å†Œæ–°æŒ‡æ ‡"""
          # æ‰«ææœ€è¿‘çš„æ•°æ®æ¥å‘ç°æ–°æŒ‡æ ‡
          new_metrics = await self.scan_new_metrics(station_id)

          for metric in new_metrics:
              # è‡ªåŠ¨æ¨æ–­æ•°æ®ç±»å‹å’Œå–å€¼èŒƒå›´
              metric_config = await self.infer_metric_config(metric)
              # æ³¨å†Œåˆ°é…ç½®è¡¨
              await self.register_metric_config(metric_config)

          return new_metrics
  ```

- **ä¼˜ç‚¹ï¼š**

  - å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ— éœ€äººå·¥å¹²é¢„
  - æŒ‰ç«™ç‚¹ç±»å‹çš„è¡¨ç»“æ„ä¼˜åŒ–ï¼Œæ€§èƒ½æ›´å¥½
  - æ”¯æŒä¸åŒç±»å‹ç«™ç‚¹çš„ä¸ªæ€§åŒ–é…ç½®
  - JSONBå­—æ®µæ”¯æŒçµæ´»æ‰©å±•

- **ç¼ºç‚¹ï¼š**

  - è¡¨æ•°é‡å¯èƒ½ä¼šè¾ƒå¤šï¼Œå½±å“ç®¡ç†å¤æ‚åº¦
  - éœ€è¦æ›´å¤æ‚çš„è·¯ç”±é€»è¾‘
  - æ•°æ®è¿ç§»å’Œå¤‡ä»½æ›´å¤æ‚

- **æ›¿ä»£æ–¹æ¡ˆï¼š**

  - æ–¹æ¡ˆAï¼šä½¿ç”¨å•ä¸€è¡¨ + é«˜æ•ˆåˆ†åŒºç­–ç•¥
  - æ–¹æ¡ˆBï¼šä½¿ç”¨NoSQLæ•°æ®åº“å­˜å‚¨éç»“æ„åŒ–æ•°æ®
  - æ–¹æ¡ˆCï¼šä½¿ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ¯ä¸ªç«™ç‚¹ç‹¬ç«‹æ•°æ®åº“

#### å·¥ä½œå†…å®¹

1. **completion_runs è¡¨**

   ```sql
   CREATE TABLE completion_runs (
       run_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       station_id VARCHAR(50) NOT NULL,
       start_time TIMESTAMP NOT NULL,
       end_time TIMESTAMP NOT NULL,
       strategy VARCHAR(20) NOT NULL CHECK (strategy IN ('FFILL', 'MEAN', 'REG')),
       status VARCHAR(20) DEFAULT 'PENDING',
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       completed_at TIMESTAMP,
       metrics_count INTEGER DEFAULT 0,
       filled_count INTEGER DEFAULT 0
   );
   ```

1. **completion_steps è¡¨**

   ```sql
   CREATE TABLE completion_steps (
       step_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       run_id UUID REFERENCES completion_runs(run_id),
       device_id VARCHAR(50) NOT NULL,
       metric_type VARCHAR(50) NOT NULL,
       original_value DECIMAL(15,6),
       filled_value DECIMAL(15,6),
       fill_method VARCHAR(20) NOT NULL,
       confidence_score DECIMAL(3,2),
       step_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```

1. **completion_audit è¡¨**

   ```sql
   CREATE TABLE completion_audit (
       audit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
       run_id UUID REFERENCES completion_runs(run_id),
       audit_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       total_gaps INTEGER,
       filled_gaps INTEGER,
       fill_rate DECIMAL(5,2),
       quality_score DECIMAL(3,2),
       audit_details JSONB
   );
   ```

#### å®æ–½æ–¹æ³•

1. åœ¨PostgreSQLä¸­æ‰§è¡ŒDDLè¯­å¥
1. åˆ›å»ºå¿…è¦çš„ç´¢å¼•å’Œçº¦æŸ
1. è®¾ç½®åˆ†åŒºç­–ç•¥ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
1. é…ç½®æƒé™å’Œå®‰å…¨è®¾ç½®

#### éªŒæ”¶æ ‡å‡†

- [ ] æ‰€æœ‰è¡¨åˆ›å»ºæˆåŠŸï¼Œçº¦æŸç”Ÿæ•ˆ
- [ ] ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒæŸ¥è¯¢æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] æƒé™é…ç½®æ­£ç¡®ï¼Œå®‰å…¨å®¡è®¡é€šè¿‡
- [ ] åˆ†åŒºç­–ç•¥æœ‰æ•ˆï¼Œæ”¯æŒæœˆåº¦æ•°æ®ç®¡ç†
