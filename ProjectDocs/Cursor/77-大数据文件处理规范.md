---
description: 大数据文件处理规范（流式处理、分片策略、并行化、内存优化、容错恢复）
---
# 大数据文件处理规范

## 文件大小分级处理
### 处理策略分级
- 小文件（<10MB）：全量内存加载，单线程处理；
- 中等文件（10MB-1GB）：分块读取，多线程处理；
- 大文件（1GB-10GB）：流式处理，分片并行；
- 超大文件（>10GB）：分布式处理，外部排序。

### 自动检测与路由
```python
def get_processing_strategy(file_size):
    if file_size < 10 * 1024 * 1024:  # 10MB
        return "single_thread_memory"
    elif file_size < 1024 * 1024 * 1024:  # 1GB
        return "multi_thread_chunks"
    elif file_size < 10 * 1024 * 1024 * 1024:  # 10GB
        return "streaming_parallel"
    else:
        return "distributed_processing"
```

## 流式处理架构
### 生产者-消费者模式
```python
import queue
import threading
from typing import Iterator, Any

class StreamProcessor:
    def __init__(self, buffer_size: int = 1000):
        self.buffer = queue.Queue(maxsize=buffer_size)
        self.producer_thread = None
        self.consumer_threads = []
        
    def producer(self, file_path: str):
        """生产者：读取文件并放入队列"""
        try:
            with open(file_path, 'r', encoding='utf-8', buffering=8192) as f:
                for line_no, line in enumerate(f, 1):
                    self.buffer.put((line_no, line.strip()))
        finally:
            # 发送结束信号
            self.buffer.put(None)
    
    def consumer(self, processor_func):
        """消费者：从队列获取数据并处理"""
        while True:
            item = self.buffer.get()
            if item is None:
                break
            try:
                processor_func(*item)
            except Exception as e:
                # 错误处理
                self.handle_error(item, e)
            finally:
                self.buffer.task_done()
```

### 背压控制
- 队列大小限制：防止内存溢出；
- 动态调速：根据处理速度调整读取速度；
- 优雅降级：高负载时减少并发度；
- 熔断机制：错误率过高时暂停处理。

## 分片策略
### 按行数分片
```python
def split_file_by_lines(file_path: str, lines_per_chunk: int = 100000):
    """按行数分片文件"""
    chunk_files = []
    with open(file_path, 'r', encoding='utf-8') as input_file:
        chunk_no = 0
        while True:
            chunk_file = f"{file_path}.chunk_{chunk_no}"
            lines_written = 0
            
            with open(chunk_file, 'w', encoding='utf-8') as output_file:
                for line in input_file:
                    output_file.write(line)
                    lines_written += 1
                    if lines_written >= lines_per_chunk:
                        break
            
            if lines_written == 0:
                break
                
            chunk_files.append(chunk_file)
            chunk_no += 1
    
    return chunk_files
```

### 按大小分片
```python
def split_file_by_size(file_path: str, chunk_size_mb: int = 100):
    """按文件大小分片"""
    chunk_size = chunk_size_mb * 1024 * 1024
    chunk_files = []
    
    with open(file_path, 'rb') as input_file:
        chunk_no = 0
        while True:
            chunk_file = f"{file_path}.chunk_{chunk_no}"
            bytes_written = 0
            
            with open(chunk_file, 'wb') as output_file:
                while bytes_written < chunk_size:
                    data = input_file.read(8192)
                    if not data:
                        break
                    output_file.write(data)
                    bytes_written += len(data)
            
            if bytes_written == 0:
                break
                
            chunk_files.append(chunk_file)
            chunk_no += 1
    
    return chunk_files
```

### 智能分片
- 边界处理：确保记录完整性，避免截断；
- 负载均衡：根据内容复杂度调整分片大小；
- 依赖处理：保持相关记录在同一分片；
- 元数据记录：记录分片信息用于合并。

## 并行处理框架
### 进程池处理
```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

def process_chunk_parallel(chunk_files: list, max_workers: int = None):
    """并行处理文件分片"""
    if max_workers is None:
        max_workers = min(len(chunk_files), mp.cpu_count())
    
    results = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # 提交任务
        future_to_chunk = {
            executor.submit(process_single_chunk, chunk_file): chunk_file
            for chunk_file in chunk_files
        }
        
        # 收集结果
        for future in as_completed(future_to_chunk):
            chunk_file = future_to_chunk[future]
            try:
                result = future.result()
                results.append((chunk_file, result))
            except Exception as e:
                print(f"Chunk {chunk_file} failed: {e}")
                # 错误处理和重试
                
    return results

def process_single_chunk(chunk_file: str):
    """处理单个文件分片"""
    # 实现具体的处理逻辑
    processed_rows = 0
    error_count = 0
    
    try:
        # 处理逻辑
        pass
    except Exception as e:
        error_count += 1
        # 错误处理
        
    return {
        'file': chunk_file,
        'processed_rows': processed_rows,
        'error_count': error_count
    }
```

### 线程池处理
```python
import threading
from concurrent.futures import ThreadPoolExecutor

class ThreadSafeProcessor:
    def __init__(self):
        self.lock = threading.Lock()
        self.total_processed = 0
        self.total_errors = 0
    
    def process_with_threads(self, data_chunks: list, max_workers: int = 4):
        """使用线程池处理数据"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self.process_chunk, chunk) 
                for chunk in data_chunks
            ]
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    with self.lock:
                        self.total_processed += result['processed']
                        self.total_errors += result['errors']
                except Exception as e:
                    print(f"Thread processing failed: {e}")
```

## 内存优化策略
### 懒加载和生成器
```python
def lazy_file_reader(file_path: str, buffer_size: int = 8192):
    """懒加载文件读取器"""
    with open(file_path, 'r', encoding='utf-8', buffering=buffer_size) as f:
        for line in f:
            yield line.strip()

def batch_generator(iterator, batch_size: int = 1000):
    """批量生成器"""
    batch = []
    for item in iterator:
        batch.append(item)
        if len(batch) >= batch_size:
            yield batch
            batch = []
    if batch:
        yield batch

# 使用示例
for batch in batch_generator(lazy_file_reader('large_file.csv'), 1000):
    # 处理批量数据
    process_batch(batch)
    # 手动清理内存
    del batch
```

### 内存监控和GC
```python
import gc
import psutil
import os

class MemoryMonitor:
    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        self.process = psutil.Process(os.getpid())
    
    def get_memory_usage(self) -> float:
        """获取当前内存使用率"""
        memory_info = self.process.memory_info()
        total_memory = psutil.virtual_memory().total
        return memory_info.rss / total_memory
    
    def check_and_cleanup(self):
        """检查内存使用并清理"""
        usage = self.get_memory_usage()
        
        if usage > self.critical_threshold:
            # 强制垃圾回收
            gc.collect()
            # 可以考虑其他清理措施
            print(f"Critical memory usage: {usage:.2%}, forced GC")
            return "critical"
        elif usage > self.warning_threshold:
            print(f"High memory usage: {usage:.2%}")
            return "warning"
        
        return "normal"
```

## 容错与恢复机制
### 检查点系统
```python
import json
import os
from typing import Dict, Any

class CheckpointManager:
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    def save_checkpoint(self, task_id: str, state: Dict[str, Any]):
        """保存检查点"""
        checkpoint_file = os.path.join(self.checkpoint_dir, f"{task_id}.json")
        with open(checkpoint_file, 'w') as f:
            json.dump(state, f, indent=2)
    
    def load_checkpoint(self, task_id: str) -> Dict[str, Any]:
        """加载检查点"""
        checkpoint_file = os.path.join(self.checkpoint_dir, f"{task_id}.json")
        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                return json.load(f)
        return {}
    
    def cleanup_checkpoint(self, task_id: str):
        """清理检查点"""
        checkpoint_file = os.path.join(self.checkpoint_dir, f"{task_id}.json")
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)

class ResumableProcessor:
    def __init__(self, checkpoint_manager: CheckpointManager):
        self.checkpoint_manager = checkpoint_manager
    
    def process_with_resume(self, file_path: str, task_id: str):
        """支持断点续传的处理"""
        # 加载检查点
        state = self.checkpoint_manager.load_checkpoint(task_id)
        start_line = state.get('last_processed_line', 0)
        processed_count = state.get('processed_count', 0)
        
        try:
            with open(file_path, 'r') as f:
                # 跳到上次处理的位置
                for _ in range(start_line):
                    next(f)
                
                # 继续处理
                for line_no, line in enumerate(f, start_line + 1):
                    # 处理逻辑
                    self.process_line(line)
                    processed_count += 1
                    
                    # 定期保存检查点
                    if line_no % 10000 == 0:
                        self.checkpoint_manager.save_checkpoint(task_id, {
                            'last_processed_line': line_no,
                            'processed_count': processed_count,
                            'file_path': file_path
                        })
            
            # 处理完成，清理检查点
            self.checkpoint_manager.cleanup_checkpoint(task_id)
            
        except Exception as e:
            # 保存当前状态
            self.checkpoint_manager.save_checkpoint(task_id, {
                'last_processed_line': line_no,
                'processed_count': processed_count,
                'file_path': file_path,
                'error': str(e)
            })
            raise
```

### 失败重试机制
```python
import time
import random
from functools import wraps

def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0, 
                      max_delay: float = 60.0, exponential_base: float = 2.0):
    """指数退避重试装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt == max_retries:
                        break
                    
                    # 计算延迟时间
                    delay = min(
                        base_delay * (exponential_base ** attempt) + 
                        random.uniform(0, 1),  # 添加抖动
                        max_delay
                    )
                    
                    print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s")
                    time.sleep(delay)
            
            # 所有重试都失败
            raise last_exception
        
        return wrapper
    return decorator

# 使用示例
@retry_with_backoff(max_retries=3, base_delay=2.0)
def process_file_with_retry(file_path: str):
    # 可能失败的文件处理逻辑
    pass
```

## 性能监控和调优
### 实时性能监控
```python
import time
import threading
from collections import deque
from typing import Optional

class PerformanceMonitor:
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.processing_times = deque(maxlen=window_size)
        self.throughput_samples = deque(maxlen=window_size)
        self.lock = threading.Lock()
        
    def record_processing_time(self, processing_time: float):
        """记录处理时间"""
        with self.lock:
            self.processing_times.append(processing_time)
    
    def record_throughput(self, items_processed: int, time_elapsed: float):
        """记录吞吐量"""
        throughput = items_processed / time_elapsed if time_elapsed > 0 else 0
        with self.lock:
            self.throughput_samples.append(throughput)
    
    def get_stats(self) -> dict:
        """获取性能统计"""
        with self.lock:
            if not self.processing_times:
                return {}
            
            processing_times = list(self.processing_times)
            throughput_samples = list(self.throughput_samples)
        
        return {
            'avg_processing_time': sum(processing_times) / len(processing_times),
            'max_processing_time': max(processing_times),
            'min_processing_time': min(processing_times),
            'avg_throughput': sum(throughput_samples) / len(throughput_samples) if throughput_samples else 0,
            'samples_count': len(processing_times)
        }

class AdaptiveProcessor:
    def __init__(self, initial_workers: int = 4, max_workers: int = 16):
        self.current_workers = initial_workers
        self.max_workers = max_workers
        self.monitor = PerformanceMonitor()
        
    def adjust_workers(self):
        """根据性能调整工作线程数"""
        stats = self.monitor.get_stats()
        if not stats:
            return
        
        avg_time = stats['avg_processing_time']
        avg_throughput = stats['avg_throughput']
        
        # 简单的自适应策略
        if avg_time > 2.0 and self.current_workers < self.max_workers:
            # 处理时间过长，增加工作线程
            self.current_workers = min(self.current_workers + 1, self.max_workers)
        elif avg_time < 0.5 and self.current_workers > 1:
            # 处理时间很短，可以减少工作线程
            self.current_workers = max(self.current_workers - 1, 1)
```

## 最佳实践总结
### 设计原则
1. **分而治之**：大文件拆分为小块并行处理
2. **流式处理**：避免全量内存加载
3. **容错设计**：支持断点续传和失败重试
4. **性能监控**：实时监控并自适应调整
5. **资源限制**：防止资源耗尽和系统崩溃

### 配置建议
```yaml
big_file_processing:
  # 文件大小阈值（字节）
  size_thresholds:
    small: 10485760      # 10MB
    medium: 1073741824   # 1GB
    large: 10737418240   # 10GB
  
  # 并行处理配置
  parallel:
    max_workers: 8
    chunk_size_mb: 100
    batch_size: 10000
  
  # 内存管理
  memory:
    warning_threshold: 0.8
    critical_threshold: 0.9
    gc_frequency: 1000  # 每处理1000条记录检查一次
  
  # 重试配置
  retry:
    max_attempts: 3
    base_delay: 2.0
    max_delay: 60.0
  
  # 检查点配置
  checkpoint:
    enabled: true
    frequency: 10000  # 每10000行保存一次
    cleanup_on_success: true
```

### 性能调优清单
- [ ] 根据文件大小选择合适的处理策略
- [ ] 配置合理的分片大小和并行度
- [ ] 实施内存监控和清理机制
- [ ] 设置检查点和断点续传
- [ ] 配置重试机制和错误处理
- [ ] 监控处理性能和资源使用
- [ ] 测试各种异常场景的恢复能力
- [ ] 优化I/O操作和数据结构
- [ ] 实施背压控制和优雅降级
- [ ] 文档化配置参数和使用方法